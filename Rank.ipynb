{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485b75ae-02d8-4d3e-b0f4-c8cad9518e8c",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa116d3b-51ac-42de-bf7a-fb55eb148689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score,roc_curve, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna.integration.xgboost as xgb_o\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "import scipy as sp\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import sklearn\n",
    "from scipy.special import comb\n",
    "from itertools import combinations\n",
    "import copy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import comb\n",
    "from itertools import permutations\n",
    "import datetime\n",
    "import lxml\n",
    "import seaborn as sns\n",
    "from hyperopt import hp, tpe, Trials, fmin,STATUS_OK\n",
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782806f-f29f-40fe-9c06-7dfca7dd2339",
   "metadata": {},
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4008e3-1c76-4f8e-b375-0f3533379d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ubu = '/home/hipro/デスクトップ/Horse/Data/20_21'\n",
    "path_mac2 = '/Users/rince/Desktop/Horse/Data/saishin2/'\n",
    "path_mac = '/Users/rince/Desktop/Horse/Data/saishin/'\n",
    "path_win = '/Users/Owner/Desktop/program/Horse/Data/saishin/'\n",
    "path_win2 = '/Users/Owner/Desktop/program/Horse/Data/saishin2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abdac4-ad14-4a42-a549-33a6270db233",
   "metadata": {},
   "source": [
    "# funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa496290-3b79-4e19-9f1b-a7c4a08f209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(df, test_size=0.2, rank_learning=True):\n",
    "    \"\"\"\n",
    "    データを学習データと, 訓練データに分ける関数\n",
    "    \"\"\"\n",
    "    df_ = df.copy()\n",
    "    if not rank_learning:\n",
    "        df_['rank'] = df_['rank'].map(lambda x:1 if x<4 else 0)\n",
    "    sorted_id_list = df_.sort_values(\"date\").index.unique()\n",
    "    train_id_list = sorted_id_list[: round(len(sorted_id_list) * (1 - test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list) * (1 - test_size)) :]\n",
    "    train = df_.loc[train_id_list]#.drop(['date'], axis=1)\n",
    "    test = df_.loc[test_id_list]#.drop(['date'], axis=1)\n",
    "    return train, test\n",
    "\n",
    "def rus_data(df, test_size=0.2):\n",
    "    train, test = split_data(df,test_size=test_size)\n",
    "    x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "    y_train = train['rank']\n",
    "    x_test = test.drop(['rank', 'date','単勝'], axis=1)\n",
    "    y_test = test['rank']\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    x_resampled, y_resampled = rus.fit_resample(x_train, y_train)\n",
    "    return x_resampled, y_resampled, x_test, y_test\n",
    "\n",
    "def load_csv(load_path):\n",
    "    df = pd.read_csv(load_path, index_col=0)\n",
    "    return df\n",
    "\n",
    "def gain(return_func, x_, n_samples=100,lower=50,t_range=[0.5,3.5]):\n",
    "    gain = {}\n",
    "    for i in range(n_samples):\n",
    "        threshold = t_range[1] * (i/n_samples) + t_range[0] *(1-i/n_samples)\n",
    "        n_bets, return_rate, n_hits,std = return_func(x_, threshold)\n",
    "        if n_bets > lower:\n",
    "            gain[threshold] = {'return_rate':return_rate,'n_hits':n_hits,'std':std,'n_bets':n_bets}\n",
    "    return pd.DataFrame(gain).T\n",
    "\n",
    "place_dict = {\n",
    "    '札幌':'01',  '函館':'02',  '福島':'03',  '新潟':'04',  '東京':'05', \n",
    "    '中山':'06',  '中京':'07',  '京都':'08',  '阪神':'09',  '小倉':'10'\n",
    "}\n",
    "\n",
    "race_type_dict = {\n",
    "    '芝': '芝', 'ダ': 'ダート', '障': '障害'\n",
    "}\n",
    "\n",
    "def plot(g,label=''):\n",
    "    plt.fill_between(g.index,y1 = g['return_rate'] - g['std'],y2=g['return_rate']+g['std'],alpha=0.3)\n",
    "    plt.plot(g.index,g['return_rate'],label=label)\n",
    "    plt.grid(True)\n",
    "    \n",
    "def update_data(old, new):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    old : pandas.DataFrame\n",
    "        古いデータ\n",
    "    new : pandas.DataFrame\n",
    "        新しいデータ\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])\n",
    "\n",
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    for race_id in race_id_list:\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            race_results[race_id] = pd.read_html(url)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return race_results\n",
    "\n",
    "def plot_importances(xgb_model, x_test):\n",
    "    importances = pd.DataFrame(\n",
    "    {'features' : x_test.columns, 'importances' : xgb_model.feature_importances_})\n",
    "    print(importances.sort_values('importances', ascending=False)[:20])\n",
    "    \n",
    "def xgb_pred(x_train, y_train, x_test, y_test):\n",
    "    param_dist = {'objective':'binary:logistic',\n",
    "                  'n_estimators':14,\n",
    "                  'use_label_encoder':False,\n",
    "                 'max_depth':4,\n",
    "                 'random_state':100}\n",
    "    \n",
    "    best_params = {'booster': 'gbtree', \n",
    "                   'objective': 'binary:logistic',\n",
    "                   'use_label_encoder':False,\n",
    "                   'eval_metric': 'rmse', \n",
    "                   'random_state': 100, \n",
    "                   'use_label_encoder':False,\n",
    "                   'eta': 0.13449222415941048,\n",
    "                   'max_depth': 3,\n",
    "                   'lambda': 0.7223936363734638, \n",
    "                   'n_estimators': 14, \n",
    "                   'reg_alpha': 0.7879044553842869,\n",
    "                   'reg_lambda': 0.7780344172793093,\n",
    "                   'importance_type': 'gain'}\n",
    "    xgb_model = xgb.XGBClassifier(**best_params)\n",
    "    hr_pred = xgb_model.fit(x_train.astype(float), np.array(y_train), eval_metric='logloss').predict(x_test.astype(float))\n",
    "    print(\"---------------------\")\n",
    "    y_proba_train = xgb_model.predict_proba(x_train)[:,1]\n",
    "    y_proba = xgb_model.predict_proba(x_test)[:,1]\n",
    "    print('AUC train:',roc_auc_score(y_train,y_proba_train))    \n",
    "    print('AUC test :',roc_auc_score(y_test,y_proba))\n",
    "    print(classification_report(np.array(y_test), hr_pred))\n",
    "    xgb.plot_importance(xgb_model) \n",
    "    plot_importances(xgb_model, x_test)\n",
    "    return xgb_model\n",
    "\n",
    "def lgb_pred(x_train, y_train, x_test, y_test):\n",
    "    param_dist = {\n",
    "        'objective' : 'binary',\n",
    "          'random_state':100,\n",
    "                 }\n",
    "    best_params = {'objective': 'binary',\n",
    "     'metric': 'l1',\n",
    "     'verbosity': -1,\n",
    "     'boosting_type': 'gbdt',\n",
    "     'feature_pre_filter': False,\n",
    "     'lambda_l1': 0.001101158293733924,\n",
    "     'lambda_l2': 7.419556660834531e-07,\n",
    "     'num_leaves': 254,\n",
    "     'feature_fraction': 1.0,\n",
    "     'bagging_fraction': 0.9773374137350906,\n",
    "     'bagging_freq': 1,\n",
    "     'min_child_samples': 5,\n",
    "    #  'num_iterations': 200,\n",
    "    #  'early_stopping_round': 50,\n",
    "     'categorical_column': [4,\n",
    "                            5,94,95,96,97,  98,  99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,  121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,  132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,  143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,  154,\n",
    "      155]\n",
    "                  }\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "    hr_pred = lgb_model.fit(x_train.astype(float), np.array(y_train), eval_metric='logloss').predict(x_test.astype(float))\n",
    "    print(\"---------------------\")\n",
    "    y_proba_train = lgb_model.predict_proba(x_train.astype(float))[:,1]\n",
    "    y_proba = lgb_model.predict_proba(x_test.astype(float))[:,1]\n",
    "    print('AUC train:',roc_auc_score(y_train,y_proba_train))    \n",
    "    print('AUC test :',roc_auc_score(y_test,y_proba))\n",
    "    print(classification_report(np.array(y_test), hr_pred))\n",
    "    plt.clf()\n",
    "    lgb.plot_importance(lgb_model) \n",
    "    plot_importances(lgb_model, x_test)\n",
    "    return lgb_model\n",
    "\n",
    "def make_data(data_,test_rate=0.8,is_rus=True):\n",
    "    data_ = data_.sort_values('date')\n",
    "    x_ = data_.drop(['rank','date','単勝'],axis=1)\n",
    "    y_ = data_['rank']\n",
    "\n",
    "    test_rate = int(test_rate*len(x_))\n",
    "    x_train, x_test = x_.iloc[:test_rate],x_.iloc[test_rate:]\n",
    "    y_train, y_test = y_.iloc[:test_rate],y_.iloc[test_rate:]\n",
    "    if is_rus:\n",
    "        rus = RandomUnderSampler(random_state=0)\n",
    "        x_resampled, y_resampled = rus.fit_resample(x_train, y_train)\n",
    "        return x_resampled, y_resampled, x_test, y_test\n",
    "    else:\n",
    "        return x_train,y_train,x_test,y_test\n",
    "\n",
    "def make_check_data(data_,test_rate=0.8):\n",
    "    data_ = data_.sort_values('date')\n",
    "    x_ = data_.drop(['rank','date'],axis=1)\n",
    "    y_ = data_['rank']\n",
    "\n",
    "    test_rate = int(test_rate*len(x_))\n",
    "    x_train, x_check = x_.iloc[:test_rate],x_.iloc[test_rate:]\n",
    "    y_train, y_check = y_.iloc[:test_rate],y_.iloc[test_rate:]\n",
    "\n",
    "    return x_check,y_check\n",
    "\n",
    "def grid_search(x_train,y_train,x_test,y_test):\n",
    "    trains = xgb.DMatrix(x_train.astype(float), label=y_train)\n",
    "    tests = xgb.DMatrix(x_test.astype(float), label=y_test)\n",
    "\n",
    "    base_params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state':100,\n",
    "        'use_label_encoder':False\n",
    "    }\n",
    "\n",
    "    watchlist = [(trains, 'train'), (tests, 'eval')]\n",
    "    tmp_params = copy.deepcopy(base_params)\n",
    "    \n",
    "#     インナー関数\n",
    "    def optimizer(trial):\n",
    "        eta = trial.suggest_uniform('eta', 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        __lambda = trial.suggest_uniform('lambda', 0.7, 2)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 3, 20)\n",
    "        learning_rate = trial.suggest_uniform('lambda', 0.01, 1)\n",
    "        reg_alpha = trial.suggest_uniform('reg_alpha', 0.01, 1)\n",
    "        reg_lambda = trial.suggest_uniform('reg_lambda', 0.01, 1)\n",
    "        importance_type = trial.suggest_categorical('importance_type',\n",
    "                                                    ['gain', 'weight', 'cover','total_gain','total_cover'])\n",
    "\n",
    "        tmp_params['eta'] = eta\n",
    "        tmp_params['max_depth'] = max_depth\n",
    "        tmp_params['lambda'] = __lambda\n",
    "        tmp_params['n_estimators'] = n_estimators\n",
    "        tmp_params['learning_rate'] = learning_rate\n",
    "        tmp_params['reg_alpha'] = reg_alpha\n",
    "        tmp_params['reg_lambda'] = reg_lambda\n",
    "        tmp_params['importance_type'] = importance_type\n",
    "        model = xgb.train(tmp_params, trains, num_boost_round=50)\n",
    "        predicts = model.predict(tests)\n",
    "        r2 = r2_score(y_test, predicts)\n",
    "        print(f'#{trial.number}, Result: {r2}, {trial.params}')\n",
    "        return r2\n",
    "    \n",
    "def predict(race_id,p,hr,r,return_tables,lgb_clf,date):\n",
    "    data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "    st = ShutubaTable(data)\n",
    "    st.preprocessing()\n",
    "    st.merge_horse_results(hr)\n",
    "    st.merge_peds(p.peds_e)\n",
    "    st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "    return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)\n",
    "    me_st = ModelEvaluator(lgb_clf, return_tables)\n",
    "\n",
    "    \n",
    "    #予測\n",
    "    scores = me_st.predict_proba(st.data_c.drop(['date'],axis=1),train=False)\n",
    "    pred = st.data_c[['馬番']].copy()\n",
    "    pred['scores'] = scores\n",
    "    print(pred.loc[race_id].sort_values('scores',ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897b3ed-08f1-452f-bc5c-840f04015aed",
   "metadata": {},
   "source": [
    "# race_id 命名規則"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840d19f-0d83-4c24-a9d6-245b20e6eac6",
   "metadata": {},
   "source": [
    "race_id 202105040802\\\n",
    "yyyy_pp_xx_xxrr\\\n",
    "y : year\\\n",
    "p : palce\\\n",
    "x : 謎\\\n",
    "r : race番号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0bd9a-f85f-4949-90e8-4915a8a43ff3",
   "metadata": {},
   "source": [
    "# r.data_c['単勝'] == st.data_c[オッズ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4a8a9-d289-4262-bbd7-e7196f6b2d2c",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d27a0076-950a-49b1-9b34-5ca61c6adfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金', '着差', '通過',\n",
    "                                            '開催', '距離']]\n",
    "        self.preprocessing()\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        #horse_idをkeyにしてDataFrame型を格納\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "#         for horse_id in horse_id_list:\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "                df = pd.read_html(url)[3]\n",
    "                #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
    "                if df.columns[0]=='受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる        \n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "\n",
    "        return horse_results_df\n",
    "    \n",
    "    \n",
    "    #省略\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        #1着の着差を0にする\n",
    "        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n",
    "        \n",
    "        #レース展開データ\n",
    "        #n=1: 最初のコーナー位置, n=4: 最終コーナー位置\n",
    "        def corner(x, n):\n",
    "            if type(x) != str:\n",
    "                return x\n",
    "            elif n==4:\n",
    "                return int(re.findall(r'\\d+', x)[-1])\n",
    "            elif n==1:\n",
    "                return int(re.findall(r'\\d+', x)[0])\n",
    "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
    "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
    "        \n",
    "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
    "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
    "        df['first_to_final'] = df['first_corner'] - df['final_corner']\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
    "        #race_type\n",
    "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n",
    "        #距離\n",
    "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
    "        df.drop(['距離'], axis=1, inplace=True)\n",
    "        \n",
    "        #インデックス名を与える\n",
    "        df.index.name = 'horse_id'\n",
    "    \n",
    "        self.horse_results = df\n",
    "        self.target_list = ['着順', '賞金', '着差', 'first_corner',\n",
    "                            'first_to_rank', 'first_to_final','final_to_rank']\n",
    "        \n",
    "        \n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.query('index in @horse_id_list')\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "          \n",
    "        self.average_dict = {}\n",
    "        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list]\\\n",
    "            .mean().add_suffix('_{}R'.format(n_samples))\n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            self.average_dict[column] = filtered_df.groupby(['horse_id', column])\\\n",
    "                [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples)).fillna(0)\n",
    "\n",
    "    \n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        self.average(horse_id_list, date, n_samples)\n",
    "        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id',\n",
    "                             right_index=True, how='left')\n",
    "        for column in ['course_len','race_type', '開催']:\n",
    "            merged_df = merged_df.merge(self.average_dict[column], \n",
    "                                        left_on=['horse_id', column],\n",
    "                                        right_index=True, how='left').fillna(0)\n",
    "        return merged_df\n",
    "    \n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat(\n",
    "            [self.merge(results, date, n_samples) for date in tqdm(date_list)]\n",
    "        )\n",
    "        return merged_df\n",
    "\n",
    "class Return:\n",
    "\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        \"\"\"\n",
    "        払い戻し表データをスクレイピングする関数\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        return_tables_df : pandas.DataFrame\n",
    "            全払い戻し表データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "\n",
    "                #普通にスクレイピングすると複勝やワイドなどが区切られないで繋がってしまう。\n",
    "                #そのため、改行コードを文字列brに変換して後でsplitする\n",
    "                f = urllib.request.urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')\n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "\n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        return return_tables_df\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        \n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        \n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "        return df.fillna(0).astype(int)\n",
    "    \n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1,2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        \n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "            \n",
    "        return tansho\n",
    "    \n",
    "    @property\n",
    "    def umaren(self):\n",
    "        umaren = self.return_tables[self.return_tables[0]=='馬連'][[1,2]]\n",
    "        wins = umaren[1].str.split('-', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umaren[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def umatan(self):\n",
    "        umatan = self.return_tables[self.return_tables[0]=='馬単'][[1,2]]\n",
    "        wins = umatan[1].str.split('→', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umatan[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def wide(self):\n",
    "        wide = self.return_tables[self.return_tables[0]=='ワイド'][[1,2]]\n",
    "        wins = wide[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        wins = wins.stack().str.split('-', expand=True).add_prefix('win_')\n",
    "        return_ = wide[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        return_ = return_.stack().rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1)\n",
    "        return df.apply(lambda x: pd.to_numeric(x.str.replace(',',''), errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def sanrentan(self):\n",
    "        rentan = self.return_tables[self.return_tables[0]=='三連単'][[1,2]]\n",
    "        wins = rentan[1].str.split('→', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = rentan[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def sanrenpuku(self):\n",
    "        renpuku = self.return_tables[self.return_tables[0]=='三連複'][[1,2]]\n",
    "        wins = renpuku[1].str.split('-', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = renpuku[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "class ModelEvaluator:\n",
    "\n",
    "    \n",
    "    def __init__(self, model, return_tables):\n",
    "        self.model = model\n",
    "        self.rt = Return(return_tables)\n",
    "        self.fukusho = self.rt.fukusho\n",
    "        self.tansho = self.rt.tansho\n",
    "        self.umaren = self.rt.umaren\n",
    "        self.umatan = self.rt.umatan\n",
    "        self.wide = self.rt.wide\n",
    "        self.sanrenpuku = self.rt.sanrenpuku\n",
    "        self.sanrentan = self.rt.sanrentan\n",
    "\n",
    "    \n",
    "    #3着以内に入る確率を予測\n",
    "    def predict_proba(self, X, train=True, std=True, minmax=False):\n",
    "        if train:\n",
    "            proba = pd.Series(\n",
    "                self.model.predict_proba(X.drop(['単勝'], axis=1))[:, 1], index=X.index\n",
    "            )\n",
    "        else:\n",
    "            proba = pd.Series(\n",
    "                self.model.predict_proba(X, axis=1)[:, 1], index=X.index\n",
    "            )\n",
    "        if std:\n",
    "            #レース内で標準化して、相対評価する。「レース内偏差値」みたいなもの。\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "        if minmax:\n",
    "            #データ全体を0~1にする\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "        return proba\n",
    "    \n",
    "    #0か1かを予測\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        self.proba = y_pred\n",
    "        return [0 if p<threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        return roc_auc_score(y_true, self.predict_proba(X))\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({\"features\": X.columns, \n",
    "                                    \"importance\": self.model.feature_importances_})\n",
    "        return importances.sort_values(\"importance\", ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['馬番', '単勝']]\n",
    "        pred_table['pred'] = self.predict(X, threshold)\n",
    "        pred_table['score'] = self.proba\n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred']==1][['馬番', '単勝', 'score','pred']]\n",
    "        else:\n",
    "            return pred_table[['馬番', '単勝', 'score', 'pred']]\n",
    "        \n",
    "    def bet(self, race_id, kind, umaban, amount):\n",
    "        if kind == 'fukusho':\n",
    "            rt_1R = self.fukusho.loc[race_id]\n",
    "            return_ = (rt_1R[['win_0', 'win_1', 'win_2']]==umaban).values * \\\n",
    "                rt_1R[['return_0', 'return_1', 'return_2']].values * amount/100\n",
    "            return_ = np.sum(return_)\n",
    "        if kind == 'tansho':\n",
    "            rt_1R = self.tansho.loc[race_id]\n",
    "            return_ = (rt_1R['win']==umaban) * rt_1R['return'] * amount/100\n",
    "        if kind == 'umaren':\n",
    "            rt_1R = self.umaren.loc[race_id]\n",
    "            return_ = (set(rt_1R[['win_0', 'win_1']]) == set(umaban)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if kind == 'umatan':\n",
    "            rt_1R = self.umatan.loc[race_id]\n",
    "            return_ = (list(rt_1R[['win_0', 'win_1']]) == list(umaban))\\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if kind == 'wide':\n",
    "            rt_1R = self.wide.loc[race_id]\n",
    "            return_ = (rt_1R[['win_0', 'win_1']].\\\n",
    "                           apply(lambda x: set(x)==set(umaban), axis=1)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "            return_ = return_.sum()\n",
    "        if kind == 'sanrentan':\n",
    "            rt_1R = self.sanrentan.loc[race_id]\n",
    "            return_ = (list(rt_1R[['win_0', 'win_1', 'win_2']]) == list(umaban)) * \\\n",
    "                rt_1R['return']/100 * amount\n",
    "        if kind == 'sanrenpuku':\n",
    "            rt_1R = self.sanrenpuku.loc[race_id]\n",
    "            return_ = (set(rt_1R[['win_0', 'win_1', 'win_2']]) == set(umaban)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if not (return_ >= 0):\n",
    "                return_ = amount\n",
    "        return return_\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(np.sum([\n",
    "                self.bet(race_id, 'fukusho', umaban, 1) for umaban in preds['馬番']\n",
    "            ]))\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        self.sample = pred_table\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(\n",
    "                np.sum([self.bet(race_id, 'tansho', umaban, 1) for umaban in preds['馬番']])\n",
    "            )\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(\n",
    "                np.sum(preds.apply(lambda x: self.bet(\n",
    "                    race_id, 'tansho', x['馬番'], 1/x['単勝']), axis=1)))\n",
    "        \n",
    "        bet_money = (1 / pred_table['単勝']).sum()\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / bet_money\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / bet_money\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umaren_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umaren', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umatan_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue   \n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in permutations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umatan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "            return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def wide_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'wide', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std  \n",
    "        \n",
    "    def sanrentan_box(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            if len(preds)<3:\n",
    "                continue\n",
    "            else:\n",
    "                for umaban in permutations(preds['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def sanrenpuku_box(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            if len(preds)<3:\n",
    "                continue\n",
    "            else:\n",
    "                for umaban in combinations(preds['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrenpuku', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umaren_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'umaren', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += n_aite\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umaren', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umatan_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'umatan', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += n_aite\n",
    "                \n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in permutations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umatan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "            return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def wide_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'wide', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += len(preds_aite)\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'wide', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def sanrentan_nagashi(self, X, threshold = 1.5, n_aite=7):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) == 2:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[2:(n_aite+2)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'sanrentan',\n",
    "                        np.append(preds_jiku['馬番'].values, x),\n",
    "                        1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += len(preds_aite)\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 3:\n",
    "                return_ = 0\n",
    "                for umaban in permutations(preds_jiku['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "class DataProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame() #raw data\n",
    "        self.data_p = pd.DataFrame() #after preprocessing\n",
    "        self.data_h = pd.DataFrame() #after merging horse_results\n",
    "        self.data_pe = pd.DataFrame() #after merging peds\n",
    "        self.data_c = pd.DataFrame() #after processing categorical features\n",
    "        \n",
    "    #馬の過去成績データの追加\n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "        self.data_h.drop(['開催'], axis=1, inplace=True)\n",
    "            \n",
    "    #血統データ追加\n",
    "    def merge_peds(self, peds):\n",
    "        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True,how='left')\n",
    "#         重複データを削除\n",
    "        self.data_pe = self.data_pe[~self.data_pe.duplicated()]\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()\n",
    "#         print(\"type :\",type(self.no_peds)) ndarray\n",
    "#         Peds.scrape()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "            \n",
    "        #カテゴリ変数の処理\n",
    "    def process_categorical(self, le_horse, le_jockey,results_m):\n",
    "        df = self.data_pe.copy()\n",
    "        \n",
    "        #ラベルエンコーディング。horse_id, jockey_idを0始まりの整数に変換\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        \n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "#         pedsデータのラベルエンコーディング\n",
    "\n",
    "#         for column in p.peds_e.columns:\n",
    "# #             self.le_peds_dict[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "# #             mask_peds = df[column].isin(p.le_peds[column].classes_)\n",
    "#             new_peds_id = df[column].dropna().unique()\n",
    "# #             p.le_peds[column].classes_ = np.concatenate([p.le_peds[column].classes_, new_peds_id])\n",
    "#             df[column] = p.le_peds[column].transform(df[column])\n",
    "        \n",
    "        \n",
    "        #horse_id, jockey_idをpandasのcategory型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "        \n",
    "        #そのほかのカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n",
    "        #列を一定にするため\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n",
    "        \n",
    "        self.data_c = df    \n",
    "    \n",
    "class ShutubaTable(DataProcessor):\n",
    "    \n",
    "    \n",
    "    def __init__(self, shutuba_tables):\n",
    "        super(ShutubaTable, self).__init__()\n",
    "        self.data = shutuba_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, race_id_list, date):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                    df[\"weather\"] = [text] * len(df)\n",
    "                if text in [\"良\", \"稍重\", \"重\",\"稍\"]:\n",
    "                    df[\"ground_state\"] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df[\"ground_state\"] = ['不良'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            df['date'] = [date] * len(df)\n",
    "\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_td_list = soup.find_all(\"td\", attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_td_list = soup.find_all(\"td\", attrs={'class': 'Jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['horse_id'] = list(map(lambda x: int(x),horse_id_list)) \n",
    "            df['jockey_id'] = jockey_id_list\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "#             win 環境だとなぜかintに直せない.floatならつかえる\n",
    "            df.index = df.index.astype(int)\n",
    "            data = data.append(df)\n",
    "\n",
    "            \n",
    "        return data\n",
    "                \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "#         体重変化をデータから消した\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df = df[df[\"馬体重(増減)\"] != '--']\n",
    "        df[\"体重\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "        df[\"体重変化\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[1].str[:-1].replace('前計不',0).astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        \n",
    "        df['枠'] = df['枠'].astype(int)\n",
    "        df['馬番'] = df['馬番'].astype(int)\n",
    "        df['斤量'] = df['斤量'].astype(int)\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        df['n_horse'] = df.index.map(lambda x: len(df.loc[x]))\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df = df[['枠', '馬番', '斤量', 'course_len', 'weather','race_type',\n",
    "        'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢','開催','n_horse','体重','体重変化']]\n",
    "        \n",
    "        self.data_p = df.rename(columns={'枠': '枠番'})\n",
    "        \n",
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "        self.le_peds = None\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        #race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(0.5)\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "                #メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "                html = requests.get(url)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "\n",
    "                #馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            #存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            #Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "        \n",
    "    #前処理    \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "#         rank学習の場合はそのまま\n",
    "#         df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
    "        df['rank'] = df['着順']\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\", '馬名', '騎手', '人気', '着順'],\n",
    "                axis=1, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        df['n_horse'] = df.index.map(lambda x: len(df.loc[x]))\n",
    "        \n",
    "        self.data_p = df\n",
    "    \n",
    "    #カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])\n",
    "#         self.le_peds = p.le_peds_dict\n",
    "        super().process_categorical(self.le_horse, self.le_jockey,self.data_pe)\n",
    "        \n",
    "class Peds:\n",
    "\n",
    "    def __init__(self, peds):\n",
    "        self.peds = peds\n",
    "        self.peds_cat = pd.DataFrame() #after label encoding and transforming into category\n",
    "        self.peds_re = pd.DataFrame()\n",
    "        self.peds_vec = pd.DataFrame()\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "#         for horse_id in horse_id_list:\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            \n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                #重複を削除して1列のSeries型データに直す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "\n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #列名をpeds_0, ..., peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict],\n",
    "                            axis=1).T.add_prefix('peds_')\n",
    "        peds_df.index =peds_df.index.astype(int)\n",
    "\n",
    "        return peds_df\n",
    "    \n",
    "    \n",
    "#     血統データが正規化されたいないデータに対して, 正規化する関数\n",
    "    def regularize_peds(self):\n",
    "        peds = self.peds.copy()\n",
    "        error_idx_list = []\n",
    "        for idx in tqdm(peds.index):\n",
    "            for col in peds.columns:\n",
    "            #     漢字 : 一-龥\n",
    "                code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％一-龥\\d]')\n",
    "                try:\n",
    "                    cleaned_text = code_regex.sub('', peds[col].loc[idx])\n",
    "                    one_word = \"\".join(cleaned_text.split())\n",
    "                    p_alphabet = re.compile('[a-zA-Z]+')\n",
    "                    p_katakana = re.compile(r'[ァ-ヶー]+')\n",
    "\n",
    "                    peds[col].loc[idx] = one_word\n",
    "                    if (not p_alphabet.fullmatch(one_word)) and not (p_katakana.fullmatch(one_word)):\n",
    "                        peds[col].loc[idx] = re.sub('[a-zA-Z]+', '', one_word)\n",
    "                except:\n",
    "                    error_idx_list.append(idx)\n",
    "        self.error_idx_list_r = error_idx_list\n",
    "        self.peds_re = peds\n",
    "\n",
    "    \n",
    "    def categorize(self):\n",
    "        df = self.peds.copy()\n",
    "        self.le_peds_dict = {}\n",
    "        \n",
    "        \n",
    "        for column in df.columns:\n",
    "            \n",
    "            self.le_peds_dict[column] = LabelEncoder()\n",
    "            df[column] = self.le_peds_dict[column].fit_transform(df[column].fillna('Na'))\n",
    "#             df[column] = self.le_peds_dict[column]\n",
    "        self.peds_cat = df.astype('category')\n",
    "        self.le_peds = self.le_peds_dict\n",
    "        \n",
    "        \n",
    "#         血統データをベクトル化する関数\n",
    "# peds_re は 正規化済み血統データを仮定\n",
    "# model_ft : fasttextモデル\n",
    "    def vectorize(self,peds_re,model_ft):\n",
    "        df = peds_re.copy()\n",
    "        error_idx_list = []\n",
    "        for idx in tqdm(df.index):\n",
    "            text = ','.join(df.loc[idx].tolist())\n",
    "            df.loc[idx] = model_ft[text]\n",
    "#             except:\n",
    "#                 error_idx_list.append(idx)\n",
    "        self.error_idx_list_v = error_idx_list\n",
    "        self.peds_vec = df.astype('float')\n",
    "#     def vectorize(self,peds_re,model_ft):\n",
    "#         df = peds_re.copy()\n",
    "        \n",
    "#         for idx in tqdm(df.index):\n",
    "#             for column in df.columns:\n",
    "#                 horse_name = df[column].loc[idx]\n",
    "#                 df[column].loc[idx] = model_ft[horse_name][0]\n",
    "\n",
    "#         self.peds_vec = df.astype('float')\n",
    "\n",
    "class Simulater():\n",
    "    \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.return_tables = None\n",
    "        self.pred_df = None\n",
    "    \n",
    "\n",
    "    #     当日のデータでシミュレートするとあかん\n",
    "    def return_table(self, race_id_list):\n",
    "        return_tables = Return.scrape(race_id_list)\n",
    "        return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    \n",
    "    def return_table_today(self,race_id_list):\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = 'https://race.netkeiba.com/race/result.html?race_id='+race_id+'&amp;rf=race_submenu'\n",
    "                dfs = pd.read_html(url)\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "            #pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        return_tables_df.index = return_tables_df.index.astype(int)\n",
    "        self.return_tables = return_tables_df\n",
    "    \n",
    "   \n",
    "    def return_pred_table(self,st,return_tables):\n",
    "        me_st = ModelEvaluator(self.model, return_tables)\n",
    "        #予測\n",
    "        scores = me_st.predict_proba(st.data_c.drop(['date'],axis=1),train=False)\n",
    "        pred = st.data_c[['馬番']].copy()\n",
    "        pred['scores'] = scores\n",
    "        pred.index = pred.index.astype(int)\n",
    "        return pred\n",
    "        \n",
    "        \n",
    "    def show_results(self , st ,race_id_list ,bet = 100):\n",
    "        self.return_table_today(race_id_list)\n",
    "        return_tables = self.return_tables.copy()\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        target_race_dict = {}\n",
    "        self.pred_df = self.return_pred_table(st,return_tables)\n",
    "        tansho_list = []\n",
    "        fukusho_list = []\n",
    "        wide_list =[]\n",
    "\n",
    "        for race_id in race_id_list:\n",
    "            df_  = self.return_tables.loc[race_id]\n",
    "            print(\"-------------------\")\n",
    "            print(\"predict\")\n",
    "            pred_df = self.pred_df.loc[int(race_id)]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            print(pred_df.iloc[:3])\n",
    "            print(\"actual\")\n",
    "            print(self.return_tables.loc[race_id])\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "\n",
    "\n",
    "            if  pred_1 == df_[df_[0]=='単勝'][1].values[0]:\n",
    "                acc_dict['単勝'] += 1\n",
    "                profit = int(df_[df_[0]=='単勝'][2].values[0].replace('円','').replace(',',''))\n",
    "                return_dict['単勝'] += profit\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                tansho_list.append(race_id[-2:])\n",
    "                fukusho_list.append(race_id[-2:])\n",
    "\n",
    "            elif pred_1 in df_[df_[0]=='複勝'][1].str.split(' ')[0]:\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                fukusho_list.append(race_id[-2:])\n",
    "                \n",
    "\n",
    "            for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "                if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                    if i!=0:\n",
    "                        return_index = i-1\n",
    "                    else:\n",
    "                        return_index = i\n",
    "                    profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                    return_dict['ワイド'] += profit\n",
    "                    print(\"profit\",profit)\n",
    "                    acc_dict['ワイド'] += 1\n",
    "                    wide_list.append(race_id[-2:])\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for i, key in enumerate(acc_dict):\n",
    "            return_dict[key] -= bet * len(race_id_list)\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース\",tansho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        print(\"的中レース\",fukusho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"ワイド\")\n",
    "        print(\"的中率 :\",acc_dict['ワイド'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['ワイド'],'円')\n",
    "        print(\"的中レース\",wide_list)\n",
    "             \n",
    "class RankSimulater(Simulater):\n",
    "    \n",
    "    \n",
    "    def return_pred_table(self,data_c,is_long=False):\n",
    "        # is_long って何？\n",
    "        #予測\n",
    "        if not is_long:\n",
    "            scores = pd.Series(self.model.predict(data_c.drop(['date'],axis=1)),index=data_c.index)\n",
    "        else:\n",
    "            scores = pd.Series(self.model.predict(data_c.drop(['date','rank','単勝'],axis=1)),index=data_c.index)\n",
    "        pred = data_c[['馬番']].copy()\n",
    "        pred['scores'] = scores\n",
    "        pred = pred.sort_values('scores',ascending=False)\n",
    "        return pred\n",
    "\n",
    "# 的中レースの分布を表示できるように\n",
    "\n",
    "    def show_results(self , st ,race_id_list ,bet = 100):\n",
    "        self.return_table_today(race_id_list)\n",
    "        return_tables = self.return_tables.copy()\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        # target_race_dict  = {}\n",
    "        self.pred_df = self.return_pred_table(st.data_c)\n",
    "        tansho_list = []\n",
    "        fukusho_list = []\n",
    "        wide_list =[]\n",
    "\n",
    "        for race_id in race_id_list:\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            print(\"-------------------\")\n",
    "            print(\"predict\")\n",
    "            pred_df = self.pred_df.loc[int(race_id)]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            print(pred_df.iloc[:3])\n",
    "            print(\"actual\")\n",
    "            print(return_tables.loc[race_id])\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "\n",
    "\n",
    "            if  pred_1 == df_[df_[0]=='単勝'][1].values[0]:\n",
    "                acc_dict['単勝'] += 1\n",
    "                profit = int(df_[df_[0]=='単勝'][2].values[0].replace('円','').replace(',',''))\n",
    "                return_dict['単勝'] += profit\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                tansho_list.append(race_id[-2:])\n",
    "                fukusho_list.append(race_id[-2:])\n",
    "\n",
    "            elif pred_1 in df_[df_[0]=='複勝'][1].str.split(' ')[0]:\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                fukusho_list.append(race_id[-2:])\n",
    "                \n",
    "\n",
    "            for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "                if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                    if i!=0:\n",
    "                        return_index = i-1\n",
    "                    else:\n",
    "                        return_index = i\n",
    "                    profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                    return_dict['ワイド'] += profit\n",
    "                    print(\"profit\",profit)\n",
    "                    acc_dict['ワイド'] += 1\n",
    "                    wide_list.append(race_id[-2:])\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for i, key in enumerate(acc_dict):\n",
    "            return_dict[key] -= bet * len(race_id_list)\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース\",tansho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        print(\"的中レース\",fukusho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"ワイド\")\n",
    "        print(\"的中率 :\",acc_dict['ワイド'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['ワイド'],'円')\n",
    "        print(\"的中レース\",wide_list)\n",
    "    \n",
    "#     odds以上の馬券しか買わない\n",
    "    def show_long_results(self, data_c, return_tables, kaime='tansho', odds=2.0, bet = 100):\n",
    "        if kaime=='tansho':\n",
    "            pass\n",
    "        elif kaime=='fukusho':\n",
    "            pass\n",
    "        elif kaime=='wide':\n",
    "            pass\n",
    "        elif kaime=='wide_3_box':\n",
    "            pass\n",
    "        elif kaime=='umaren':\n",
    "            pass\n",
    "        elif kaime=='umatan':\n",
    "            pass\n",
    "        elif kaime=='sanrentan':\n",
    "            pass\n",
    "        elif kaime=='sanrenpuku':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"No such kaime.\")\n",
    "\n",
    "            \n",
    "    def calc_tansho(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "#         data_c = r.data_cを仮定\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        race_count_dict = {\n",
    "            '01':0,\n",
    "            '02':0,\n",
    "            '03':0,\n",
    "            '04':0,\n",
    "            '05':0,\n",
    "            '06':0,\n",
    "            '07':0,\n",
    "            '08':0,\n",
    "            '09':0,\n",
    "            '10':0,\n",
    "            '11':0,\n",
    "            '12':0\n",
    "        }\n",
    "\n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=is_long)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "\n",
    "            \n",
    "            pred_odds = data_c[data_c['馬番']==pred_1].loc[race_id]['単勝']\n",
    "            try:\n",
    "                rank = data_c[data_c['rank']==1].loc[race_id]['馬番']\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if type(rank)!=pd.core.series.Series:\n",
    "                if  pred_1 == rank:\n",
    "                    race_count_dict[str(race_id)[-2:]] += 1\n",
    "                    if pred_odds>=odds and score_1!= score_2:\n",
    "                        acc_dict['単勝'] += 1\n",
    "                        profit = pred_odds*bet\n",
    "                        return_dict['単勝'] += profit\n",
    "                        tansho_list.append(race_id)\n",
    "                    else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                        not_bet_count += 1\n",
    "#                     odds低かったら買わない\n",
    "                elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                    not_bet_count+=1\n",
    "            else:\n",
    "                if  pred_1 == rank.values[0] or pred_1 == rank.values[1]:\n",
    "                    race_count_dict[str(race_id)[-2:]] += 1\n",
    "                    if pred_odds>=odds and score_1!= score_2:\n",
    "                        acc_dict['単勝'] += 1\n",
    "                        profit = pred_odds*bet\n",
    "                        return_dict['単勝'] += profit\n",
    "                        tansho_list.append(race_id)\n",
    "                    else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                        not_bet_count += 1\n",
    "                elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                    not_bet_count+=1\n",
    "\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['単勝'] -= bet * real_race_len\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/real_race_len*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース :\",race_count_dict)\n",
    "#         print(\"的中レース\",tansho_list)\n",
    "\n",
    "    \n",
    "    def calc_tansho_top3(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=is_long)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "            pred_3 = pred_df['馬番'].iloc[2]\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "        \n",
    "        \n",
    "            odds_tmp = return_tables.loc[race_id].iloc[0][2].split('br')\n",
    "            real_odds = int(odds_tmp[0])/100\n",
    "            \n",
    "            \n",
    "            \n",
    "            rank_tmp = df_.iloc[0][1].split('br')\n",
    "            rank = int(rank_tmp[0])\n",
    "            # df_.iloc[0]が単勝\n",
    "            # df_.iloc[1]が複勝, etc..\n",
    "            # df_.iloc[x][1] が１着の馬番\n",
    "            # df_.iloc[x][2] がodds\n",
    "            # df_.iloc[x][3] が人気\n",
    "\n",
    "            if  pred_1 == rank or pred_2 == rank or pred_3==rank:\n",
    "                if real_odds>=odds and score_1!= score_2:\n",
    "                    acc_dict['単勝'] += 1\n",
    "                    profit = real_odds*bet\n",
    "                    return_dict['単勝'] += profit\n",
    "                    tansho_list.append(race_id)\n",
    "                else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                    not_bet_count += 1\n",
    "        \n",
    "#         top3 全てに賭けるから賭け金の3倍\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['単勝'] -= 3*bet * real_race_len\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list)-not_bet_count)\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        \n",
    "    \n",
    "    def calc_fukusho(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "#         data_c = r.data_cを仮定\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        \n",
    "        \n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=is_long)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # df_.iloc[0]が単勝\n",
    "            # df_.iloc[1]が複勝, etc..\n",
    "            # df_.iloc[x][1] が１着の馬番\n",
    "            # df_.iloc[x][2] がodds\n",
    "            # df_.iloc[x][3] が人気\n",
    "#             # 一着にのみかける\n",
    "# ############### 確定した odds と 単勝 odds が混在している, よくない\n",
    "            if pred_1 in df_[df_[0]=='複勝'][1].str.split('br').tolist()[0] and score_1!= score_2:\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split('br').tolist()[0].index(pred_1)\n",
    "                real_odds = int(df_[df_[0]=='複勝'][2].str.split('br').tolist()[0][return_index].replace(',',''))/100\n",
    "                \n",
    "                \n",
    "                if real_odds>=odds:    \n",
    "                    acc_dict['複勝'] += 1\n",
    "                    profit = real_odds*bet\n",
    "                    return_dict['複勝'] += profit \n",
    "                else:\n",
    "                    not_bet_count+=1\n",
    "#             odds が低かったら賭けない\n",
    "            elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                not_bet_count+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['複勝'] -= bet * real_race_len\n",
    "\n",
    "        print(\"---------------------\")\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format((acc_dict['複勝']/real_race_len)*100),'%')\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        \n",
    "        \n",
    "    def calc_wide(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        wide_list = []\n",
    "        race_id_list = data_c.index.tolist()\n",
    "        \n",
    "        for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "            if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                if i!=0:\n",
    "                    return_index = i-1\n",
    "                else:\n",
    "                    return_index = i\n",
    "\n",
    "            profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "            return_dict['ワイド'] += profit\n",
    "            print(\"profit\",profit)\n",
    "            acc_dict['ワイド'] += 1\n",
    "            wide_list.append(race_id[-2:])\n",
    "            break\n",
    "            \n",
    "            \n",
    "    def calc_wide_3box(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def calc_sanrenpuku(self,data_c,return_tables,bet=100,is_long=True):\n",
    "        acc_dict = {'三連複':0}\n",
    "        return_dict = {'三連複':0}\n",
    "        sanrenpuku_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        \n",
    "        \n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=is_long)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "            try:\n",
    "                pred_3 = pred_df['馬番'].iloc[2]\n",
    "            except:\n",
    "                print(\"race_id\",race_id)\n",
    "                print(\"pred_df\",pred_df)\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1] \n",
    "            \n",
    "#             data_cから観測できる odds は100をかけた時の ×odds だが, return_tables の オッズは, 100円をかけた時の払い戻し金額\n",
    "            odds_tmp = df_[df_[0]=='三連複'][2].values[0].replace(',','').split('br')\n",
    "            if len(odds_tmp)==1:\n",
    "                odds = int(odds_tmp[0])\n",
    "            else:\n",
    "                odds = int(odds_tmp[0])\n",
    "                odds2 = int(odds_tmp[1])\n",
    "\n",
    "            if score_1 != score_2:\n",
    "#                 当たってた時\n",
    "                try:\n",
    "                    if [int(i) for i in df_[df_[0]=='三連複'][1].values[0].replace(' ','').split('-')] == sorted([pred_1,pred_2,pred_3]):\n",
    "                        acc_dict['三連複'] += 1\n",
    "                        profit = (bet/100)*odds\n",
    "                        return_dict['三連複'] += profit\n",
    "                except:\n",
    "                    print()\n",
    "                    print('race_id',race_id)\n",
    "            else:\n",
    "                not_bet_count += 1\n",
    "            \n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['三連複'] -= bet * real_race_len\n",
    "#         この辺のロジック同じだから, 関数でまとめたい\n",
    "        print(\"---------------------\")\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"三連複\")\n",
    "        print(\"的中率 :\",acc_dict['三連複'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format((acc_dict['三連複']/real_race_len)*100),'%')\n",
    "        print(\"収支   :\",return_dict['三連複'],'円')\n",
    "    \n",
    "    def calc_sanrenpuku_box(self,data_c,return_tables,odds=2.0,bet=100,is_long=True):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def calc_sanrentan(self,data_c,return_tables,bet=100,is_long=True):\n",
    "        acc_dict = {'三連単':0}\n",
    "        return_dict = {'三連単':0}\n",
    "        sanrenpuku_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        \n",
    "        \n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=is_long)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "            try:\n",
    "                pred_3 = pred_df['馬番'].iloc[2]\n",
    "            except:\n",
    "                print(\"race_id\",race_id)\n",
    "                print(\"pred_df\",pred_df)\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1] \n",
    "            \n",
    "#             data_cから観測できる odds は100をかけた時の ×odds だが, return_tables の オッズは, 100円をかけた時の払い戻し金額\n",
    "            odds_tmp = df_[df_[0]=='三連単'][2].values[0].replace(',','').split('br')\n",
    "            if len(odds_tmp)==1:\n",
    "                odds = int(odds_tmp[0])\n",
    "            else:\n",
    "                odds = int(odds_tmp[0])\n",
    "                odds2 = int(odds_tmp[1])\n",
    "\n",
    "            if score_1 != score_2:\n",
    "#                 当たってた時\n",
    "                try:\n",
    "                    if [int(i) for i in df_[df_[0]=='三連単'][1].values[0].replace(' ','').split('→')] == [pred_1,pred_2,pred_3]:\n",
    "                        acc_dict['三連単'] += 1\n",
    "                        profit = (bet/100)*odds\n",
    "                        return_dict['三連単'] += profit\n",
    "                except:\n",
    "                    print()\n",
    "                    print('race_id',race_id)\n",
    "            else:\n",
    "                not_bet_count += 1\n",
    "            \n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['三連単'] -= bet * real_race_len\n",
    "#         この辺のロジック同じだから, 関数でまとめたい\n",
    "        print(\"---------------------\")\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"三連単\")\n",
    "        print(\"的中率 :\",acc_dict['三連単'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format((acc_dict['三連単']/real_race_len)*100),'%')\n",
    "        print(\"収支   :\",return_dict['三連単'],'円')\n",
    "    \n",
    "    \n",
    "    def show_results_today(self , st ,race_id_list ,bet = 100):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        fukusho_list = []\n",
    "        wide_list =[]\n",
    "\n",
    "        for race_id in race_id_list:\n",
    "            self.pred_df = self.return_pred_table(st.data_c.loc[int(race_id)])\n",
    "#             self.return_tables.index =  self.return_tables.index.astype(int)\n",
    "            df_  = self.return_tables.loc[race_id]\n",
    "            print(\"-------------------\")\n",
    "            print(\"predict\")\n",
    "            pred_df = self.pred_df.loc[int(race_id)]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            print(pred_df.iloc[:3])\n",
    "            print(\"actual\")\n",
    "            print(self.return_tables.loc[race_id])\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "\n",
    "\n",
    "            if  pred_1 == df_[df_[0]=='単勝'][1].values[0]:\n",
    "                acc_dict['単勝'] += 1\n",
    "                profit = int(df_[df_[0]=='単勝'][2].values[0].replace('円','').replace(',',''))\n",
    "                return_dict['単勝'] += profit\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                tansho_list.append(race_id[-2:])\n",
    "                fukusho_list.append(race_id[-2:])\n",
    "\n",
    "            elif pred_1 in df_[df_[0]=='複勝'][1].str.split(' ')[0]:\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                fukusho_list.append(race_id[-2:])\n",
    "                \n",
    "\n",
    "            for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "                if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                    if i!=0:\n",
    "                        return_index = i-1\n",
    "                    else:\n",
    "                        return_index = i\n",
    "                    profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                    return_dict['ワイド'] += profit\n",
    "                    print(\"profit\",profit)\n",
    "                    acc_dict['ワイド'] += 1\n",
    "                    wide_list.append(race_id[-2:])\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for i, key in enumerate(acc_dict):\n",
    "            return_dict[key] -= bet * len(race_id_list)\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース\",tansho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['複勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        print(\"的中レース\",fukusho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"ワイド\")\n",
    "        print(\"的中率 :\",acc_dict['ワイド'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['ワイド']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['ワイド'],'円')\n",
    "        print(\"的中レース\",wide_list)\n",
    "        \n",
    "class LearnLGBM():\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def get_train_data(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff45a5-ed87-4f2c-9c50-8059c145f8de",
   "metadata": {},
   "source": [
    "回収率 \\\n",
    "(profit - real_race_len*bet) /real_race_len * bet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e17aadb6-decc-4070-b4d2-b97c41ddd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = return_tables.loc[202001010101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bb9255aa-35f1-4d67-b6d4-e5c36ad57631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(i) for i in df_[df_[0]=='三連複'][1].values[0].replace(' ','').split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b5e65fa1-890f-4b24-b980-73bfe9bf5366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9060', '9810']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'9,060br9,810'.replace(',','').split('br')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3aca7555-19cf-4624-83c7-49ee254668dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.data_c.iloc[0]['単勝']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "79305132-3e9c-46cd-aec3-90ec2672902b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', '10', '6br12', '10', '11']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_[df_[0]=='三連単'][1].values[0].replace(' ','').split('→')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c738-f553-4488-8239-235c9808c184",
   "metadata": {},
   "source": [
    "# Simulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4c3498b-a9bf-4891-9ec4-748f1681fe24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odds 1.1\n",
      "not_bet_count 2\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 583 / 1512\n",
      "的中% : 38.56 %\n",
      "収支   : 90490.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 45.751676082611084\n",
      "odds 2.0\n",
      "not_bet_count 209\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 434 / 1305\n",
      "的中% : 33.26 %\n",
      "収支   : 87600.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 87.51412010192871\n",
      "odds 3.0\n",
      "not_bet_count 495\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 269 / 1019\n",
      "的中% : 26.40 %\n",
      "収支   : 76240.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 130.4610562324524\n",
      "odds 4.0\n",
      "not_bet_count 725\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 183 / 789\n",
      "的中% : 23.19 %\n",
      "収支   : 69810.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 174.98983216285706\n",
      "odds 5.0\n",
      "not_bet_count 905\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 125 / 609\n",
      "的中% : 20.53 %\n",
      "収支   : 62210.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 216.80688214302063\n",
      "odds 6.0\n",
      "not_bet_count 1022\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 87 / 492\n",
      "的中% : 17.68 %\n",
      "収支   : 53250.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 261.6003291606903\n",
      "odds 7.0\n",
      "not_bet_count 1113\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 62 / 401\n",
      "的中% : 15.46 %\n",
      "収支   : 46350.0 円\n",
      "的中レース : {'01': 56, '02': 57, '03': 49, '04': 56, '05': 44, '06': 55, '07': 53, '08': 51, '09': 47, '10': 44, '11': 30, '12': 43}\n",
      "time 303.6184170246124\n"
     ]
    }
   ],
   "source": [
    "sl = RankSimulater(lgb_rank)\n",
    "start_time = time.time()\n",
    "for odds in [1.1,2.0,3.0,4.0,5.0,6.0,7.0]:\n",
    "    print(\"odds\",odds)\n",
    "    sl.calc_tansho(test.fillna(0),return_tables,odds=odds)\n",
    "    print(\"time\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a5acc8c-484d-4fef-ae88-1c20180881d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "not_bet_count 0\n",
      "三連複\n",
      "的中率 : 4 / 69\n",
      "的中% : 5.80 %\n",
      "収支   : -2250.0 円\n"
     ]
    }
   ],
   "source": [
    "sl = RankSimulater(lgb_rank)\n",
    "sl.calc_sanrenpuku(r.data_c.iloc[-1000:].fillna(0),return_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a4f65-19bc-482c-be47-daeb67e6623a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2b0526b6-d15f-4e39-abaf-40aaf6cfd0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "not_bet_count 0\n",
      "三連単\n",
      "的中率 : 0 / 71\n",
      "的中% : 0.00 %\n",
      "収支   : -7100 円\n"
     ]
    }
   ],
   "source": [
    "sl = RankSimulater(lgb_rank)\n",
    "sl.calc_sanrentan(r.data_c.iloc[-1000:].fillna(0),return_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "87421b65-3751-43ca-a7cb-e5dae35ae82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = return_tables.loc[202206030211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a23be3e1-765c-4435-9355-1008834f3119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '10', '12br10', '11', '12']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_[df_[0]=='三連複'][1].values[0].replace(' ','').split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3e7693d2-fdf5-4745-b652-ef2333e0b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>単勝</td>\n",
       "      <td>4</td>\n",
       "      <td>410</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>複勝</td>\n",
       "      <td>4br15br11</td>\n",
       "      <td>120br180br110</td>\n",
       "      <td>2br3br1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>枠連</td>\n",
       "      <td>2 - 7</td>\n",
       "      <td>1300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>馬連</td>\n",
       "      <td>4 - 15</td>\n",
       "      <td>2510</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>ワイド</td>\n",
       "      <td>4 - 15br4 - 11br11 - 15</td>\n",
       "      <td>620br170br360</td>\n",
       "      <td>7br1br2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>馬単</td>\n",
       "      <td>4 → 15</td>\n",
       "      <td>4120</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>三連複</td>\n",
       "      <td>4 - 11 - 15</td>\n",
       "      <td>1060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020404</th>\n",
       "      <td>三連単</td>\n",
       "      <td>4 → 15 → 11</td>\n",
       "      <td>8530</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0                        1              2        3\n",
       "202209020404   単勝                        4            410        2\n",
       "202209020404   複勝                4br15br11  120br180br110  2br3br1\n",
       "202209020404   枠連                    2 - 7           1300        5\n",
       "202209020404   馬連                   4 - 15           2510        8\n",
       "202209020404  ワイド  4 - 15br4 - 11br11 - 15  620br170br360  7br1br2\n",
       "202209020404   馬単                   4 → 15           4120       11\n",
       "202209020404  三連複              4 - 11 - 15           1060        1\n",
       "202209020404  三連単              4 → 15 → 11           8530       17"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_tables.loc[202209020404]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920e716-5257-45d8-9831-2effe586aa2c",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cafb76f-29dc-4841-bb1b-2792c246cd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = load_csv(path_mac+'results.csv')\n",
    "horse_results = load_csv(path_mac+'horse_results.csv')\n",
    "peds = load_csv(path_mac+'peds.csv')\n",
    "# 何回やってもロードすると, nanが出る\n",
    "peds.fillna('nan',inplace=True)\n",
    "return_tables = load_csv(path_mac+'return.csv')\n",
    "return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b97e7-5e69-4334-a268-c7a57eaa10d8",
   "metadata": {},
   "source": [
    "# 日付に注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e00c431-6a37-4f54-bfd3-7a1cc01780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022/12/31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff135d8f-2a18-4583-8747-68c2938e6b79",
   "metadata": {},
   "source": [
    "# race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca02207f-1319-48d2-943f-195f395e1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 202206030101\n",
    "race_id_list = ['2022060305{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090205{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022070205{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060306{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090206{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022100204{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "\n",
    "# race_id_list += ['2022060208{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022070204{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060307{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022060308{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090207{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022090208{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022030101{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022030102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "# race_id_list += ['2022060303{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022090203{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# # race_id_list += ['2022070206{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "# race_id_list += ['2022060304{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022090204{}'.format(str(i).zfill(2)) for i in range(1,13)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ec390-bc7f-4821-8417-76382d034041",
   "metadata": {},
   "source": [
    "# Results scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131add0a-4ac5-4133-b242-3fa2109a4762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a354ce46a140e2a2c6312b2e092092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# race_id_list = results.index.astype('str')\n",
    "\n",
    "results = Results.scrape(race_id_list)\n",
    "\n",
    "results.to_csv(path_mac+'results_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ac189-894a-48ac-adb2-4e731640bc5c",
   "metadata": {},
   "source": [
    "# Horse_results scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb495b3-1044-4b5a-afdc-cdf1b60aa6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8382262b1cab4b51881a205c44632555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1744.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "horse_id_list = results['horse_id'].astype(str).unique()\n",
    "horse_results = HorseResults.scrape(horse_id_list)\n",
    "# save_path = '/Users/rince/Desktop/Horse/Data/horse_2020.csv'\n",
    "horse_results.to_csv(path_mac+'horse_results_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1569216-aa8f-40a3-8303-5d54a442b00e",
   "metadata": {},
   "source": [
    "# Peds scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d56e6b-7a60-4721-a111-892aafa204d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f09bccdecef47a3b5dbb5c949bfaf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1744.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd43d1c47d3a479e84379bbd63ffb1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1744.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "peds_2021 = Peds.scrape(horse_id_list)\n",
    "pe_2021 = Peds(peds_2021)\n",
    "pe_2021.regularize_peds()\n",
    "pe_2021.peds_re.to_csv(path_mac+'peds_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1db0da-3505-4825-8408-e7dd58205c96",
   "metadata": {},
   "source": [
    "# Return scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a4d97d-94cc-477f-b643-5144c7173613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5292e06b9648fd83de7f902192fdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "returns_2021 = Return.scrape(race_id_list)\n",
    "returns_2021.to_csv(path_mac+'returns_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e175b9a-1a2d-4d5d-a5e2-e771c86dd8d2",
   "metadata": {},
   "source": [
    "# update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428fa2b8-2058-4c1d-af05-ec10e9f3768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = update_data(load_csv(path_mac+'results.csv'), load_csv(path_mac+'results_new.csv'))\n",
    "new_horse_results = update_data(load_csv(path_mac+'horse_results.csv'), load_csv(path_mac+'horse_results_new.csv'))\n",
    "new_peds = update_data(load_csv(path_mac+'peds.csv'), load_csv(path_mac+'peds_new.csv'))\n",
    "new_return = update_data(load_csv(path_mac+'return.csv'), load_csv(path_mac+'returns_new.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77364dd-545a-43d4-97aa-070ca0595356",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875dc8e7-f696-4b78-933d-4f62f3603430",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results.to_csv(path_mac2+'results.csv')\n",
    "new_horse_results.to_csv(path_mac2+'horse_results.csv')\n",
    "new_peds.to_csv(path_mac2+'peds.csv')\n",
    "new_return.to_csv(path_mac2+'return.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e35c3-7240-4c3a-9a17-e840ccfcd68b",
   "metadata": {},
   "source": [
    "# 今後の方針\n",
    "1. XGB試してみる\n",
    "2. ME 自己流につくりかえる\n",
    "3. シミュレーションとか, 自分流に変える."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274e51f-047b-4f8a-903d-47cb0ef3f1e1",
   "metadata": {},
   "source": [
    "# rank　学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ff12d5-29ce-4ab4-8c8a-43c30d3653a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b21be002eca4ebd94e100863812a1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b7219ee14487982e783c5c2ddffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922f8c11032a4810bb23c82eb8d096b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be0452235444b888e5bec5e5b711483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# peds_id = results['horse_id'].astype(str).unique()\n",
    "# peds_tmp = Peds.scrape(peds_id)\n",
    "# new_peds = update_data(peds, peds_tmp)\n",
    "# ここで初めて学習データを作る\n",
    "date = '2022/12/31'\n",
    "# model_ft 作成　\n",
    "model_ft = ft.train_unsupervised('test.txt',dim=62,minn=2,maxn=14)\n",
    "\n",
    "pe = Peds(peds)\n",
    "# pe.regularize_peds()\n",
    "pe.vectorize(pe.peds,model_ft)\n",
    "\n",
    "\n",
    "# pe.categorize()\n",
    "r = Results(results)\n",
    "#前処理\n",
    "r.preprocessing()\n",
    "#馬の過去成績データ追加\n",
    "# 過去聖遺跡データも最新にupdateする\n",
    "# horse_id_list = data['horse_id'].astype(str).unique()\n",
    "# horse_results_tmp = HorseResults.scrape(horse_id_list)\n",
    "# new_horse_results = update_data(horse_results,horse_results_tmp)\n",
    "hr = HorseResults(horse_results)\n",
    "r.merge_horse_results(hr)\n",
    "\n",
    "r.merge_peds(pe.peds_vec)\n",
    "\n",
    "# r.merge_peds(pe.peds_cat)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "# pedsは既にカテゴリ化したdataをconcatしているので, ここでカテゴリ化せずとも良い\n",
    "r.process_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ac3a4a-1f62-4605-8edf-307aed2cc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値は 0 とした\n",
    "train, test = split_data(r.data_c.fillna(0),test_size=0.2,rank_learning=False)\n",
    "# x_train = train.drop(['rank', 'date','体重','体重変化','単勝'], axis=1)\n",
    "x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_train = train['rank']\n",
    "\n",
    "x_test = test.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "train_query = x_train.groupby(x_train.index).size()\n",
    "test_query = x_test.groupby(x_test.index).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc24cf8-2253-4a67-bfc8-2149d412700d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 45821\n",
      "[LightGBM] [Info] Number of data points in the train set: 83611, number of used features: 172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgbm_params = {\n",
    "    'lambdarank_truncation_level': 2,\n",
    "    'metric': 'ndcg',\n",
    "    'objective': 'lambdarank',\n",
    "    'ndcg_eval_at': [1,2,3],\n",
    "    'learning_rate': 0.06748036714102541,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 777\n",
    "}\n",
    "\n",
    " #学習 \n",
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "\n",
    "lgb_rank = lgb.train(\n",
    "   lgbm_params,\n",
    "   train,\n",
    "   num_boost_round=100,\n",
    "#    valid_sets=valid,\n",
    "   valid_names=['train'],\n",
    "#    early_stopping_rounds=20,\n",
    "#    verbose_eval=5\n",
    ")\n",
    "\n",
    "# early stopping -> test data ないと怒られる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b5244-fc8d-4f9d-b9cd-69b895851b73",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4c5a59d7-adb9-4e89-b317-ef9eb4ade4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>斤量</th>\n",
       "      <th>course_len</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>年齢</th>\n",
       "      <th>体重</th>\n",
       "      <th>体重変化</th>\n",
       "      <th>n_horse</th>\n",
       "      <th>...</th>\n",
       "      <th>race_type_ダート</th>\n",
       "      <th>race_type_芝</th>\n",
       "      <th>race_type_障害</th>\n",
       "      <th>ground_state_稍重</th>\n",
       "      <th>ground_state_良</th>\n",
       "      <th>ground_state_不良</th>\n",
       "      <th>ground_state_重</th>\n",
       "      <th>性_牡</th>\n",
       "      <th>性_牝</th>\n",
       "      <th>性_セ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202106040907</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12477</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>486</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202106040907</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>57.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4070</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>482</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202106040907</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11380</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>480</td>\n",
       "      <td>-2</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202106040907</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8542</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>496</td>\n",
       "      <td>-12</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202106040907</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7064</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>438</td>\n",
       "      <td>-14</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206030408</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9517</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>462</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206030408</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6192</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>506</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206030408</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3743</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "      <td>456</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206030408</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3605</td>\n",
       "      <td>58</td>\n",
       "      <td>6</td>\n",
       "      <td>432</td>\n",
       "      <td>-2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206030408</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8432</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>422</td>\n",
       "      <td>-2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20599 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              枠番  馬番    斤量  course_len horse_id jockey_id  年齢   体重  体重変化  \\\n",
       "202106040907   2   2  54.0        22.0    12477       120   3  486     2   \n",
       "202106040907   6   6  57.0        22.0     4070        74   4  482     0   \n",
       "202106040907   8   8  54.0        22.0    11380       161   3  480    -2   \n",
       "202106040907   3   3  54.0        22.0     8542         8   3  496   -12   \n",
       "202106040907   4   4  57.0        22.0     7064        61   4  438   -14   \n",
       "...           ..  ..   ...         ...      ...       ...  ..  ...   ...   \n",
       "202206030408   5   7  52.0        18.0     9517       132   4  462     6   \n",
       "202206030408   1   1  57.0        18.0     6192         1   5  506     8   \n",
       "202206030408   7  10  52.0        18.0     3743       126   6  456     8   \n",
       "202206030408   6   9  55.0        18.0     3605        58   6  432    -2   \n",
       "202206030408   2   2  52.0        18.0     8432       133   4  422    -2   \n",
       "\n",
       "              n_horse  ...  race_type_ダート  race_type_芝  race_type_障害  \\\n",
       "202106040907        9  ...              0            1             0   \n",
       "202106040907        9  ...              0            1             0   \n",
       "202106040907        9  ...              0            1             0   \n",
       "202106040907        9  ...              0            1             0   \n",
       "202106040907        9  ...              0            1             0   \n",
       "...               ...  ...            ...          ...           ...   \n",
       "202206030408       13  ...              1            0             0   \n",
       "202206030408       13  ...              1            0             0   \n",
       "202206030408       13  ...              1            0             0   \n",
       "202206030408       13  ...              1            0             0   \n",
       "202206030408       13  ...              1            0             0   \n",
       "\n",
       "              ground_state_稍重  ground_state_良  ground_state_不良  \\\n",
       "202106040907                1               0                0   \n",
       "202106040907                1               0                0   \n",
       "202106040907                1               0                0   \n",
       "202106040907                1               0                0   \n",
       "202106040907                1               0                0   \n",
       "...                       ...             ...              ...   \n",
       "202206030408                1               0                0   \n",
       "202206030408                1               0                0   \n",
       "202206030408                1               0                0   \n",
       "202206030408                1               0                0   \n",
       "202206030408                1               0                0   \n",
       "\n",
       "              ground_state_重  性_牡  性_牝  性_セ  \n",
       "202106040907               0    1    0    0  \n",
       "202106040907               0    1    0    0  \n",
       "202106040907               0    1    0    0  \n",
       "202106040907               0    1    0    0  \n",
       "202106040907               0    1    0    0  \n",
       "...                      ...  ...  ...  ...  \n",
       "202206030408               0    0    1    0  \n",
       "202206030408               0    0    0    1  \n",
       "202206030408               0    0    1    0  \n",
       "202206030408               0    0    1    0  \n",
       "202206030408               0    0    1    0  \n",
       "\n",
       "[20599 rows x 172 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fa0af40b-fb3b-42b6-a50e-0943959c14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test.iloc[:10599], y_test.iloc[:10599], reference=train, group=test_query)\n",
    "trials = Trials()\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebee6f9a-20ac-4523-adb6-378f9fcfcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    print(\"Training start:\")\n",
    "\n",
    "    N_boost_round = []\n",
    "    Score = []\n",
    "\n",
    "    lgb_results={}  #履歴格納用\n",
    "    train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "    valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "\n",
    "    \n",
    "    lgb_clf = lgb.train(\n",
    "       params,\n",
    "       train,\n",
    "       num_boost_round=1000,\n",
    "       valid_sets=valid,\n",
    "       valid_names=['valid'],\n",
    "       early_stopping_rounds=20,\n",
    "       verbose_eval=5,\n",
    "       evals_result=lgb_results\n",
    "    )\n",
    "#     return lgb_results\n",
    "    return {'loss': -1.0 * lgb_results['valid']['ndcg@3'][lgb_clf.best_iteration], 'status': STATUS_OK}\n",
    "\n",
    "def optimize(trials):\n",
    "#探索スペース\n",
    "    space = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1,2,3],\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "#         なぜか, uniformだと strに変換されてしまう\n",
    "#         lambda_rank_truncation_levelは int型\n",
    "#         よって, int以外はstrに勝手に変換されてしまい, エラーとなったのではないか\n",
    "        'lambdarank_truncation_level': hp.choice('lambdarank_truncation_level',[ 1,2\n",
    "                                                                                ,4,6,8,10]),\n",
    "#         best paramsの返り値は, choiceだとindexか？\n",
    "#         n_estimaterとか サーチしてみたい\n",
    "#         'n_estimators': hp.choice('n_estimators',[ 1,10,100,500,750]),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 777,\n",
    "    }\n",
    "\n",
    "    max_evals = 50      #探索回数(25くらいで十分)\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "    print(\"best parameters:\", best)\n",
    "\n",
    "#     return {'loss': -1.0 * lgb_results['ndcg@3'][lgb_clf.best_iteration], 'status': STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6985907-c8c8-426d-be48-086c7ba1af64",
   "metadata": {},
   "source": [
    "# milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67532ce2-4766-485a-b767-2d20e84ebc74",
   "metadata": {},
   "source": [
    "# 実際に予測するときの手順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd8daf5-2a98-4901-aaec-a88887b81c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "race_id_list = ['2022050201{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090209{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022030103{}'.format(str(i).zfill(2)) for i in range(1,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a13685f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd4573f20b04a4cb59da6a241ff4b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data =  ShutubaTable.scrape(race_id_list, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d2f548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopeds_id_list = []\n",
    "for ind in data['horse_id'].astype(int).unique():\n",
    "    if ind not in peds.index:\n",
    "        nopeds_id_list.append(str(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aafaaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2077d39dfe4267b77216a179e94f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34121a3e9b8948e88c07b4900e115afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17475.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pe finish\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6926f36433f4fddb85593038ec07116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=503.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4df312e7244867a4a27816a816d7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f959b3a4534b4cb4751bbd7d7bdbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78151d2d2b684a049129aa660909d323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# peds_id = data['horse_id'].astype(str).unique( peds_tmp = Peds.scrape(peds_id)\n",
    "peds_tmp = Peds.scrape(nopeds_id_list)\n",
    "new_peds = update_data(peds, peds_tmp)\n",
    "# ここで初めて学習データを作る\n",
    "date = '2022/12/31'\n",
    "# model_ft 作成　\n",
    "model_ft = ft.train_unsupervised('test.txt',dim=62,minn=2,maxn=14)\n",
    "\n",
    "pe = Peds(new_peds)\n",
    "# pe.regularize_peds()\n",
    "pe.vectorize(pe.peds,model_ft)\n",
    "print(\"pe finish\")\n",
    "\n",
    "# pe.categorize()\n",
    "r = Results(results)\n",
    "#前処理\n",
    "r.preprocessing()\n",
    "#馬の過去成績データ追加\n",
    "# 過去聖遺跡データも最新にupdateする\n",
    "horse_id_list = data['horse_id'].astype(str).unique()\n",
    "horse_results_tmp = HorseResults.scrape(horse_id_list)\n",
    "new_horse_results = update_data(horse_results,horse_results_tmp)\n",
    "hr = HorseResults(horse_results)\n",
    "r.merge_horse_results(hr)\n",
    "\n",
    "r.merge_peds(pe.peds_vec)\n",
    "\n",
    "# r.merge_peds(pe.peds_cat)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "# pedsは既にカテゴリ化したdataをconcatしているので, ここでカテゴリ化せずとも良い\n",
    "r.process_categorical()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e023bb",
   "metadata": {},
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eca46264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:                                       \n",
      "[LightGBM] [Warning]                                  \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.063405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                     \n",
      "Total Bins 45821                                      \n",
      "[LightGBM] [Info]                                     \n",
      "Number of data points in the train set: 83611, number of used features: 172\n",
      "  0%|          | 0/50 [00:01<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rince/Library/Python/3.7/lib/python/site-packages/lightgbm/basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
      "\n",
      "/Users/rince/Library/Python/3.7/lib/python/site-packages/lightgbm/basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
      "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[5]\tvalid's ndcg@1: 0.505945\tvalid's ndcg@2: 0.500469\tvalid's ndcg@3: 0.488679\n",
      "[10]\tvalid's ndcg@1: 0.507266\tvalid's ndcg@2: 0.50144\tvalid's ndcg@3: 0.491415\n",
      "[15]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.501877\tvalid's ndcg@3: 0.493117\n",
      "[20]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.505246\tvalid's ndcg@3: 0.496331\n",
      "Early stopping, best iteration is:                    \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.062858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.505284\tvalid's ndcg@2: 0.500131\tvalid's ndcg@3: 0.490005    \n",
      "[10]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.502896\tvalid's ndcg@3: 0.49155    \n",
      "[15]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.501308\tvalid's ndcg@3: 0.493678   \n",
      "[20]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.502211\tvalid's ndcg@3: 0.493206   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.065950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.510568\tvalid's ndcg@2: 0.496889\tvalid's ndcg@3: 0.490177    \n",
      "[10]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.504012\tvalid's ndcg@3: 0.493606   \n",
      "[15]\tvalid's ndcg@1: 0.519155\tvalid's ndcg@2: 0.503714\tvalid's ndcg@3: 0.497652   \n",
      "[20]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.50621\tvalid's ndcg@3: 0.498569    \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.116118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.498976\tvalid's ndcg@3: 0.492893    \n",
      "[10]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.503427\tvalid's ndcg@3: 0.494402   \n",
      "[15]\tvalid's ndcg@1: 0.523118\tvalid's ndcg@2: 0.503911\tvalid's ndcg@3: 0.497161   \n",
      "[20]\tvalid's ndcg@1: 0.532365\tvalid's ndcg@2: 0.509231\tvalid's ndcg@3: 0.503166   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.016827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.505706\tvalid's ndcg@3: 0.495078    \n",
      "[10]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.504591\tvalid's ndcg@3: 0.500231   \n",
      "[15]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.508723\tvalid's ndcg@3: 0.502342   \n",
      "[20]\tvalid's ndcg@1: 0.521136\tvalid's ndcg@2: 0.509689\tvalid's ndcg@3: 0.501236   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.062167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.492662\tvalid's ndcg@3: 0.488577    \n",
      "[10]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.497364\tvalid's ndcg@3: 0.490553    \n",
      "[15]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.497845\tvalid's ndcg@3: 0.49362    \n",
      "[20]\tvalid's ndcg@1: 0.519155\tvalid's ndcg@2: 0.50518\tvalid's ndcg@3: 0.494553    \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.091718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.502366\tvalid's ndcg@3: 0.493504    \n",
      "[10]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.505958\tvalid's ndcg@3: 0.496668    \n",
      "[15]\tvalid's ndcg@1: 0.511229\tvalid's ndcg@2: 0.507192\tvalid's ndcg@3: 0.498303   \n",
      "[20]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.503607\tvalid's ndcg@3: 0.494772   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.076928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.502236\tvalid's ndcg@3: 0.495093    \n",
      "[10]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.507464\tvalid's ndcg@3: 0.496467   \n",
      "[15]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.50573\tvalid's ndcg@3: 0.497272     \n",
      "[20]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.506205\tvalid's ndcg@3: 0.497365   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.070736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.508561\tvalid's ndcg@3: 0.497358    \n",
      "[10]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.513774\tvalid's ndcg@3: 0.50196    \n",
      "[15]\tvalid's ndcg@1: 0.52576\tvalid's ndcg@2: 0.511569\tvalid's ndcg@3: 0.500288    \n",
      "[20]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.509028\tvalid's ndcg@3: 0.502171   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.497051\tvalid's ndcg@3: 0.491143    \n",
      "[10]\tvalid's ndcg@1: 0.525099\tvalid's ndcg@2: 0.504426\tvalid's ndcg@3: 0.497784   \n",
      "[15]\tvalid's ndcg@1: 0.510568\tvalid's ndcg@2: 0.500277\tvalid's ndcg@3: 0.496941   \n",
      "[20]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.504708\tvalid's ndcg@3: 0.498536    \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.060073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.500435\tvalid's ndcg@3: 0.489611     \n",
      "[10]\tvalid's ndcg@1: 0.510568\tvalid's ndcg@2: 0.500883\tvalid's ndcg@3: 0.489672    \n",
      "[15]\tvalid's ndcg@1: 0.505945\tvalid's ndcg@2: 0.49747\tvalid's ndcg@3: 0.493133     \n",
      "[20]\tvalid's ndcg@1: 0.511229\tvalid's ndcg@2: 0.501732\tvalid's ndcg@3: 0.495535    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.059314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.504452\tvalid's ndcg@3: 0.492973      \n",
      "[10]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.504103\tvalid's ndcg@3: 0.493534     \n",
      "[15]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.504625\tvalid's ndcg@3: 0.499671     \n",
      "[20]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.505494\tvalid's ndcg@3: 0.495868    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.059721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.503303\tvalid's ndcg@2: 0.500072\tvalid's ndcg@3: 0.492531     \n",
      "[10]\tvalid's ndcg@1: 0.511229\tvalid's ndcg@2: 0.501893\tvalid's ndcg@3: 0.496837    \n",
      "[15]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.501206\tvalid's ndcg@3: 0.495111    \n",
      "[20]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.504815\tvalid's ndcg@3: 0.499912    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.124058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.503303\tvalid's ndcg@2: 0.503838\tvalid's ndcg@3: 0.492178     \n",
      "[10]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.50078\tvalid's ndcg@3: 0.49283      \n",
      "[15]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.503607\tvalid's ndcg@3: 0.497198    \n",
      "[20]\tvalid's ndcg@1: 0.511229\tvalid's ndcg@2: 0.503615\tvalid's ndcg@3: 0.495179    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.064903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.497203\tvalid's ndcg@3: 0.486521     \n",
      "[10]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.50183\tvalid's ndcg@3: 0.493617      \n",
      "[15]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.506111\tvalid's ndcg@3: 0.497223    \n",
      "[20]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.507723\tvalid's ndcg@3: 0.495853    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.072753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.499506\tvalid's ndcg@3: 0.488573     \n",
      "[10]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.503701\tvalid's ndcg@3: 0.491525    \n",
      "[15]\tvalid's ndcg@1: 0.515852\tvalid's ndcg@2: 0.505172\tvalid's ndcg@3: 0.494866    \n",
      "[20]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.503957\tvalid's ndcg@3: 0.493336    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.067356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.507044\tvalid's ndcg@3: 0.49208      \n",
      "[10]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.505113\tvalid's ndcg@3: 0.495569    \n",
      "[15]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.505419\tvalid's ndcg@3: 0.495171     \n",
      "[20]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.506317\tvalid's ndcg@3: 0.49892     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.058961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.501407\tvalid's ndcg@3: 0.492781     \n",
      "[10]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.50393\tvalid's ndcg@3: 0.493228     \n",
      "[15]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.504697\tvalid's ndcg@3: 0.493877    \n",
      "[20]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.505325\tvalid's ndcg@3: 0.493887     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.061233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.49964\tvalid's ndcg@3: 0.488159     \n",
      "[10]\tvalid's ndcg@1: 0.501321\tvalid's ndcg@2: 0.49403\tvalid's ndcg@3: 0.489041    \n",
      "[15]\tvalid's ndcg@1: 0.521136\tvalid's ndcg@2: 0.503946\tvalid's ndcg@3: 0.497286   \n",
      "[20]\tvalid's ndcg@1: 0.525099\tvalid's ndcg@2: 0.505515\tvalid's ndcg@3: 0.50088    \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.132820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.501879\tvalid's ndcg@3: 0.493233    \n",
      "[10]\tvalid's ndcg@1: 0.525099\tvalid's ndcg@2: 0.507198\tvalid's ndcg@3: 0.498354   \n",
      "[15]\tvalid's ndcg@1: 0.521136\tvalid's ndcg@2: 0.509595\tvalid's ndcg@3: 0.49851    \n",
      "[20]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.509957\tvalid's ndcg@3: 0.498539   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.506191\tvalid's ndcg@3: 0.491899    \n",
      "[10]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.509045\tvalid's ndcg@3: 0.496722   \n",
      "[15]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.511332\tvalid's ndcg@3: 0.499662   \n",
      "[20]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.505332\tvalid's ndcg@3: 0.497802   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.497471\tvalid's ndcg@3: 0.492154    \n",
      "[10]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.501111\tvalid's ndcg@3: 0.492817   \n",
      "[15]\tvalid's ndcg@1: 0.511229\tvalid's ndcg@2: 0.497616\tvalid's ndcg@3: 0.494206   \n",
      "[20]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.50406\tvalid's ndcg@3: 0.499795    \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.017125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.505577\tvalid's ndcg@3: 0.493607    \n",
      "[10]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.500658\tvalid's ndcg@3: 0.494197   \n",
      "[15]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.499825\tvalid's ndcg@3: 0.495209   \n",
      "[20]\tvalid's ndcg@1: 0.505945\tvalid's ndcg@2: 0.503091\tvalid's ndcg@3: 0.497073   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.065853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.505165\tvalid's ndcg@3: 0.496263    \n",
      "[10]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.507724\tvalid's ndcg@3: 0.496498   \n",
      "[15]\tvalid's ndcg@1: 0.52576\tvalid's ndcg@2: 0.511219\tvalid's ndcg@3: 0.500341    \n",
      "[20]\tvalid's ndcg@1: 0.523778\tvalid's ndcg@2: 0.510543\tvalid's ndcg@3: 0.499154   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                   \n",
      "[LightGBM] [Warning]                                                              \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.072185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                 \n",
      "Total Bins 45821                                                                  \n",
      "[LightGBM] [Info]                                                                 \n",
      "Number of data points in the train set: 83611, number of used features: 172       \n",
      "Training until validation scores don't improve for 20 rounds                      \n",
      "[5]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.494062\tvalid's ndcg@3: 0.491044    \n",
      "[10]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.491031\tvalid's ndcg@3: 0.489713   \n",
      "[15]\tvalid's ndcg@1: 0.506605\tvalid's ndcg@2: 0.491286\tvalid's ndcg@3: 0.490731   \n",
      "[20]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.494117\tvalid's ndcg@3: 0.490967   \n",
      "Early stopping, best iteration is:                                                \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.495674\tvalid's ndcg@3: 0.493666     \n",
      "[10]\tvalid's ndcg@1: 0.507266\tvalid's ndcg@2: 0.493519\tvalid's ndcg@3: 0.488212    \n",
      "[15]\tvalid's ndcg@1: 0.510568\tvalid's ndcg@2: 0.498638\tvalid's ndcg@3: 0.490334    \n",
      "[20]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.500093\tvalid's ndcg@3: 0.495174    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.075298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.497758\tvalid's ndcg@3: 0.493796     \n",
      "[10]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.497341\tvalid's ndcg@3: 0.495657    \n",
      "[15]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.494561\tvalid's ndcg@3: 0.494349    \n",
      "[20]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.499676\tvalid's ndcg@3: 0.495585    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.016284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.502849\tvalid's ndcg@3: 0.493226     \n",
      "[10]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.504051\tvalid's ndcg@3: 0.49603     \n",
      "[15]\tvalid's ndcg@1: 0.527081\tvalid's ndcg@2: 0.507713\tvalid's ndcg@3: 0.501268    \n",
      "[20]\tvalid's ndcg@1: 0.525099\tvalid's ndcg@2: 0.513075\tvalid's ndcg@3: 0.504293    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.501662\tvalid's ndcg@3: 0.492821     \n",
      "[10]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.502354\tvalid's ndcg@3: 0.493749    \n",
      "[15]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.500093\tvalid's ndcg@3: 0.489611    \n",
      "[20]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.502259\tvalid's ndcg@3: 0.491044     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.500635\tvalid's ndcg@3: 0.493724     \n",
      "[10]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.494828\tvalid's ndcg@3: 0.491212    \n",
      "[15]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.503022\tvalid's ndcg@3: 0.494638    \n",
      "[20]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.504629\tvalid's ndcg@3: 0.496681    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.014691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.505945\tvalid's ndcg@2: 0.49642\tvalid's ndcg@3: 0.489226      \n",
      "[10]\tvalid's ndcg@1: 0.524439\tvalid's ndcg@2: 0.502999\tvalid's ndcg@3: 0.498078    \n",
      "[15]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.505581\tvalid's ndcg@3: 0.4969      \n",
      "[20]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.506391\tvalid's ndcg@3: 0.496697    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.067501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.507926\tvalid's ndcg@2: 0.49964\tvalid's ndcg@3: 0.487874      \n",
      "[10]\tvalid's ndcg@1: 0.516513\tvalid's ndcg@2: 0.500722\tvalid's ndcg@3: 0.489921    \n",
      "[15]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.499519\tvalid's ndcg@3: 0.491772    \n",
      "[20]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.497718\tvalid's ndcg@3: 0.489902    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.074292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.498914\tvalid's ndcg@3: 0.491708     \n",
      "[10]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.505295\tvalid's ndcg@3: 0.497208    \n",
      "[15]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.505802\tvalid's ndcg@3: 0.495545    \n",
      "[20]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.503297\tvalid's ndcg@3: 0.494868     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.069102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.51321\tvalid's ndcg@2: 0.504668\tvalid's ndcg@3: 0.493746      \n",
      "[10]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.515478\tvalid's ndcg@3: 0.504623    \n",
      "[15]\tvalid's ndcg@1: 0.52642\tvalid's ndcg@2: 0.516507\tvalid's ndcg@3: 0.505035     \n",
      "[20]\tvalid's ndcg@1: 0.52576\tvalid's ndcg@2: 0.516707\tvalid's ndcg@3: 0.504836     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.067195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.502911\tvalid's ndcg@3: 0.493669     \n",
      "[10]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.504593\tvalid's ndcg@3: 0.496509    \n",
      "[15]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.510542\tvalid's ndcg@3: 0.501027    \n",
      "[20]\tvalid's ndcg@1: 0.515852\tvalid's ndcg@2: 0.509072\tvalid's ndcg@3: 0.499469    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.017178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.494337\tvalid's ndcg@3: 0.489873     \n",
      "[10]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.49955\tvalid's ndcg@3: 0.492459     \n",
      "[15]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.507047\tvalid's ndcg@3: 0.498704    \n",
      "[20]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.506859\tvalid's ndcg@3: 0.499415    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.072056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.503963\tvalid's ndcg@2: 0.497116\tvalid's ndcg@3: 0.490732     \n",
      "[10]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.504106\tvalid's ndcg@3: 0.498555    \n",
      "[15]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.505856\tvalid's ndcg@3: 0.496973    \n",
      "[20]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.503732\tvalid's ndcg@3: 0.496514    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.013731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.494978\tvalid's ndcg@3: 0.48839      \n",
      "[10]\tvalid's ndcg@1: 0.515852\tvalid's ndcg@2: 0.502456\tvalid's ndcg@3: 0.496237    \n",
      "[15]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.504724\tvalid's ndcg@3: 0.495179    \n",
      "[20]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.502467\tvalid's ndcg@3: 0.495032    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.064695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.500821\tvalid's ndcg@3: 0.494435     \n",
      "[10]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.502734\tvalid's ndcg@3: 0.493845    \n",
      "[15]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.503737\tvalid's ndcg@3: 0.497919    \n",
      "[20]\tvalid's ndcg@1: 0.507926\tvalid's ndcg@2: 0.50354\tvalid's ndcg@3: 0.496564     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.070109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.505945\tvalid's ndcg@2: 0.500119\tvalid's ndcg@3: 0.493593     \n",
      "[10]\tvalid's ndcg@1: 0.527081\tvalid's ndcg@2: 0.50938\tvalid's ndcg@3: 0.501181     \n",
      "[15]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.508368\tvalid's ndcg@3: 0.498002    \n",
      "[20]\tvalid's ndcg@1: 0.523778\tvalid's ndcg@2: 0.50901\tvalid's ndcg@3: 0.501787     \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.069563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.499952\tvalid's ndcg@3: 0.495286     \n",
      "[10]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.504209\tvalid's ndcg@3: 0.498454    \n",
      "[15]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.507578\tvalid's ndcg@3: 0.49843     \n",
      "[20]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.507295\tvalid's ndcg@3: 0.498966    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.520077\tvalid's ndcg@3: 0.502421\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.500541\tvalid's ndcg@3: 0.490915     \n",
      "[10]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.495856\tvalid's ndcg@3: 0.492533    \n",
      "[15]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.493638\tvalid's ndcg@3: 0.490465    \n",
      "[20]\tvalid's ndcg@1: 0.515192\tvalid's ndcg@2: 0.495663\tvalid's ndcg@3: 0.493543    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.540951\tvalid's ndcg@2: 0.517778\tvalid's ndcg@3: 0.50076\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.069934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.505284\tvalid's ndcg@2: 0.492882\tvalid's ndcg@3: 0.486789     \n",
      "[10]\tvalid's ndcg@1: 0.508587\tvalid's ndcg@2: 0.497301\tvalid's ndcg@3: 0.489239    \n",
      "[15]\tvalid's ndcg@1: 0.51255\tvalid's ndcg@2: 0.500014\tvalid's ndcg@3: 0.492998     \n",
      "[20]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.500698\tvalid's ndcg@3: 0.494607    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.535007\tvalid's ndcg@2: 0.519633\tvalid's ndcg@3: 0.502932\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.011825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.520476\tvalid's ndcg@2: 0.497558\tvalid's ndcg@3: 0.492761     \n",
      "[10]\tvalid's ndcg@1: 0.515852\tvalid's ndcg@2: 0.503761\tvalid's ndcg@3: 0.495289    \n",
      "[15]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.513318\tvalid's ndcg@3: 0.49736     \n",
      "[20]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.509639\tvalid's ndcg@3: 0.497099    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.065366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.513871\tvalid's ndcg@2: 0.501657\tvalid's ndcg@3: 0.493551     \n",
      "[10]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.500517\tvalid's ndcg@3: 0.495956    \n",
      "[15]\tvalid's ndcg@1: 0.509247\tvalid's ndcg@2: 0.504927\tvalid's ndcg@3: 0.49788     \n",
      "[20]\tvalid's ndcg@1: 0.503303\tvalid's ndcg@2: 0.503421\tvalid's ndcg@3: 0.497848    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing row-wise multi-threading, the overhead of testing was 0.012070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.509713\tvalid's ndcg@3: 0.498215     \n",
      "[10]\tvalid's ndcg@1: 0.521136\tvalid's ndcg@2: 0.510967\tvalid's ndcg@3: 0.502399    \n",
      "[15]\tvalid's ndcg@1: 0.530383\tvalid's ndcg@2: 0.513382\tvalid's ndcg@3: 0.502911    \n",
      "[20]\tvalid's ndcg@1: 0.524439\tvalid's ndcg@2: 0.512359\tvalid's ndcg@3: 0.501613    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.065967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.529062\tvalid's ndcg@2: 0.506762\tvalid's ndcg@3: 0.502206     \n",
      "[10]\tvalid's ndcg@1: 0.529062\tvalid's ndcg@2: 0.510367\tvalid's ndcg@3: 0.503717    \n",
      "[15]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.508522\tvalid's ndcg@3: 0.50128     \n",
      "[20]\tvalid's ndcg@1: 0.519815\tvalid's ndcg@2: 0.505786\tvalid's ndcg@3: 0.499989    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.068400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.519155\tvalid's ndcg@2: 0.503808\tvalid's ndcg@3: 0.496357     \n",
      "[10]\tvalid's ndcg@1: 0.511889\tvalid's ndcg@2: 0.501842\tvalid's ndcg@3: 0.497913    \n",
      "[15]\tvalid's ndcg@1: 0.518494\tvalid's ndcg@2: 0.502448\tvalid's ndcg@3: 0.497687    \n",
      "[20]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.504779\tvalid's ndcg@3: 0.496856    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.061220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.514531\tvalid's ndcg@2: 0.504173\tvalid's ndcg@3: 0.4969       \n",
      "[10]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.503871\tvalid's ndcg@3: 0.490443    \n",
      "[15]\tvalid's ndcg@1: 0.507926\tvalid's ndcg@2: 0.502746\tvalid's ndcg@3: 0.496348    \n",
      "[20]\tvalid's ndcg@1: 0.517834\tvalid's ndcg@2: 0.507126\tvalid's ndcg@3: 0.498328    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "Training start:                                                                    \n",
      "[LightGBM] [Warning]                                                               \n",
      "Auto-choosing col-wise multi-threading, the overhead of testing was 0.075142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info]                                                                  \n",
      "Total Bins 45821                                                                   \n",
      "[LightGBM] [Info]                                                                  \n",
      "Number of data points in the train set: 83611, number of used features: 172        \n",
      "Training until validation scores don't improve for 20 rounds                       \n",
      "[5]\tvalid's ndcg@1: 0.517173\tvalid's ndcg@2: 0.500777\tvalid's ndcg@3: 0.490222     \n",
      "[10]\tvalid's ndcg@1: 0.509908\tvalid's ndcg@2: 0.498461\tvalid's ndcg@3: 0.491719    \n",
      "[15]\tvalid's ndcg@1: 0.521797\tvalid's ndcg@2: 0.505495\tvalid's ndcg@3: 0.497568    \n",
      "[20]\tvalid's ndcg@1: 0.522457\tvalid's ndcg@2: 0.507339\tvalid's ndcg@3: 0.499573    \n",
      "Early stopping, best iteration is:                                                 \n",
      "[1]\tvalid's ndcg@1: 0.542933\tvalid's ndcg@2: 0.521965\tvalid's ndcg@3: 0.507486\n",
      "100%|██████████| 50/50 [02:23<00:00,  2.87s/trial, best loss: -0.49744374991711987]\n",
      "best parameters: {'lambdarank_truncation_level': 0, 'learning_rate': 0.03705171865722612}\n"
     ]
    }
   ],
   "source": [
    "# 欠損値は 0 とした\n",
    "train, test = split_data(r.data_c.fillna(0),test_size=0.2,rank_learning=False)\n",
    "# x_train = train.drop(['rank', 'date','体重','体重変化','単勝'], axis=1)\n",
    "x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_train = train['rank']\n",
    "\n",
    "x_test = test.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "train_query = x_train.groupby(x_train.index).size()\n",
    "test_query = x_test.groupby(x_test.index).size()\n",
    "\n",
    "\n",
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "trials = Trials()\n",
    "optimize(trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385f355",
   "metadata": {},
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e286b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 45821\n",
      "[LightGBM] [Info] Number of data points in the train set: 83611, number of used features: 172\n"
     ]
    }
   ],
   "source": [
    "# 'lambdarank_truncation_level': 0, 'learning_rate': 0.03705171865722612\n",
    "lgbm_params = {\n",
    "    'lambdarank_truncation_level': 2,\n",
    "    'metric': 'ndcg',\n",
    "    'objective': 'lambdarank',\n",
    "    'ndcg_eval_at': [1,2,3],\n",
    "    'learning_rate':0.03705171865722612,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 777\n",
    "}\n",
    "\n",
    " #学習 \n",
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "\n",
    "lgb_rank = lgb.train(\n",
    "   lgbm_params,\n",
    "   train,\n",
    "   num_boost_round=100,\n",
    "#    valid_sets=valid,\n",
    "#    valid_names=['train'],\n",
    "#    early_stopping_rounds=20,\n",
    "#    verbose_eval=5\n",
    ")\n",
    "\n",
    "# early stopping -> test data ないと怒られる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a38cc",
   "metadata": {},
   "source": [
    "# 福島"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "582846ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037548c25e241b0adf67a6488aaa343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca9cf7b67234af4bf9fbcfd211895e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54affec469a6487a9fa4579c2734a3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625d1ef517544f63907830b262d8337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>馬番</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.347316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.487014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.613774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.641546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>12</td>\n",
       "      <td>-1.169239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>9</td>\n",
       "      <td>-1.173335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.214738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.297549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.335746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.418716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>14</td>\n",
       "      <td>-1.469132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>15</td>\n",
       "      <td>-1.756284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.931996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>3</td>\n",
       "      <td>-2.361873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>10</td>\n",
       "      <td>-2.428898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202203010307</th>\n",
       "      <td>11</td>\n",
       "      <td>-2.428898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              馬番    scores\n",
       "202203010307  16 -0.347316\n",
       "202203010307   6 -0.487014\n",
       "202203010307   1 -0.613774\n",
       "202203010307   5 -0.641546\n",
       "202203010307  12 -1.169239\n",
       "202203010307   9 -1.173335\n",
       "202203010307  13 -1.214738\n",
       "202203010307   2 -1.297549\n",
       "202203010307   8 -1.335746\n",
       "202203010307   7 -1.418716\n",
       "202203010307  14 -1.469132\n",
       "202203010307  15 -1.756284\n",
       "202203010307   4 -1.931996\n",
       "202203010307   3 -2.361873\n",
       "202203010307  10 -2.428898\n",
       "202203010307  11 -2.428898"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 東京\n",
    "race_id = 202203010307\n",
    "# race_id_list = ['2022070102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "\n",
    "# race_id_list = ['2020010106{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "st = ShutubaTable(data)\n",
    "\n",
    "\n",
    "#前処理\n",
    "st.preprocessing()\n",
    "\n",
    "#馬の過去成績データ追加\n",
    "st.merge_horse_results(hr)\n",
    "\n",
    "#血統データ追加\n",
    "st.merge_peds(pe.peds_vec)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "# ここで初めてdeta_cができる\n",
    "st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "sl = RankSimulater(lgb_rank)\n",
    "sl.return_pred_table(st.data_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c2e39",
   "metadata": {},
   "source": [
    "# 阪神"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4782efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798a9612b6014bacbc94f8ecf0fe5de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc4454ab0944477bb39171334fe2fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b327b58e0c46499c888f5f1c77fc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cdd447c73847e0b03a658148096709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>馬番</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>10</td>\n",
       "      <td>0.413193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>2</td>\n",
       "      <td>0.066315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>15</td>\n",
       "      <td>-0.067301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.131124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.256977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.263322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.346877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>14</td>\n",
       "      <td>-0.393592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.808658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.851345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.894834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.964796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>11</td>\n",
       "      <td>-1.040351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.052602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209020908</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.160124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              馬番    scores\n",
       "202209020908  10  0.413193\n",
       "202209020908   2  0.066315\n",
       "202209020908  15 -0.067301\n",
       "202209020908   1 -0.131124\n",
       "202209020908   4 -0.256977\n",
       "202209020908  12 -0.263322\n",
       "202209020908   5 -0.346877\n",
       "202209020908  14 -0.393592\n",
       "202209020908   8 -0.808658\n",
       "202209020908  13 -0.851345\n",
       "202209020908   7 -0.894834\n",
       "202209020908   9 -0.964796\n",
       "202209020908  11 -1.040351\n",
       "202209020908   6 -1.052602\n",
       "202209020908   3 -1.160124"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 東京\n",
    "race_id = 202209020908\n",
    "# race_id_list = ['2022070102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "\n",
    "# race_id_list = ['2020010106{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "st = ShutubaTable(data)\n",
    "\n",
    "\n",
    "#前処理\n",
    "st.preprocessing()\n",
    "\n",
    "#馬の過去成績データ追加\n",
    "st.merge_horse_results(hr)\n",
    "\n",
    "#血統データ追加\n",
    "st.merge_peds(pe.peds_vec)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "# ここで初めてdeta_cができる\n",
    "st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "sl = RankSimulater(lgb_rank)\n",
    "sl.return_pred_table(st.data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc00d0",
   "metadata": {},
   "source": [
    "# 東京"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d7091fe-528d-4964-91e1-8adb4dcda22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3352e351e12c4466bf19829d765ea7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc71559b637a4eb18efe2a3cd49ec7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bf906238594fea8b882507a678b63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e3b7af09b49cca95647282a0823c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>馬番</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>6</td>\n",
       "      <td>1.228955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>12</td>\n",
       "      <td>-0.028493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.326073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>9</td>\n",
       "      <td>-0.364029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.411081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.432029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.630979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.739869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.744779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>14</td>\n",
       "      <td>-0.877849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.952914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.106052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.258035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202205020108</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.451258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              馬番    scores\n",
       "202205020108   6  1.228955\n",
       "202205020108  12 -0.028493\n",
       "202205020108  11 -0.326073\n",
       "202205020108   9 -0.364029\n",
       "202205020108   2 -0.411081\n",
       "202205020108   8 -0.432029\n",
       "202205020108  10 -0.630979\n",
       "202205020108   5 -0.739869\n",
       "202205020108   4 -0.744779\n",
       "202205020108  14 -0.877849\n",
       "202205020108  13 -0.952914\n",
       "202205020108   1 -1.106052\n",
       "202205020108   7 -1.258035\n",
       "202205020108   3 -1.451258"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 東京\n",
    "race_id = 202205020108\n",
    "# race_id_list = ['2022070102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "\n",
    "# race_id_list = ['2020010106{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "st = ShutubaTable(data)\n",
    "\n",
    "\n",
    "#前処理\n",
    "st.preprocessing()\n",
    "\n",
    "#馬の過去成績データ追加\n",
    "st.merge_horse_results(hr)\n",
    "\n",
    "#血統データ追加\n",
    "st.merge_peds(pe.peds_vec)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "# ここで初めてdeta_cができる\n",
    "st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "sl = RankSimulater(lgb_rank)\n",
    "sl.return_pred_table(st.data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0ca11-eae4-4d5e-b833-3184f6e85f85",
   "metadata": {},
   "source": [
    "# 重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c64f0ed3-8651-4105-aa14-83bfa45b20c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               features  importances\n",
      "4              horse_id         2172\n",
      "5             jockey_id          407\n",
      "25      賞金_race_type_5R           81\n",
      "52      着順_race_type_9R           42\n",
      "11                賞金_5R           27\n",
      "10                着順_5R           26\n",
      "7                    体重           25\n",
      "24      着順_race_type_5R           23\n",
      "6                    年齢           23\n",
      "32             賞金_開催_5R           21\n",
      "38                着順_9R           18\n",
      "80    着順_race_type_allR           17\n",
      "53      賞金_race_type_9R           13\n",
      "40                着差_9R           10\n",
      "9               n_horse           10\n",
      "60             賞金_開催_9R            9\n",
      "12                着差_5R            9\n",
      "45     着順_course_len_9R            7\n",
      "73   着順_course_len_allR            7\n",
      "18     賞金_course_len_5R            6\n",
      "26      着差_race_type_5R            5\n",
      "74   賞金_course_len_allR            4\n",
      "68              着差_allR            4\n",
      "8                  体重変化            4\n",
      "46     賞金_course_len_9R            3\n",
      "39                賞金_9R            3\n",
      "17     着順_course_len_5R            3\n",
      "75   着差_course_len_allR            2\n",
      "95               peds_1            2\n",
      "103              peds_9            2\n"
     ]
    }
   ],
   "source": [
    "importances = pd.DataFrame(\n",
    "{'features' : x_train.columns, 'importances' : lgb_rank.feature_importance()})\n",
    "print(importances.sort_values('importances', ascending=False)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb716e-ef27-4c7b-8854-c68efd89d7be",
   "metadata": {},
   "source": [
    "# Rank Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19d8dbd3-4a97-47d0-91c9-e2001aafcb56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc154ae72c4d4c3a8b5e9c0be20e2fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a919f5a02fb449ab9b398e12b165e096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75679b239f0a47e8a8b786006cd00e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a9e7af58ca425192a7c69dd96f3bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35432dbbea7844abadac9918cdb0aa1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020101  14 -0.309343\n",
      "202205020101  16 -0.509149\n",
      "202205020101  15 -0.830221\n",
      "actual\n",
      "                0                  1                    2             3\n",
      "202205020101   単勝                 16                 370円           2人気\n",
      "202205020101   複勝           16 15 10     230円1,000円3,750円    2人気8人気12人気\n",
      "202205020101   枠連                8 8               5,060円          12人気\n",
      "202205020101   馬連              15 16               3,410円          12人気\n",
      "202205020101  ワイド  15 16 10 16 10 15  1,080円4,740円23,590円  12人気35人気74人気\n",
      "202205020101   馬単              16 15               5,530円          18人気\n",
      "202205020101  3連複           10 15 16              82,770円         124人気\n",
      "202205020101  3連単           16 15 10             261,380円         461人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020102   1 -0.035508\n",
      "202205020102   9 -0.118165\n",
      "202205020102   6 -0.525759\n",
      "actual\n",
      "                0              1             2           3\n",
      "202205020102   単勝              6        1,140円         4人気\n",
      "202205020102   複勝         6 9 12  200円110円170円   4人気2人気3人気\n",
      "202205020102   枠連            3 5          790円         3人気\n",
      "202205020102   馬連            6 9          950円         3人気\n",
      "202205020102  ワイド  6 9 6 12 9 12  400円930円360円  3人気12人気2人気\n",
      "202205020102   馬単            6 9        2,770円        10人気\n",
      "202205020102  3連複         6 9 12        1,890円         4人気\n",
      "202205020102  3連単         6 9 12       15,480円        44人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020103   8 -0.066142\n",
      "202205020103  11 -0.394158\n",
      "202205020103  15 -0.452027\n",
      "actual\n",
      "                0              1             2          3\n",
      "202205020103   単勝              5          970円        4人気\n",
      "202205020103   複勝         5 11 8  160円140円110円  4人気2人気1人気\n",
      "202205020103   枠連            3 6        1,160円        4人気\n",
      "202205020103   馬連           5 11        2,030円        8人気\n",
      "202205020103  ワイド  5 11 5 8 8 11  610円290円230円  8人気3人気1人気\n",
      "202205020103   馬単           5 11        4,990円       18人気\n",
      "202205020103  3連複         5 8 11        1,100円        2人気\n",
      "202205020103  3連単         5 11 8       12,810円       42人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020104   9 -0.112690\n",
      "202205020104   8 -0.361525\n",
      "202205020104  11 -0.751871\n",
      "actual\n",
      "                0                1               2           3\n",
      "202205020104   単勝                9            130円         1人気\n",
      "202205020104   複勝          9 16 11    110円350円290円   1人気6人気5人気\n",
      "202205020104   枠連              5 8            430円         2人気\n",
      "202205020104   馬連             9 16          1,070円         5人気\n",
      "202205020104  ワイド  9 16 9 11 11 16  480円480円2,280円  4人気5人気21人気\n",
      "202205020104   馬単             9 16          1,570円         6人気\n",
      "202205020104  3連複          9 11 16          3,990円        13人気\n",
      "202205020104  3連単          9 16 11         12,010円        36人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020105  10 -0.045440\n",
      "202205020105  14 -0.064320\n",
      "202205020105  18 -0.296185\n",
      "actual\n",
      "                0                  1                 2            3\n",
      "202205020105   単勝                 10              250円          1人気\n",
      "202205020105   複勝           10 14 12      140円120円510円    2人気1人気8人気\n",
      "202205020105   枠連                5 7              290円          1人気\n",
      "202205020105   馬連              10 14              300円          1人気\n",
      "202205020105  ワイド  10 14 10 12 12 14  190円1,070円1,080円  1人気13人気14人気\n",
      "202205020105   馬単              10 14              650円          2人気\n",
      "202205020105  3連複           10 12 14            2,760円          6人気\n",
      "202205020105  3連単           10 14 12            8,710円         17人気\n",
      "profit 190\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020106   4 -0.075993\n",
      "202205020106   7 -0.172997\n",
      "202205020106   1 -0.252366\n",
      "actual\n",
      "                0                1                 2            3\n",
      "202205020106   単勝               16              300円          1人気\n",
      "202205020106   複勝          16 11 4      160円490円200円    2人気8人気3人気\n",
      "202205020106   枠連              6 8              950円          3人気\n",
      "202205020106   馬連            11 16            4,440円         18人気\n",
      "202205020106  ワイド  11 16 4 16 4 11  1,420円440円2,350円  18人気2人気26人気\n",
      "202205020106   馬単            16 11            6,880円         30人気\n",
      "202205020106  3連複          4 11 16            7,960円         28人気\n",
      "202205020106  3連単          16 11 4           43,400円        149人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020107  14  0.022092\n",
      "202205020107  16 -0.124502\n",
      "202205020107   7 -0.164419\n",
      "actual\n",
      "                0              1                     2             3\n",
      "202205020107   単勝             14                5,840円          12人気\n",
      "202205020107   複勝         14 1 8        1,440円470円480円    12人気6人気8人気\n",
      "202205020107   枠連            1 7               12,950円          27人気\n",
      "202205020107   馬連           1 14               54,220円          78人気\n",
      "202205020107  ワイド  1 14 8 14 1 8  11,350円12,130円2,990円  69人気71人気32人気\n",
      "202205020107   馬単           14 1               95,800円         147人気\n",
      "202205020107  3連複         1 8 14              242,220円         282人気\n",
      "202205020107  3連単         14 1 8            1,460,720円       1,677人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020108   6  1.228955\n",
      "202205020108  12 -0.028493\n",
      "202205020108  11 -0.326073\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202205020108   単勝              1            4,200円          7人気\n",
      "202205020108   複勝         1 12 6      720円190円150円    9人気4人気1人気\n",
      "202205020108   枠連            1 7            4,660円         11人気\n",
      "202205020108   馬連           1 12           14,640円         38人気\n",
      "202205020108  ワイド  1 12 1 6 6 12  3,600円1,820円410円  40人気15人気3人気\n",
      "202205020108   馬単           1 12           35,720円         81人気\n",
      "202205020108  3連複         1 6 12           12,460円         41人気\n",
      "202205020108  3連単         1 12 6          156,570円        394人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020109   7 -0.490270\n",
      "202205020109   3 -0.493196\n",
      "202205020109   6 -0.502763\n",
      "actual\n",
      "                0            1                 2            3\n",
      "202205020109   単勝            6              490円          3人気\n",
      "202205020109   複勝        6 5 7      150円390円200円    2人気6人気5人気\n",
      "202205020109   枠連          5 6            3,470円         13人気\n",
      "202205020109   馬連          5 6            3,150円         12人気\n",
      "202205020109  ワイド  5 6 6 7 5 7  1,000円450円1,340円  12人気6人気19人気\n",
      "202205020109   馬単          6 5            6,380円         28人気\n",
      "202205020109  3連複        5 6 7            4,830円         19人気\n",
      "202205020109  3連単        6 5 7           30,240円        121人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020110  11  1.279207\n",
      "202205020110   9  0.886995\n",
      "202205020110   3  0.867733\n",
      "actual\n",
      "                0              1               2            3\n",
      "202205020110   単勝             11            770円          5人気\n",
      "202205020110   複勝         11 4 7    300円230円220円    6人気4人気3人気\n",
      "202205020110   枠連            2 6          2,280円         12人気\n",
      "202205020110   馬連           4 11          4,030円         17人気\n",
      "202205020110  ワイド  4 11 7 11 4 7  1,340円960円840円  15人気10人気6人気\n",
      "202205020110   馬単           11 4          7,240円         32人気\n",
      "202205020110  3連複         4 7 11          7,490円         20人気\n",
      "202205020110  3連単         11 4 7         49,520円        155人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020111   7  1.305018\n",
      "202205020111  14  1.216018\n",
      "202205020111   8  0.902979\n",
      "actual\n",
      "                0            1             2           3\n",
      "202205020111   単勝            6          520円         2人気\n",
      "202205020111   複勝        6 3 7  170円240円230円   1人気6人気4人気\n",
      "202205020111   枠連          2 3        1,810円        10人気\n",
      "202205020111   馬連          3 6        1,830円         5人気\n",
      "202205020111  ワイド  3 6 6 7 3 7  670円590円950円  4人気3人気11人気\n",
      "202205020111   馬単          6 3        3,380円         4人気\n",
      "202205020111  3連複        3 6 7        3,720円         6人気\n",
      "202205020111  3連単        6 3 7       18,480円        17人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205020112  12  1.949129\n",
      "202205020112   9 -0.026419\n",
      "202205020112   7 -0.114620\n",
      "actual\n",
      "                0                1             2          3\n",
      "202205020112   単勝                7          490円        1人気\n",
      "202205020112   複勝          7 13 16  220円210円280円  4人気3人気5人気\n",
      "202205020112   枠連              4 7        1,130円        3人気\n",
      "202205020112   馬連             7 13        1,640円        5人気\n",
      "202205020112  ワイド  7 13 7 16 13 16  690円900円930円  6人気8人気9人気\n",
      "202205020112   馬単             7 13        3,270円       11人気\n",
      "202205020112  3連複          7 13 16        4,060円        5人気\n",
      "202205020112  3連単          7 13 16       21,790円       30人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020901   1 -0.042015\n",
      "202209020901  12 -0.104164\n",
      "202209020901  11 -0.248484\n",
      "actual\n",
      "                0                  1               2           3\n",
      "202209020901   単勝                 14          1,310円         5人気\n",
      "202209020901   複勝           14 12 16    220円110円210円   5人気1人気4人気\n",
      "202209020901   枠連                6 7            760円         5人気\n",
      "202209020901   馬連              12 14          1,040円         5人気\n",
      "202209020901  ワイド  12 14 14 16 12 16  370円1,450円370円  4人気16人気3人気\n",
      "202209020901   馬単              14 12          2,970円        10人気\n",
      "202209020901  3連複           12 14 16          2,320円         7人気\n",
      "202209020901  3連単           14 12 16         22,550円        73人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020902   7 -0.022631\n",
      "202209020902   1 -0.099130\n",
      "202209020902   9 -1.297549\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209020902   単勝            8          570円        3人気\n",
      "202209020902   複勝        8 7 1  120円100円100円  3人気2人気1人気\n",
      "202209020902   枠連          6 6          410円        2人気\n",
      "202209020902   馬連          7 8          420円        2人気\n",
      "202209020902  ワイド  7 8 1 8 1 7  160円210円110円  2人気3人気1人気\n",
      "202209020902   馬単          8 7        1,040円        4人気\n",
      "202209020902  3連複        1 7 8          230円        1人気\n",
      "202209020902  3連単        8 7 1        2,020円        5人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020903   7 -0.139737\n",
      "202209020903   5 -0.223958\n",
      "202209020903   8 -0.398055\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202209020903   単勝              8              150円          1人気\n",
      "202209020903   複勝         8 9 16      110円400円990円   1人気7人気10人気\n",
      "202209020903   枠連            4 5              410円          1人気\n",
      "202209020903   馬連            8 9            1,870円          7人気\n",
      "202209020903  ワイド  8 9 8 16 9 16  710円1,200円9,190円  7人気12人気54人気\n",
      "202209020903   馬単            8 9            2,920円         10人気\n",
      "202209020903  3連複         8 9 16           18,050円         51人気\n",
      "202209020903  3連単         8 9 16           69,330円        194人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020904   5 -0.142302\n",
      "202209020904   4 -0.413153\n",
      "202209020904   3 -0.456026\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209020904   単勝            3          380円        2人気\n",
      "202209020904   複勝        3 5 4  100円100円160円  1人気2人気4人気\n",
      "202209020904   枠連          3 5          130円        1人気\n",
      "202209020904   馬連          3 5          150円        1人気\n",
      "202209020904  ワイド  3 5 3 4 4 5  110円320円250円  1人気6人気4人気\n",
      "202209020904   馬単          3 5          560円        2人気\n",
      "202209020904  3連複        3 4 5          390円        2人気\n",
      "202209020904  3連単        3 5 4        2,070円        5人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020905   7  0.563529\n",
      "202209020905   1 -0.132691\n",
      "202209020905  11 -0.175102\n",
      "actual\n",
      "                0            1                 2            3\n",
      "202209020905   単勝            1              150円          1人気\n",
      "202209020905   複勝        1 2 4      110円210円930円   1人気3人気11人気\n",
      "202209020905   枠連          1 1              510円          2人気\n",
      "202209020905   馬連          1 2              530円          1人気\n",
      "202209020905  ワイド  1 2 1 4 2 4  280円1,410円4,010円  1人気17人気33人気\n",
      "202209020905   馬単          1 2              710円          1人気\n",
      "202209020905  3連複        1 2 4            5,680円         17人気\n",
      "202209020905  3連単        1 2 4           12,790円         34人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020906  14 -0.167149\n",
      "202209020906   6 -0.185293\n",
      "202209020906  15 -0.199294\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202209020906   単勝             15            1,710円          7人気\n",
      "202209020906   複勝         15 5 1      380円170円140円    8人気2人気1人気\n",
      "202209020906   枠連            3 8              880円          3人気\n",
      "202209020906   馬連           5 15            5,090円         19人気\n",
      "202209020906  ワイド  5 15 1 15 1 5  1,560円1,060円330円  21人気14人気1人気\n",
      "202209020906   馬単           15 5           10,930円         42人気\n",
      "202209020906  3連複         1 5 15            3,860円         11人気\n",
      "202209020906  3連単         15 5 1           42,240円        148人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020907   2 -0.004023\n",
      "202209020907   8 -0.377247\n",
      "202209020907   9 -0.607483\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209020907   単勝            2          200円        1人気\n",
      "202209020907   複勝        2 5 8  110円170円130円  1人気5人気2人気\n",
      "202209020907   枠連          2 5        1,230円        6人気\n",
      "202209020907   馬連          2 5        1,140円        5人気\n",
      "202209020907  ワイド  2 5 2 8 5 8  310円240円480円  3人気2人気9人気\n",
      "202209020907   馬単          2 5        1,500円        6人気\n",
      "202209020907  3連複        2 5 8        1,320円        5人気\n",
      "202209020907  3連単        2 5 8        5,660円       16人気\n",
      "profit 240\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020908  10  0.413193\n",
      "202209020908   2  0.066315\n",
      "202209020908  15 -0.067301\n",
      "actual\n",
      "                0              1             2          3\n",
      "202209020908   単勝              4          420円        2人気\n",
      "202209020908   複勝         4 10 1  140円140円140円  3人気2人気1人気\n",
      "202209020908   枠連            3 6          820円        3人気\n",
      "202209020908   馬連           4 10        1,140円        3人気\n",
      "202209020908  ワイド  4 10 1 4 1 10  430円430円320円  3人気2人気1人気\n",
      "202209020908   馬単           4 10        2,410円        6人気\n",
      "202209020908  3連複         1 4 10        1,300円        1人気\n",
      "202209020908  3連単         4 10 1        8,830円        8人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020909   5 -0.215405\n",
      "202209020909   1 -0.595548\n",
      "202209020909   7 -0.930799\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209020909   単勝            7        1,020円        4人気\n",
      "202209020909   複勝        7 5 1  130円100円100円  3人気2人気1人気\n",
      "202209020909   馬連          5 7          630円        2人気\n",
      "202209020909  ワイド  5 7 1 7 1 5  210円270円120円  2人気4人気1人気\n",
      "202209020909   馬単          7 5        2,310円        7人気\n",
      "202209020909  3連複        1 5 7          400円        2人気\n",
      "202209020909  3連単        7 5 1        5,810円       22人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020910   8  0.823075\n",
      "202209020910   6 -0.575680\n",
      "202209020910   3 -0.576556\n",
      "actual\n",
      "                0            1             2           3\n",
      "202209020910   単勝            8          170円         1人気\n",
      "202209020910   複勝        8 3 5  110円150円210円   1人気4人気5人気\n",
      "202209020910   馬連          3 8          600円         3人気\n",
      "202209020910  ワイド  3 8 5 8 3 5  250円340円670円  3人気4人気11人気\n",
      "202209020910   馬単          8 3          800円         3人気\n",
      "202209020910  3連複        3 5 8        1,340円         6人気\n",
      "202209020910  3連単        8 3 5        3,820円        10人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020911   4  0.517809\n",
      "202209020911   5  0.434039\n",
      "202209020911   3 -0.183451\n",
      "actual\n",
      "                0            1                 2            3\n",
      "202209020911   単勝            5            1,030円          4人気\n",
      "202209020911   複勝          5 3        440円1,150円       4人気7人気\n",
      "202209020911   馬連          3 5            8,750円         19人気\n",
      "202209020911  ワイド  3 5 4 5 3 4  2,090円690円1,700円  19人気8人気16人気\n",
      "202209020911   馬単          5 3           16,260円         33人気\n",
      "202209020911  3連複        3 4 5           11,180円         28人気\n",
      "202209020911  3連単        5 3 4          106,830円        157人気\n",
      "profit 2090\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209020912   1 -0.270156\n",
      "202209020912   6 -0.275481\n",
      "202209020912   7 -0.330042\n",
      "actual\n",
      "                0              1             2          3\n",
      "202209020912   単勝             10          860円        4人気\n",
      "202209020912   複勝         10 1 5  360円260円270円  4人気2人気3人気\n",
      "202209020912   枠連            1 8        2,740円        8人気\n",
      "202209020912   馬連           1 10        1,990円        7人気\n",
      "202209020912  ワイド  1 10 5 10 1 5  630円620円390円  6人気8人気5人気\n",
      "202209020912   馬単           10 1        3,880円       13人気\n",
      "202209020912  3連複         1 5 10        2,570円       10人気\n",
      "202209020912  3連単         10 1 5       14,810円       52人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010301  13 -0.326447\n",
      "202203010301   1 -0.382623\n",
      "202203010301  10 -0.530116\n",
      "actual\n",
      "                0                1               2            3\n",
      "202203010301   単勝               11          1,070円          5人気\n",
      "202203010301   複勝          11 10 1    290円250円210円    5人気4人気3人気\n",
      "202203010301   枠連              5 6          1,090円          5人気\n",
      "202203010301   馬連            10 11          4,630円         14人気\n",
      "202203010301  ワイド  10 11 1 11 1 10  1,160円900円610円  12人気11人気7人気\n",
      "202203010301   馬単            11 10          8,240円         27人気\n",
      "202203010301  3連複          1 10 11          6,340円         20人気\n",
      "202203010301  3連単          11 10 1         47,930円        125人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010302   8 -0.538328\n",
      "202203010302   1 -1.245074\n",
      "202203010302   6 -1.258023\n",
      "actual\n",
      "                0            1             2          3\n",
      "202203010302   単勝            6          480円        2人気\n",
      "202203010302   複勝        6 1 8  150円130円170円  2人気1人気4人気\n",
      "202203010302   枠連          1 3          430円        1人気\n",
      "202203010302   馬連          1 6          870円        1人気\n",
      "202203010302  ワイド  1 6 6 8 1 8  360円580円430円  1人気5人気4人気\n",
      "202203010302   馬単          6 1        1,690円        2人気\n",
      "202203010302  3連複        1 6 8        1,630円        2人気\n",
      "202203010302  3連単        6 1 8        7,200円        3人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010303  14 -0.338982\n",
      "202203010303   6 -0.402918\n",
      "202203010303   8 -0.689709\n",
      "actual\n",
      "                0              1             2          3\n",
      "202203010303   単勝              6          450円        2人気\n",
      "202203010303   複勝         6 11 5  160円240円280円  2人気4人気5人気\n",
      "202203010303   枠連            4 6        1,730円        7人気\n",
      "202203010303   馬連           6 11        1,740円        6人気\n",
      "202203010303  ワイド  6 11 5 6 5 11  620円630円840円  6人気7人気9人気\n",
      "202203010303   馬単           6 11        2,850円       10人気\n",
      "202203010303  3連複         5 6 11        4,740円       14人気\n",
      "202203010303  3連単         6 11 5       19,410円       56人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010304   2 -1.070842\n",
      "202203010304   6 -1.164817\n",
      "202203010304   4 -1.258895\n",
      "actual\n",
      "                0              1             2           3\n",
      "202203010304   単勝              9          440円         2人気\n",
      "202203010304   複勝         9 14 2  150円440円130円   2人気8人気1人気\n",
      "202203010304   枠連            6 8        1,810円         7人気\n",
      "202203010304   馬連           9 14        3,390円        13人気\n",
      "202203010304  ワイド  9 14 2 9 2 14  930円210円760円  13人気1人気9人気\n",
      "202203010304   馬単           9 14        4,490円        15人気\n",
      "202203010304  3連複         2 9 14        2,560円         5人気\n",
      "202203010304  3連単         9 14 2       19,870円        50人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010305   1 -0.036859\n",
      "202203010305   7 -0.531037\n",
      "202203010305   5 -1.072887\n",
      "actual\n",
      "                0              1                   2             3\n",
      "202203010305   単勝              8              2,760円          10人気\n",
      "202203010305   複勝         8 12 1    1,030円770円1,460円   11人気9人気12人気\n",
      "202203010305   枠連            5 7                510円           1人気\n",
      "202203010305   馬連           8 12             20,460円          61人気\n",
      "202203010305  ワイド  8 12 1 8 1 12  5,670円8,740円7,490円  61人気71人気69人気\n",
      "202203010305   馬単           8 12             49,650円         130人気\n",
      "202203010305  3連複         1 8 12            128,340円         247人気\n",
      "202203010305  3連単         8 12 1            990,120円       1,535人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010306  15  1.029178\n",
      "202203010306  11  0.339984\n",
      "202203010306  10 -0.657262\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202203010306   単勝              8            1,960円          8人気\n",
      "202203010306   複勝         8 10 4      630円170円380円    9人気2人気5人気\n",
      "202203010306   枠連            5 6              650円          2人気\n",
      "202203010306   馬連           8 10            4,620円         16人気\n",
      "202203010306  ワイド  8 10 4 8 4 10  1,340円2,270円730円  15人気27人気7人気\n",
      "202203010306   馬単           8 10           10,100円         37人気\n",
      "202203010306  3連複         4 8 10           11,350円         45人気\n",
      "202203010306  3連単         8 10 4          108,160円        346人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010307  16 -0.347316\n",
      "202203010307   6 -0.487014\n",
      "202203010307   1 -0.613774\n",
      "actual\n",
      "                0              1             2          3\n",
      "202203010307   単勝             16          320円        2人気\n",
      "202203010307   複勝         16 1 6  130円190円130円  2人気4人気1人気\n",
      "202203010307   枠連            1 8          960円        4人気\n",
      "202203010307   馬連           1 16        1,220円        5人気\n",
      "202203010307  ワイド  1 16 6 16 1 6  440円250円400円  5人気1人気4人気\n",
      "202203010307   馬単           16 1        2,040円        8人気\n",
      "202203010307  3連複         1 6 16        1,290円        3人気\n",
      "202203010307  3連単         16 1 6        6,820円       11人気\n",
      "profit 440\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010308   3  0.006098\n",
      "202203010308   6 -0.419889\n",
      "202203010308   9 -0.580601\n",
      "actual\n",
      "                0            1                   2             3\n",
      "202203010308   単勝            9                390円           2人気\n",
      "202203010308   複勝        9 5 2        170円540円400円     2人気9人気6人気\n",
      "202203010308   枠連          4 6              3,140円          12人気\n",
      "202203010308   馬連          5 9              5,320円          20人気\n",
      "202203010308  ワイド  5 9 2 9 2 5  1,810円1,110円5,360円  20人気11人気50人気\n",
      "202203010308   馬単          9 5              9,710円          35人気\n",
      "202203010308  3連複        2 5 9             24,130円          79人気\n",
      "202203010308  3連単        9 5 2            116,090円         369人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010309   4  0.205829\n",
      "202203010309   1  0.071692\n",
      "202203010309   3 -0.158203\n",
      "actual\n",
      "                0            1             2          3\n",
      "202203010309   単勝            9          600円        3人気\n",
      "202203010309   複勝        9 4 3  150円120円130円  3人気1人気2人気\n",
      "202203010309   枠連          4 8          560円        2人気\n",
      "202203010309   馬連          4 9          840円        2人気\n",
      "202203010309  ワイド  4 9 3 9 3 4  290円340円190円  2人気3人気1人気\n",
      "202203010309   馬単          9 4        2,140円        5人気\n",
      "202203010309  3連複        3 4 9          890円        1人気\n",
      "202203010309  3連単        9 4 3        6,900円       11人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010310   1 -0.305962\n",
      "202203010310  14 -0.512628\n",
      "202203010310   7 -0.519313\n",
      "actual\n",
      "                0              1                    2             3\n",
      "202203010310   単勝             13               5,150円          12人気\n",
      "202203010310   複勝         13 7 9       1,220円240円590円    12人気3人気9人気\n",
      "202203010310   枠連            4 7               1,070円           4人気\n",
      "202203010310   馬連           7 13              11,160円          42人気\n",
      "202203010310  ワイド  7 13 9 13 7 9  3,550円11,150円1,830円  43人気74人気22人気\n",
      "202203010310   馬単           13 7              29,210円          94人気\n",
      "202203010310  3連複         7 9 13              55,660円         168人気\n",
      "202203010310  3連単         13 7 9             489,640円       1,191人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010311  15  0.506004\n",
      "202203010311   4  0.273726\n",
      "202203010311  13  0.260304\n",
      "actual\n",
      "                0            1                   2             3\n",
      "202203010311   単勝            4                690円           3人気\n",
      "202203010311   複勝        4 2 8        270円360円280円     4人気8人気5人気\n",
      "202203010311   枠連          1 2              1,150円           5人気\n",
      "202203010311   馬連          2 4              3,560円          15人気\n",
      "202203010311  ワイド  2 4 4 8 2 8  1,410円1,490円2,020円  16人気20人気32人気\n",
      "202203010311   馬単          4 2              6,940円          31人気\n",
      "202203010311  3連複        2 4 8             13,390円          52人気\n",
      "202203010311  3連単        4 2 8             71,050円         268人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202203010312   1 -0.359074\n",
      "202203010312   6 -0.646252\n",
      "202203010312   2 -0.719715\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202203010312   単勝              6              370円          1人気\n",
      "202203010312   複勝         6 10 2      180円840円170円   2人気10人気1人気\n",
      "202203010312   枠連            5 7            3,510円         17人気\n",
      "202203010312   馬連           6 10            9,940円         31人気\n",
      "202203010312  ワイド  6 10 2 6 2 10  2,880円360円1,950円  32人気1人気24人気\n",
      "202203010312   馬単           6 10           15,600円         53人気\n",
      "202203010312  3連複         2 6 10           10,450円         35人気\n",
      "202203010312  3連単         6 10 2           60,480円        219人気\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 7 / 36\n",
      "収支   : 4080 円\n",
      "的中レース ['04', '05', '07', '10', '07', '10', '07']\n",
      "---------------------\n",
      "複勝\n",
      "的中率 : 21 / 36\n",
      "収支   : 2210 円\n",
      "的中レース ['03', '04', '05', '06', '07', '08', '09', '10', '11', '02', '04', '07', '08', '09', '10', '12', '02', '04', '05', '07', '09']\n",
      "---------------------\n",
      "ワイド\n",
      "的中率 : 4 / 36\n",
      "収支   : -640 円\n",
      "的中レース ['05', '07', '11', '07']\n"
     ]
    }
   ],
   "source": [
    "data =  ShutubaTable.scrape(race_id_list, date)\n",
    "st = ShutubaTable(data)\n",
    "st.preprocessing()\n",
    "st.merge_horse_results(hr)\n",
    "st.merge_peds(pe.peds_vec)\n",
    "st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "sl = RankSimulater(lgb_rank)\n",
    "sl.show_results(st ,race_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc1142-ff9b-4568-80b8-18ef0d9010c1",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29d7bf-5052-44dc-ac80-dea59ed65444",
   "metadata": {},
   "source": [
    "流れ\n",
    "1. fasttext用の血統データの学習データを作る (血統の情報のみ, index ヘッダはいらない)\n",
    "2. fasttext学習\n",
    "3. 学習モデルを使って, 血統データをベクトル化\n",
    "4. ベクトル化して r.data_cに concat\n",
    "5. 学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d375b9-e292-4d4a-97c2-905e3060419f",
   "metadata": {},
   "source": [
    "教師あり, 教師なしでも生成されるベクトルは等しい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5229f-9b05-4bc1-820a-0e275a1d6c8f",
   "metadata": {},
   "source": [
    "# model_ft 作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554f0c62-72c2-4a2e-b45e-b0b4d37c2e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 相対パスしかできない\n",
    "# dim : 出力の次元\n",
    "# minn : n_gramの最小単位\n",
    "# maxn : n_gramの最大単位\n",
    "model_ft = ft.train_unsupervised('test.txt',dim=62,minn=2,maxn=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94807517-3ed5-427d-9886-72c42b7589cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.23876448e-05,  1.15252602e-04, -9.46200526e-05,  1.97496520e-05,\n",
       "       -3.14811296e-05,  6.10383358e-05,  3.84694722e-05,  4.08472006e-05,\n",
       "       -2.62596750e-05, -6.39620412e-05,  2.14720822e-05, -4.69113438e-05,\n",
       "        3.77046381e-05, -1.26615938e-04,  4.62611060e-05, -4.64162804e-05,\n",
       "       -1.04648252e-05,  8.02415016e-05,  5.22616428e-05,  2.21860992e-05,\n",
       "       -1.75977038e-05, -8.26951291e-05,  3.14370882e-05,  6.86578787e-05,\n",
       "       -3.35702607e-05,  1.14919050e-04, -8.21495541e-06, -9.01657186e-05,\n",
       "       -7.84629883e-05,  2.17205616e-05, -1.27823092e-04,  7.07987565e-05,\n",
       "        2.46920517e-05,  2.06759105e-05,  1.44077581e-04,  2.31686881e-05,\n",
       "       -3.09964562e-05, -7.95884553e-05, -4.59835537e-05, -1.93069845e-05,\n",
       "        3.55003340e-06,  1.18724784e-04, -6.99495213e-05, -5.45399816e-05,\n",
       "       -7.00177625e-05,  4.58251998e-05, -5.90208510e-05,  1.51029690e-05,\n",
       "        1.06203879e-05, -4.25494000e-05, -5.48500502e-05,  1.97607969e-05,\n",
       "       -1.11221507e-05, -1.12135414e-04, -6.92541580e-05, -3.56512865e-05,\n",
       "        4.80831432e-06, -1.21586090e-04, -5.42530252e-05,  9.49262467e-05,\n",
       "        6.24590248e-05,  6.67856802e-05], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_txt['hoge'] で 'hoge'の単語ベクトル入手\n",
    "model_ft[model_ft.words[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b63a1982-6425-4887-94bc-fcd793492242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8503,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.get_subwords(model_ft.words[1])[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199fde55-17f7-4ecf-9f0f-58682ea4712a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'ディープインパクト,クロウキャニオン,サンデーサイレンス,ウインドインハーヘア,フレンチデピュティ,クロカミ,Halo,WishingWell,Alzao,Burghclere,DeputyMinister,Mitterand,Caerleon,ミルド,HailtoReason,Cosmah,Understanding,MountainFlower,Lyphard,LadyRebecca,Busted,Highclere,ViceRegent,MintCopy,HoldYourPeace,LaredoLass,Nijinsky,Foreseer,DesertWine,MargieBelle,Turnto,Nothirdchance,CosmicBomb,Almahmoud,PromisedLand,PrettyWays,Montparnasse,Edelweiss,NorthernDancer,Goofed,SirIvor,Pocahontas,Crepello,SansleSou,QueensHussar,Highlight,NorthernDancer,VictoriaRegina,BuntysFlight,Shakney,SpeakJohn,BlueMoon,BoldRuler,FortunateIsle,NorthernDancer,FlamingPage,RoundTable,RegalGleam,Damascus,AnneCampbell,VaguelyNoble,Margravine',\n",
       " 'ディープインパクト,ロベルタ,サンデーサイレンス,ウインドインハーヘア,ブライアンズタイム,グレースアドマイヤ,Halo,WishingWell,Alzao,Burghclere,Roberto,KelleysDay,トニービン,バレークイーン,HailtoReason,Cosmah,Understanding,MountainFlower,Lyphard,LadyRebecca,Busted,Highclere,HailtoReason,Bramalea,Graustark,GoldenTrail,カンパラ,SevernBridge,SadlersWells,SunPrincess,Turnto,Nothirdchance,CosmicBomb,Almahmoud,PromisedLand,PrettyWays,Montparnasse,Edelweiss,NorthernDancer,Goofed,SirIvor,Pocahontas,Crepello,SansleSou,QueensHussar,Highlight,Turnto,Nothirdchance,Nashua,Rarelea,Ribot,FlowerBowl,HastyRoad,SunnyVale,Kalamoun,StatePension,Hornbeam,PriddyFair,NorthernDancer,FairyBridge,イングリッシュプリンス,SunnyValley',\n",
       " 'ディープインパクト,ナイトマジック,サンデーサイレンス,ウインドインハーヘア,Sholokhov,NightWoman,Halo,WishingWell,Alzao,Burghclere,SadlersWells,LaMeilleure,Monsun,Noveka,HailtoReason,Cosmah,Understanding,MountainFlower,Lyphard,LadyRebecca,Busted,Highclere,NorthernDancer,FairyBridge,LordGayle,Gradille,Konigsstuhl,Mosella,Kalaglow,Novelle,Turnto,Nothirdchance,CosmicBomb,Almahmoud,PromisedLand,PrettyWays,Montparnasse,Edelweiss,NorthernDancer,Goofed,SirIvor,Pocahontas,Crepello,SansleSou,QueensHussar,Highlight,Nearctic,Natalma,BoldReason,Special,SirGaylord,StickyCase,HomeGuard,Gradiva,DschingisKhan,Konigskronung,Surumu,Monasia,Kalamoun,Rossitor,Northfields,Nigeria',\n",
       " 'ディープインパクト,シユーマ,サンデーサイレンス,ウインドインハーヘア,Medicean,Sichilla,Halo,WishingWell,Alzao,Burghclere,Machiavellian,MysticGoddess,デインヒル,SlipstreamQueen,HailtoReason,Cosmah,Understanding,MountainFlower,Lyphard,LadyRebecca,Busted,Highclere,MrProspector,CoupdeFolie,StormBird,RoseGoddess,Danzig,Razyana,ConquistadorCielo,CountryQueen,Turnto,Nothirdchance,CosmicBomb,Almahmoud,PromisedLand,PrettyWays,Montparnasse,Edelweiss,NorthernDancer,Goofed,SirIvor,Pocahontas,Crepello,SansleSou,QueensHussar,Highlight,RaiseaNative,GoldDigger,Halo,RaisetheStandard,NorthernDancer,SouthOcean,Sassafras,Cocarde,NorthernDancer,PasdeNom,HisMajesty,SpringAdieu,MrProspector,KDPrincess,イクスプロウデント,CarriesRough',\n",
       " 'ディープインパクト,ミスアンコール,サンデーサイレンス,ウインドインハーヘア,キングカメハメハ,ブロードアピール,Halo,WishingWell,Alzao,Burghclere,Kingmambo,マンファス,BroadBrush,ValidAllure,HailtoReason,Cosmah,Understanding,MountainFlower,Lyphard,LadyRebecca,Busted,Highclere,MrProspector,Miesque,ラストタイクーン,PilotBird,AckAck,HayPatcher,ValidAppeal,AlluringGirl,Turnto,Nothirdchance,CosmicBomb,Almahmoud,PromisedLand,PrettyWays,Montparnasse,Edelweiss,NorthernDancer,Goofed,SirIvor,Pocahontas,Crepello,SansleSou,QueensHussar,Highlight,RaiseaNative,GoldDigger,Nureyev,Pasadoble,トライマイベスト,MillPrincess,Blakeney,TheDancer,BattleJoined,FastTurn,HoisttheFlag,TurntoTalent,InReality,DesertTrial,Secretariat,WaterCress']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1e2be52-3b04-4390-9769-c63ce59a643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5724351e-03,  2.1741162e-03, -6.2224795e-03,  7.9450803e-03,\n",
       "        9.2663774e-03,  1.2499948e-03, -1.9003255e-03, -1.4434209e-03,\n",
       "       -7.0348015e-04, -2.3646757e-03,  8.4775481e-03,  7.6517521e-04,\n",
       "        1.2443915e-04,  6.6868594e-04, -2.2353258e-03, -3.1006148e-03,\n",
       "       -5.1603146e-04, -6.8897572e-03,  8.9530461e-03,  6.7916275e-03,\n",
       "        2.7701540e-03, -1.4256138e-03,  9.4705867e-03, -9.9947751e-03,\n",
       "       -4.6277489e-03, -7.6256958e-03,  7.5081405e-03, -3.4463892e-03,\n",
       "       -3.8665808e-03,  1.7099102e-03,  9.7447438e-03, -7.7348817e-03,\n",
       "       -2.9115018e-04, -3.1618846e-03,  6.5541668e-03, -7.9466682e-03,\n",
       "       -5.2352938e-05, -8.1277534e-04, -1.2927600e-03, -9.0491874e-03,\n",
       "       -4.8794332e-03, -1.3042025e-03, -8.9057656e-03, -2.9853662e-03,\n",
       "        6.7775971e-03,  9.9755134e-03, -2.7005693e-03,  5.2640764e-03,\n",
       "       -6.6113472e-03,  5.9692696e-04, -9.7892229e-03, -4.3281284e-03,\n",
       "        6.6364333e-03, -9.9991390e-04,  3.3466963e-03, -7.2530257e-03,\n",
       "        1.4511276e-03,  9.4374539e-03,  6.8324972e-03,  1.0399714e-03,\n",
       "        6.0194843e-03,  5.8186194e-03,  3.8527260e-03,  6.5378621e-03,\n",
       "       -3.9170850e-03,  9.6854856e-03,  4.2792736e-03,  7.6427357e-04,\n",
       "       -4.8055393e-03,  9.4173355e-03,  9.0470780e-03,  9.1776745e-03,\n",
       "        6.0645705e-03, -7.4923565e-03, -2.5749654e-03, -1.7557642e-03,\n",
       "        3.6343501e-04,  7.6567060e-03,  4.2951941e-03, -5.3092451e-03,\n",
       "       -2.7257178e-03,  5.8091432e-03, -3.5633314e-03, -3.1582750e-03,\n",
       "        9.5966822e-03, -6.4879083e-03, -3.9101331e-03,  4.1160994e-04,\n",
       "       -7.1197501e-03, -2.3638823e-03, -5.1401439e-03,  3.2334772e-03,\n",
       "        7.0011085e-03, -8.3606951e-03, -9.8121529e-03, -9.1953417e-03,\n",
       "        5.8084358e-03, -2.6821357e-03, -5.3060763e-03,  3.9062789e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.get_input_vector(ind=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd9030-310c-414d-8c91-6178159c00e6",
   "metadata": {},
   "source": [
    "model[model.words[1]] と model.get_input_vector(ind=1) は等価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f5e42fb-7fb1-4449-9d16-c1a21d87aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.64855465e-05,  1.37111065e-05,  1.41594879e-04,  3.69198642e-05,\n",
       "        9.37314871e-06,  9.83630889e-05, -4.32917550e-05, -5.60286717e-05,\n",
       "       -1.21071007e-05,  3.47241585e-05, -1.29177488e-05,  5.48821408e-05,\n",
       "       -7.11681787e-05,  1.35873206e-05, -6.51547089e-05,  1.05369854e-05,\n",
       "        2.46712134e-05, -2.98814448e-05, -6.97223822e-06,  5.47772688e-05,\n",
       "       -4.34648828e-05, -6.77032876e-05,  3.82750259e-05,  4.62639291e-05,\n",
       "        3.87809414e-05, -5.79457264e-05, -3.11739132e-05, -3.45420995e-05,\n",
       "        2.56179737e-05,  1.88591548e-05, -1.06936168e-04, -3.09621441e-06,\n",
       "       -3.30380026e-05, -2.44859002e-05,  2.54371498e-05,  2.28005192e-05,\n",
       "       -1.14125714e-05, -7.71405212e-06, -2.62292688e-05,  4.95023669e-05,\n",
       "        6.83483158e-05,  7.41472240e-06, -7.45871466e-06, -1.99570986e-05,\n",
       "       -8.77055936e-06,  6.14155870e-05, -3.37384336e-05, -7.03690312e-05,\n",
       "       -6.21120780e-05, -3.50524570e-05, -2.38443281e-05,  3.41939740e-05,\n",
       "       -5.05409917e-05, -3.19997957e-06, -3.45971457e-05,  2.91866854e-05,\n",
       "        8.44113019e-05,  3.85636376e-05, -3.52261850e-05, -7.60995536e-05,\n",
       "        4.26866463e-05,  6.05096320e-05], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[test_str]\n",
    "\n",
    "# model[model.words[1]] と model.get_input_vector(ind=1) は等価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6836443-36c4-4f2e-aa7e-c911fd9b31ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22636686,  0.0358811 ,  0.3706047 ,  0.09663288,  0.02452924,\n",
       "        0.2574478 , -0.11331306, -0.1466456 , -0.03169499,  0.0908782 ,\n",
       "       -0.03380894,  0.14364257, -0.18627687,  0.03555746, -0.17052874,\n",
       "        0.0275799 ,  0.06457247, -0.07821366, -0.01824913,  0.14337559,\n",
       "       -0.11376097, -0.1772068 ,  0.10017442,  0.12109151,  0.10149854,\n",
       "       -0.15165876, -0.08159366, -0.09040555,  0.0670519 ,  0.04935378,\n",
       "       -0.27988228, -0.00810147, -0.08646543, -0.06408333,  0.06658382,\n",
       "        0.05967693, -0.02987295, -0.02018429, -0.06864916,  0.12956837,\n",
       "        0.17888325,  0.01940197, -0.0195243 , -0.05223562, -0.02295211,\n",
       "        0.16074347, -0.08830138, -0.18417585, -0.16256206, -0.09174566,\n",
       "       -0.06240372,  0.0894906 , -0.1322762 , -0.00837872, -0.09055168,\n",
       "        0.076395  ,  0.22093119,  0.10093257, -0.09220136, -0.19917955,\n",
       "        0.11172937,  0.15837023], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文字列のベクトル表現\n",
    "model.get_sentence_vector(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84bd217d-81fc-4dd4-8c84-983f25649a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01349934,  0.01271002, -0.01006453, ...,  0.00531607,\n",
       "        -0.0106207 ,  0.00814016],\n",
       "       [ 0.01048684,  0.00816879, -0.00584027, ...,  0.01594336,\n",
       "         0.00641512,  0.01121091],\n",
       "       [-0.01389093, -0.00994238, -0.01586624, ...,  0.00450218,\n",
       "         0.00770794,  0.00581788],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これまで入力した行列を返す関数\n",
    "model.get_input_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a74eb78-be1b-4437-9dd3-0848392ff303",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = model.get_word_vector('サトノダイヤモンド')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1542697-b382-473e-9290-de1f9f1982fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = model.get_word_vector('ディープインパクト')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05b6629-3c9c-42a3-82b9-06bf42417ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(97,122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d49ced-60d6-4abd-85f8-160b5d6d5895",
   "metadata": {},
   "source": [
    "# ランダム文字列でテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5da0cf7-dbb0-4d7f-8637-14a273583b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(62):\n",
    "    rand_name = ''\n",
    "    for j in range(4):\n",
    "        rand_name += chr(random.randint(97,122))\n",
    "    word_list.append(rand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8bc506-a0e6-4bbb-a32a-6903b97093ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \",\".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0a26e83e-1f72-44e6-884b-05d3b9a0254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ポイントフラッグ', 0.951915442943573),\n",
       " ('カレイメモワール', 0.8899344801902771),\n",
       " ('リヤンドファミユ', 0.8506640791893005),\n",
       " ('パストラリズム', 0.8343662619590759),\n",
       " ('コスモスカイライン', 0.8293111324310303),\n",
       " ('ドリームジャーニー', 0.828325092792511),\n",
       " ('ハッシュバンバン', 0.8259212374687195),\n",
       " ('ナカヤマフェスタ', 0.8249091506004333),\n",
       " ('タイセイレジェンド', 0.8150109648704529),\n",
       " ('オーシャンブルー', 0.8144512176513672)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "gen_model = gensim.models.KeyedVectors.load_word2vec_format('fastText/ketto_model.vec', binary=False)\n",
    "\n",
    "# most_similarメソッドを使って演算\n",
    "# positiveに足し合わせるデータをリストで渡し、negativeに差し引くデータをリストで渡す。\n",
    "\n",
    "gen_model.most_similar(\n",
    "    positive=[ \"ゴールドシップ\"],\n",
    "#     negative=[\"ディープインパクト\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca4955e3-7b1f-4955-9066-9b7a2232f7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ウインドインハーヘア', 0.8664582371711731),\n",
       " ('ローリエ', 0.7226738333702087),\n",
       " ('オンヴェラ', 0.7160682082176208),\n",
       " ('アイスドール', 0.7097043395042419),\n",
       " ('アローム', 0.7028155326843262),\n",
       " ('サトノアラジン', 0.7014816403388977),\n",
       " ('クロノロジスト', 0.6986632943153381),\n",
       " ('ピンクアリエス', 0.6985989212989807),\n",
       " ('ナイトマジック', 0.6953324675559998),\n",
       " ('ヘヴンリークルーズ', 0.6944254636764526)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.most_similar(\n",
    "    positive=[ \"ディープインパクト\"],\n",
    "#     negative=[\"ディープインパクト\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85747d8f-c452-4eaf-ab8a-1d964ca42805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_txt = ft.train_unsupervised('fastText/text.txt',minn=2,maxn=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6752119-bd97-4367-8094-9bd13cd0d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9368"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_txt.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cb7c503-6504-4e5a-a4cb-ef33b90ea30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50ed1c05-6afa-45e7-b6f5-b089279995a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors  = model_txt['キズナ'].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e80ec-9450-43b0-8e0d-f170f67086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f41fe-1013-4eaf-9776-07d8819750b1",
   "metadata": {},
   "source": [
    "# 今後の方針\n",
    "1. とりあえず, Peds class で, 馬名 -> 馬名正規化\n",
    "2. 仕様は決めていないが, 学習ずみ, fasttext モデルで血統をベクトル化\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0b75c-8969-4b49-b522-1c0df7bbb166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
