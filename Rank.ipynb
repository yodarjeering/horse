{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485b75ae-02d8-4d3e-b0f4-c8cad9518e8c",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa116d3b-51ac-42de-bf7a-fb55eb148689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score,roc_curve, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna.integration.xgboost as xgb_o\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "import scipy as sp\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import sklearn\n",
    "from scipy.special import comb\n",
    "from itertools import combinations\n",
    "import copy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import comb\n",
    "from itertools import permutations\n",
    "import datetime\n",
    "import lxml\n",
    "import seaborn as sns\n",
    "from hyperopt import hp, tpe, Trials, fmin,STATUS_OK\n",
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782806f-f29f-40fe-9c06-7dfca7dd2339",
   "metadata": {},
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4008e3-1c76-4f8e-b375-0f3533379d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ubu = '/home/hipro/デスクトップ/Horse/Data/20_21'\n",
    "path_mac2 = '/Users/rince/Desktop/Horse/Data/saishin2/'\n",
    "path_mac = '/Users/rince/Desktop/Horse/Data/saishin/'\n",
    "path_win = '/Users/Owner/Desktop/program/Horse/Data/saishin/'\n",
    "path_win2 = '/Users/Owner/Desktop/program/Horse/Data/saishin2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abdac4-ad14-4a42-a549-33a6270db233",
   "metadata": {},
   "source": [
    "# funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa496290-3b79-4e19-9f1b-a7c4a08f209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(df, test_size=0.2, rank_learning=True):\n",
    "    df_ = df.copy()\n",
    "    if not rank_learning:\n",
    "        df_['rank'] = df_['rank'].map(lambda x:1 if x<4 else 0)\n",
    "    sorted_id_list = df_.sort_values(\"date\").index.unique()\n",
    "    train_id_list = sorted_id_list[: round(len(sorted_id_list) * (1 - test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list) * (1 - test_size)) :]\n",
    "    train = df_.loc[train_id_list]#.drop(['date'], axis=1)\n",
    "    test = df_.loc[test_id_list]#.drop(['date'], axis=1)\n",
    "    return train, test\n",
    "\n",
    "def rus_data(df, test_size=0.2):\n",
    "    train, test = split_data(df,test_size=test_size)\n",
    "    x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "    y_train = train['rank']\n",
    "    x_test = test.drop(['rank', 'date','単勝'], axis=1)\n",
    "    y_test = test['rank']\n",
    "    \n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    x_resampled, y_resampled = rus.fit_resample(x_train, y_train)\n",
    "    return x_resampled, y_resampled, x_test, y_test\n",
    "\n",
    "def load_csv(load_path):\n",
    "    df = pd.read_csv(load_path, index_col=0)\n",
    "    return df\n",
    "\n",
    "def gain(return_func, x_, n_samples=100,lower=50,t_range=[0.5,3.5]):\n",
    "    gain = {}\n",
    "    for i in range(n_samples):\n",
    "        threshold = t_range[1] * (i/n_samples) + t_range[0] *(1-i/n_samples)\n",
    "        n_bets, return_rate, n_hits,std = return_func(x_, threshold)\n",
    "        if n_bets > lower:\n",
    "            gain[threshold] = {'return_rate':return_rate,'n_hits':n_hits,'std':std,'n_bets':n_bets}\n",
    "    return pd.DataFrame(gain).T\n",
    "\n",
    "place_dict = {\n",
    "    '札幌':'01',  '函館':'02',  '福島':'03',  '新潟':'04',  '東京':'05', \n",
    "    '中山':'06',  '中京':'07',  '京都':'08',  '阪神':'09',  '小倉':'10'\n",
    "}\n",
    "\n",
    "race_type_dict = {\n",
    "    '芝': '芝', 'ダ': 'ダート', '障': '障害'\n",
    "}\n",
    "\n",
    "def plot(g,label=''):\n",
    "    plt.fill_between(g.index,y1 = g['return_rate'] - g['std'],y2=g['return_rate']+g['std'],alpha=0.3)\n",
    "    plt.plot(g.index,g['return_rate'],label=label)\n",
    "    plt.grid(True)\n",
    "    \n",
    "def update_data(old, new):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    old : pandas.DataFrame\n",
    "        古いデータ\n",
    "    new : pandas.DataFrame\n",
    "        新しいデータ\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])\n",
    "\n",
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    for race_id in race_id_list:\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            race_results[race_id] = pd.read_html(url)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return race_results\n",
    "\n",
    "def plot_importances(xgb_model, x_test):\n",
    "    importances = pd.DataFrame(\n",
    "    {'features' : x_test.columns, 'importances' : xgb_model.feature_importances_})\n",
    "    print(importances.sort_values('importances', ascending=False)[:20])\n",
    "    \n",
    "def xgb_pred(x_train, y_train, x_test, y_test):\n",
    "    param_dist = {'objective':'binary:logistic',\n",
    "                  'n_estimators':14,\n",
    "                  'use_label_encoder':False,\n",
    "                 'max_depth':4,\n",
    "                 'random_state':100}\n",
    "    \n",
    "    best_params = {'booster': 'gbtree', \n",
    "                   'objective': 'binary:logistic',\n",
    "                   'use_label_encoder':False,\n",
    "                   'eval_metric': 'rmse', \n",
    "                   'random_state': 100, \n",
    "                   'use_label_encoder':False,\n",
    "                   'eta': 0.13449222415941048,\n",
    "                   'max_depth': 3,\n",
    "                   'lambda': 0.7223936363734638, \n",
    "                   'n_estimators': 14, \n",
    "                   'reg_alpha': 0.7879044553842869,\n",
    "                   'reg_lambda': 0.7780344172793093,\n",
    "                   'importance_type': 'gain'}\n",
    "    xgb_model = xgb.XGBClassifier(**best_params)\n",
    "    hr_pred = xgb_model.fit(x_train.astype(float), np.array(y_train), eval_metric='logloss').predict(x_test.astype(float))\n",
    "    print(\"---------------------\")\n",
    "    y_proba_train = xgb_model.predict_proba(x_train)[:,1]\n",
    "    y_proba = xgb_model.predict_proba(x_test)[:,1]\n",
    "    print('AUC train:',roc_auc_score(y_train,y_proba_train))    \n",
    "    print('AUC test :',roc_auc_score(y_test,y_proba))\n",
    "    print(classification_report(np.array(y_test), hr_pred))\n",
    "    xgb.plot_importance(xgb_model) \n",
    "    plot_importances(xgb_model, x_test)\n",
    "    return xgb_model\n",
    "\n",
    "def lgb_pred(x_train, y_train, x_test, y_test):\n",
    "    param_dist = {\n",
    "        'objective' : 'binary',\n",
    "          'random_state':100,\n",
    "                 }\n",
    "    best_params = {'objective': 'binary',\n",
    "     'metric': 'l1',\n",
    "     'verbosity': -1,\n",
    "     'boosting_type': 'gbdt',\n",
    "     'feature_pre_filter': False,\n",
    "     'lambda_l1': 0.001101158293733924,\n",
    "     'lambda_l2': 7.419556660834531e-07,\n",
    "     'num_leaves': 254,\n",
    "     'feature_fraction': 1.0,\n",
    "     'bagging_fraction': 0.9773374137350906,\n",
    "     'bagging_freq': 1,\n",
    "     'min_child_samples': 5,\n",
    "    #  'num_iterations': 200,\n",
    "    #  'early_stopping_round': 50,\n",
    "     'categorical_column': [4,\n",
    "                            5,94,95,96,97,  98,  99,  100,  101,  102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,  113,  114,  115,  116,  117,  118,  119,  120,  121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,  132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,  143,  144,  145,  146,  147,  148,  149,  150,  151,  152,  153,  154,\n",
    "      155]\n",
    "                  }\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(**best_params)\n",
    "    hr_pred = lgb_model.fit(x_train.astype(float), np.array(y_train), eval_metric='logloss').predict(x_test.astype(float))\n",
    "    print(\"---------------------\")\n",
    "    y_proba_train = lgb_model.predict_proba(x_train.astype(float))[:,1]\n",
    "    y_proba = lgb_model.predict_proba(x_test.astype(float))[:,1]\n",
    "    print('AUC train:',roc_auc_score(y_train,y_proba_train))    \n",
    "    print('AUC test :',roc_auc_score(y_test,y_proba))\n",
    "    print(classification_report(np.array(y_test), hr_pred))\n",
    "    plt.clf()\n",
    "    lgb.plot_importance(lgb_model) \n",
    "    plot_importances(lgb_model, x_test)\n",
    "    return lgb_model\n",
    "\n",
    "def make_data(data_,test_rate=0.8,is_rus=True):\n",
    "    data_ = data_.sort_values('date')\n",
    "    x_ = data_.drop(['rank','date','単勝'],axis=1)\n",
    "    y_ = data_['rank']\n",
    "\n",
    "    test_rate = int(test_rate*len(x_))\n",
    "    x_train, x_test = x_.iloc[:test_rate],x_.iloc[test_rate:]\n",
    "    y_train, y_test = y_.iloc[:test_rate],y_.iloc[test_rate:]\n",
    "    if is_rus:\n",
    "        rus = RandomUnderSampler(random_state=0)\n",
    "        x_resampled, y_resampled = rus.fit_resample(x_train, y_train)\n",
    "        return x_resampled, y_resampled, x_test, y_test\n",
    "    else:\n",
    "        return x_train,y_train,x_test,y_test\n",
    "\n",
    "def make_check_data(data_,test_rate=0.8):\n",
    "    data_ = data_.sort_values('date')\n",
    "    x_ = data_.drop(['rank','date'],axis=1)\n",
    "    y_ = data_['rank']\n",
    "\n",
    "    test_rate = int(test_rate*len(x_))\n",
    "    x_train, x_check = x_.iloc[:test_rate],x_.iloc[test_rate:]\n",
    "    y_train, y_check = y_.iloc[:test_rate],y_.iloc[test_rate:]\n",
    "\n",
    "    return x_check,y_check\n",
    "\n",
    "def grid_search(x_train,y_train,x_test,y_test):\n",
    "    trains = xgb.DMatrix(x_train.astype(float), label=y_train)\n",
    "    tests = xgb.DMatrix(x_test.astype(float), label=y_test)\n",
    "\n",
    "    base_params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state':100,\n",
    "        'use_label_encoder':False\n",
    "    }\n",
    "\n",
    "    watchlist = [(trains, 'train'), (tests, 'eval')]\n",
    "    tmp_params = copy.deepcopy(base_params)\n",
    "    \n",
    "#     インナー関数\n",
    "    def optimizer(trial):\n",
    "        eta = trial.suggest_uniform('eta', 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        __lambda = trial.suggest_uniform('lambda', 0.7, 2)\n",
    "        n_estimators = trial.suggest_int('n_estimators', 3, 20)\n",
    "        learning_rate = trial.suggest_uniform('lambda', 0.01, 1)\n",
    "        reg_alpha = trial.suggest_uniform('reg_alpha', 0.01, 1)\n",
    "        reg_lambda = trial.suggest_uniform('reg_lambda', 0.01, 1)\n",
    "        importance_type = trial.suggest_categorical('importance_type',\n",
    "                                                    ['gain', 'weight', 'cover','total_gain','total_cover'])\n",
    "\n",
    "        tmp_params['eta'] = eta\n",
    "        tmp_params['max_depth'] = max_depth\n",
    "        tmp_params['lambda'] = __lambda\n",
    "        tmp_params['n_estimators'] = n_estimators\n",
    "        tmp_params['learning_rate'] = learning_rate\n",
    "        tmp_params['reg_alpha'] = reg_alpha\n",
    "        tmp_params['reg_lambda'] = reg_lambda\n",
    "        tmp_params['importance_type'] = importance_type\n",
    "        model = xgb.train(tmp_params, trains, num_boost_round=50)\n",
    "        predicts = model.predict(tests)\n",
    "        r2 = r2_score(y_test, predicts)\n",
    "        print(f'#{trial.number}, Result: {r2}, {trial.params}')\n",
    "        return r2\n",
    "    \n",
    "def predict(race_id,p,hr,r,return_tables,lgb_clf,date):\n",
    "    data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "    st = ShutubaTable(data)\n",
    "    st.preprocessing()\n",
    "    st.merge_horse_results(hr)\n",
    "    st.merge_peds(p.peds_e)\n",
    "    st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "    return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)\n",
    "    me_st = ModelEvaluator(lgb_clf, return_tables)\n",
    "\n",
    "    \n",
    "    #予測\n",
    "    scores = me_st.predict_proba(st.data_c.drop(['date'],axis=1),train=False)\n",
    "    pred = st.data_c[['馬番']].copy()\n",
    "    pred['scores'] = scores\n",
    "    print(pred.loc[race_id].sort_values('scores',ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897b3ed-08f1-452f-bc5c-840f04015aed",
   "metadata": {},
   "source": [
    "# race_id 命名規則"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840d19f-0d83-4c24-a9d6-245b20e6eac6",
   "metadata": {},
   "source": [
    "race_id 202105040802\\\n",
    "yyyy_pp_xx_xxrr\\\n",
    "y : year\\\n",
    "p : palce\\\n",
    "x : 謎\\\n",
    "r : race番号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0bd9a-f85f-4949-90e8-4915a8a43ff3",
   "metadata": {},
   "source": [
    "# r.data_c['単勝'] == st.data_c[オッズ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4a8a9-d289-4262-bbd7-e7196f6b2d2c",
   "metadata": {},
   "source": [
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27a0076-950a-49b1-9b34-5ca61c6adfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金', '着差', '通過',\n",
    "                                            '開催', '距離']]\n",
    "        self.preprocessing()\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        #horse_idをkeyにしてDataFrame型を格納\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "#         for horse_id in horse_id_list:\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "                df = pd.read_html(url)[3]\n",
    "                #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
    "                if df.columns[0]=='受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる        \n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "\n",
    "        return horse_results_df\n",
    "    \n",
    "    \n",
    "    #省略\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        #1着の着差を0にする\n",
    "        df['着差'] = df['着差'].map(lambda x: 0 if x<0 else x)\n",
    "        \n",
    "        #レース展開データ\n",
    "        #n=1: 最初のコーナー位置, n=4: 最終コーナー位置\n",
    "        def corner(x, n):\n",
    "            if type(x) != str:\n",
    "                return x\n",
    "            elif n==4:\n",
    "                return int(re.findall(r'\\d+', x)[-1])\n",
    "            elif n==1:\n",
    "                return int(re.findall(r'\\d+', x)[0])\n",
    "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
    "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
    "        \n",
    "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
    "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
    "        df['first_to_final'] = df['first_corner'] - df['final_corner']\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
    "        #race_type\n",
    "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n",
    "        #距離\n",
    "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
    "        df.drop(['距離'], axis=1, inplace=True)\n",
    "        \n",
    "        #インデックス名を与える\n",
    "        df.index.name = 'horse_id'\n",
    "    \n",
    "        self.horse_results = df\n",
    "        self.target_list = ['着順', '賞金', '着差', 'first_corner',\n",
    "                            'first_to_rank', 'first_to_final','final_to_rank']\n",
    "        \n",
    "        \n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.query('index in @horse_id_list')\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "          \n",
    "        self.average_dict = {}\n",
    "        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list]\\\n",
    "            .mean().add_suffix('_{}R'.format(n_samples))\n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            self.average_dict[column] = filtered_df.groupby(['horse_id', column])\\\n",
    "                [self.target_list].mean().add_suffix('_{}_{}R'.format(column, n_samples)).fillna(0)\n",
    "\n",
    "    \n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        self.average(horse_id_list, date, n_samples)\n",
    "        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id',\n",
    "                             right_index=True, how='left')\n",
    "        for column in ['course_len','race_type', '開催']:\n",
    "            merged_df = merged_df.merge(self.average_dict[column], \n",
    "                                        left_on=['horse_id', column],\n",
    "                                        right_index=True, how='left').fillna(0)\n",
    "        return merged_df\n",
    "    \n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat(\n",
    "            [self.merge(results, date, n_samples) for date in tqdm(date_list)]\n",
    "        )\n",
    "        return merged_df\n",
    "\n",
    "class Return:\n",
    "\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        \"\"\"\n",
    "        払い戻し表データをスクレイピングする関数\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        return_tables_df : pandas.DataFrame\n",
    "            全払い戻し表データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "\n",
    "                #普通にスクレイピングすると複勝やワイドなどが区切られないで繋がってしまう。\n",
    "                #そのため、改行コードを文字列brに変換して後でsplitする\n",
    "                f = urllib.request.urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')\n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "\n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        return return_tables_df\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0]=='複勝'][[1,2]]\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        \n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        \n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "        return df.fillna(0).astype(int)\n",
    "    \n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0]=='単勝'][[1,2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        \n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "            \n",
    "        return tansho\n",
    "    \n",
    "    @property\n",
    "    def umaren(self):\n",
    "        umaren = self.return_tables[self.return_tables[0]=='馬連'][[1,2]]\n",
    "        wins = umaren[1].str.split('-', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umaren[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def umatan(self):\n",
    "        umatan = self.return_tables[self.return_tables[0]=='馬単'][[1,2]]\n",
    "        wins = umatan[1].str.split('→', expand=True)[[0,1]].add_prefix('win_')\n",
    "        return_ = umatan[2].rename('return')  \n",
    "        df = pd.concat([wins, return_], axis=1)        \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def wide(self):\n",
    "        wide = self.return_tables[self.return_tables[0]=='ワイド'][[1,2]]\n",
    "        wins = wide[1].str.split('br', expand=True)[[0,1,2]]\n",
    "        wins = wins.stack().str.split('-', expand=True).add_prefix('win_')\n",
    "        return_ = wide[2].str.split('br', expand=True)[[0,1,2]]\n",
    "        return_ = return_.stack().rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1)\n",
    "        return df.apply(lambda x: pd.to_numeric(x.str.replace(',',''), errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def sanrentan(self):\n",
    "        rentan = self.return_tables[self.return_tables[0]=='三連単'][[1,2]]\n",
    "        wins = rentan[1].str.split('→', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = rentan[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    @property\n",
    "    def sanrenpuku(self):\n",
    "        renpuku = self.return_tables[self.return_tables[0]=='三連複'][[1,2]]\n",
    "        wins = renpuku[1].str.split('-', expand=True)[[0,1,2]].add_prefix('win_')\n",
    "        return_ = renpuku[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1) \n",
    "        return df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "class ModelEvaluator:\n",
    "\n",
    "    \n",
    "    def __init__(self, model, return_tables):\n",
    "        self.model = model\n",
    "        self.rt = Return(return_tables)\n",
    "        self.fukusho = self.rt.fukusho\n",
    "        self.tansho = self.rt.tansho\n",
    "        self.umaren = self.rt.umaren\n",
    "        self.umatan = self.rt.umatan\n",
    "        self.wide = self.rt.wide\n",
    "        self.sanrenpuku = self.rt.sanrenpuku\n",
    "        self.sanrentan = self.rt.sanrentan\n",
    "\n",
    "    \n",
    "    #3着以内に入る確率を予測\n",
    "    def predict_proba(self, X, train=True, std=True, minmax=False):\n",
    "        if train:\n",
    "            proba = pd.Series(\n",
    "                self.model.predict_proba(X.drop(['単勝'], axis=1))[:, 1], index=X.index\n",
    "            )\n",
    "        else:\n",
    "            proba = pd.Series(\n",
    "                self.model.predict_proba(X, axis=1)[:, 1], index=X.index\n",
    "            )\n",
    "        if std:\n",
    "            #レース内で標準化して、相対評価する。「レース内偏差値」みたいなもの。\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "        if minmax:\n",
    "            #データ全体を0~1にする\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "        return proba\n",
    "    \n",
    "    #0か1かを予測\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        self.proba = y_pred\n",
    "        return [0 if p<threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def score(self, y_true, X):\n",
    "        return roc_auc_score(y_true, self.predict_proba(X))\n",
    "    \n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({\"features\": X.columns, \n",
    "                                    \"importance\": self.model.feature_importances_})\n",
    "        return importances.sort_values(\"importance\", ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, X, threshold=0.5, bet_only=True):\n",
    "        pred_table = X.copy()[['馬番', '単勝']]\n",
    "        pred_table['pred'] = self.predict(X, threshold)\n",
    "        pred_table['score'] = self.proba\n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred']==1][['馬番', '単勝', 'score','pred']]\n",
    "        else:\n",
    "            return pred_table[['馬番', '単勝', 'score', 'pred']]\n",
    "        \n",
    "    def bet(self, race_id, kind, umaban, amount):\n",
    "        if kind == 'fukusho':\n",
    "            rt_1R = self.fukusho.loc[race_id]\n",
    "            return_ = (rt_1R[['win_0', 'win_1', 'win_2']]==umaban).values * \\\n",
    "                rt_1R[['return_0', 'return_1', 'return_2']].values * amount/100\n",
    "            return_ = np.sum(return_)\n",
    "        if kind == 'tansho':\n",
    "            rt_1R = self.tansho.loc[race_id]\n",
    "            return_ = (rt_1R['win']==umaban) * rt_1R['return'] * amount/100\n",
    "        if kind == 'umaren':\n",
    "            rt_1R = self.umaren.loc[race_id]\n",
    "            return_ = (set(rt_1R[['win_0', 'win_1']]) == set(umaban)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if kind == 'umatan':\n",
    "            rt_1R = self.umatan.loc[race_id]\n",
    "            return_ = (list(rt_1R[['win_0', 'win_1']]) == list(umaban))\\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if kind == 'wide':\n",
    "            rt_1R = self.wide.loc[race_id]\n",
    "            return_ = (rt_1R[['win_0', 'win_1']].\\\n",
    "                           apply(lambda x: set(x)==set(umaban), axis=1)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "            return_ = return_.sum()\n",
    "        if kind == 'sanrentan':\n",
    "            rt_1R = self.sanrentan.loc[race_id]\n",
    "            return_ = (list(rt_1R[['win_0', 'win_1', 'win_2']]) == list(umaban)) * \\\n",
    "                rt_1R['return']/100 * amount\n",
    "        if kind == 'sanrenpuku':\n",
    "            rt_1R = self.sanrenpuku.loc[race_id]\n",
    "            return_ = (set(rt_1R[['win_0', 'win_1', 'win_2']]) == set(umaban)) \\\n",
    "                * rt_1R['return']/100 * amount\n",
    "        if not (return_ >= 0):\n",
    "                return_ = amount\n",
    "        return return_\n",
    "        \n",
    "    def fukusho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(np.sum([\n",
    "                self.bet(race_id, 'fukusho', umaban, 1) for umaban in preds['馬番']\n",
    "            ]))\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def tansho_return(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        self.sample = pred_table\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(\n",
    "                np.sum([self.bet(race_id, 'tansho', umaban, 1) for umaban in preds['馬番']])\n",
    "            )\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_list.append(\n",
    "                np.sum(preds.apply(lambda x: self.bet(\n",
    "                    race_id, 'tansho', x['馬番'], 1/x['単勝']), axis=1)))\n",
    "        \n",
    "        bet_money = (1 / pred_table['単勝']).sum()\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / bet_money\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / bet_money\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umaren_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umaren', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umatan_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue   \n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in permutations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umatan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "            return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def wide_box(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'wide', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std  \n",
    "        \n",
    "    def sanrentan_box(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            if len(preds)<3:\n",
    "                continue\n",
    "            else:\n",
    "                for umaban in permutations(preds['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def sanrenpuku_box(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            if len(preds)<3:\n",
    "                continue\n",
    "            else:\n",
    "                for umaban in combinations(preds['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrenpuku', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umaren_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'umaren', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += n_aite\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umaren', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def umatan_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'umatan', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += n_aite\n",
    "                \n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in permutations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'umatan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "            return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def wide_nagashi(self, X, threshold=0.5, n_aite=5):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        \n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            return_ = 0\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[1:(n_aite+1)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'wide', [preds_jiku['馬番'].values[0], x], 1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += len(preds_aite)\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 2:\n",
    "                for umaban in combinations(preds_jiku['馬番'], 2):\n",
    "                    return_ += self.bet(race_id, 'wide', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    def sanrentan_nagashi(self, X, threshold = 1.5, n_aite=7):\n",
    "        pred_table = self.pred_table(X, threshold, bet_only = False)\n",
    "        n_bets = 0\n",
    "        return_list = []\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            preds_jiku = preds.query('pred == 1')\n",
    "            if len(preds_jiku) == 1:\n",
    "                continue\n",
    "            elif len(preds_jiku) == 2:\n",
    "                preds_aite = preds.sort_values('score', ascending = False)\\\n",
    "                    .iloc[2:(n_aite+2)]['馬番']\n",
    "                return_ = preds_aite.map(\n",
    "                    lambda x: self.bet(\n",
    "                        race_id, 'sanrentan',\n",
    "                        np.append(preds_jiku['馬番'].values, x),\n",
    "                        1\n",
    "                    )\n",
    "                ).sum()\n",
    "                n_bets += len(preds_aite)\n",
    "                return_list.append(return_)\n",
    "            elif len(preds_jiku) >= 3:\n",
    "                return_ = 0\n",
    "                for umaban in permutations(preds_jiku['馬番'], 3):\n",
    "                    return_ += self.bet(race_id, 'sanrentan', umaban, 1)\n",
    "                    n_bets += 1\n",
    "                return_list.append(return_)\n",
    "        \n",
    "        std = np.std(return_list) * np.sqrt(len(return_list)) / n_bets\n",
    "        \n",
    "        n_hits = np.sum([x>0 for x in return_list])\n",
    "        return_rate = np.sum(return_list) / n_bets\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "class DataProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame() #raw data\n",
    "        self.data_p = pd.DataFrame() #after preprocessing\n",
    "        self.data_h = pd.DataFrame() #after merging horse_results\n",
    "        self.data_pe = pd.DataFrame() #after merging peds\n",
    "        self.data_c = pd.DataFrame() #after processing categorical features\n",
    "        \n",
    "    #馬の過去成績データの追加\n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "        self.data_h.drop(['開催'], axis=1, inplace=True)\n",
    "            \n",
    "    #血統データ追加\n",
    "    def merge_peds(self, peds):\n",
    "        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True,how='left')\n",
    "#         重複データを削除\n",
    "        self.data_pe = self.data_pe[~self.data_pe.duplicated()]\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()\n",
    "#         print(\"type :\",type(self.no_peds)) ndarray\n",
    "#         Peds.scrape()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "            \n",
    "        #カテゴリ変数の処理\n",
    "    def process_categorical(self, le_horse, le_jockey,results_m):\n",
    "        df = self.data_pe.copy()\n",
    "        \n",
    "        #ラベルエンコーディング。horse_id, jockey_idを0始まりの整数に変換\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        \n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "#         pedsデータのラベルエンコーディング\n",
    "\n",
    "#         for column in p.peds_e.columns:\n",
    "# #             self.le_peds_dict[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "# #             mask_peds = df[column].isin(p.le_peds[column].classes_)\n",
    "#             new_peds_id = df[column].dropna().unique()\n",
    "# #             p.le_peds[column].classes_ = np.concatenate([p.le_peds[column].classes_, new_peds_id])\n",
    "#             df[column] = p.le_peds[column].transform(df[column])\n",
    "        \n",
    "        \n",
    "        #horse_id, jockey_idをpandasのcategory型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "        \n",
    "        #そのほかのカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n",
    "        #列を一定にするため\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n",
    "        \n",
    "        self.data_c = df    \n",
    "    \n",
    "class ShutubaTable(DataProcessor):\n",
    "    \n",
    "    \n",
    "    def __init__(self, shutuba_tables):\n",
    "        super(ShutubaTable, self).__init__()\n",
    "        self.data = shutuba_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, race_id_list, date):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                    df[\"weather\"] = [text] * len(df)\n",
    "                if text in [\"良\", \"稍重\", \"重\",\"稍\"]:\n",
    "                    df[\"ground_state\"] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df[\"ground_state\"] = ['不良'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            df['date'] = [date] * len(df)\n",
    "\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_td_list = soup.find_all(\"td\", attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_td_list = soup.find_all(\"td\", attrs={'class': 'Jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['horse_id'] = list(map(lambda x: int(x),horse_id_list)) \n",
    "            df['jockey_id'] = jockey_id_list\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "#             win 環境だとなぜかintに直せない.floatならつかえる\n",
    "            df.index = df.index.astype(int)\n",
    "            data = data.append(df)\n",
    "\n",
    "            \n",
    "        return data\n",
    "                \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "#         体重変化をデータから消した\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df = df[df[\"馬体重(増減)\"] != '--']\n",
    "        df[\"体重\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "        df[\"体重変化\"] = df[\"馬体重(増減)\"].str.split(\"(\", expand=True)[1].str[:-1].replace('前計不',0).astype(int)\n",
    "\n",
    "\n",
    "        \n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        \n",
    "        df['枠'] = df['枠'].astype(int)\n",
    "        df['馬番'] = df['馬番'].astype(int)\n",
    "        df['斤量'] = df['斤量'].astype(int)\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        df['n_horse'] = df.index.map(lambda x: len(df.loc[x]))\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df = df[['枠', '馬番', '斤量', 'course_len', 'weather','race_type',\n",
    "        'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢','開催','n_horse','体重','体重変化']]\n",
    "        \n",
    "        self.data_p = df.rename(columns={'枠': '枠番'})\n",
    "        \n",
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "        self.le_peds = None\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        #race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(0.5)\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "                #メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "                html = requests.get(url)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "\n",
    "                #馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            #存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            #Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "        \n",
    "    #前処理    \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "#         rank学習の場合はそのまま\n",
    "#         df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
    "        df['rank'] = df['着順']\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\", '馬名', '騎手', '人気', '着順'],\n",
    "                axis=1, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "        df['n_horse'] = df.index.map(lambda x: len(df.loc[x]))\n",
    "        \n",
    "        self.data_p = df\n",
    "    \n",
    "    #カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])\n",
    "#         self.le_peds = p.le_peds_dict\n",
    "        super().process_categorical(self.le_horse, self.le_jockey,self.data_pe)\n",
    "        \n",
    "class Peds:\n",
    "\n",
    "    def __init__(self, peds):\n",
    "        self.peds = peds\n",
    "        self.peds_e = pd.DataFrame() #after label encoding and transforming into category\n",
    "        self.peds_re = pd.DataFrame()\n",
    "        self.peds_vec = pd.DataFrame()\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.concat([pd.read_pickle(path) for path in path_list])\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "#         for horse_id in horse_id_list:\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            \n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                #重複を削除して1列のSeries型データに直す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "\n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #列名をpeds_0, ..., peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict],\n",
    "                            axis=1).T.add_prefix('peds_')\n",
    "        peds_df.index =peds_df.index.astype(int)\n",
    "\n",
    "        return peds_df\n",
    "    \n",
    "    \n",
    "#     血統データが正規化されたいないデータに対して, 正規化する関数\n",
    "    def regularize_peds(self):\n",
    "        peds = self.peds.copy()\n",
    "        error_idx_list = []\n",
    "        for idx in tqdm(peds.index):\n",
    "            for col in peds.columns:\n",
    "            #     漢字 : 一-龥\n",
    "                code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％一-龥\\d]')\n",
    "                try:\n",
    "                    cleaned_text = code_regex.sub('', peds[col].loc[idx])\n",
    "                    one_word = \"\".join(cleaned_text.split())\n",
    "                    p_alphabet = re.compile('[a-zA-Z]+')\n",
    "                    p_katakana = re.compile(r'[ァ-ヶー]+')\n",
    "\n",
    "                    peds[col].loc[idx] = one_word\n",
    "                    if (not p_alphabet.fullmatch(one_word)) and not (p_katakana.fullmatch(one_word)):\n",
    "                        peds[col].loc[idx] = re.sub('[a-zA-Z]+', '', one_word)\n",
    "                except:\n",
    "                    error_idx_list.append(idx)\n",
    "        self.error_idx_list_r = error_idx_list\n",
    "        self.peds_re = peds\n",
    "\n",
    "    \n",
    "    def encode(self):\n",
    "        df = self.peds.copy()\n",
    "        self.le_peds_dict = {}\n",
    "        \n",
    "        \n",
    "        for column in df.columns:\n",
    "            \n",
    "            self.le_peds_dict[column] = LabelEncoder()\n",
    "            df[column] = self.le_peds_dict[column].fit_transform(df[column].fillna('Na'))\n",
    "#             df[column] = self.le_peds_dict[column]\n",
    "        self.peds_e = df.astype('category')\n",
    "        self.le_peds = self.le_peds_dict\n",
    "        \n",
    "        \n",
    "#         血統データをベクトル化する関数\n",
    "# peds_re は 正規化済み血統データを仮定\n",
    "# model_ft : fasttextモデル\n",
    "    def vectorize(self,peds_re,model_ft):\n",
    "        df = peds_re.copy()\n",
    "        error_idx_list = []\n",
    "        for idx in tqdm(df.index):\n",
    "            text = ','.join(df.loc[idx].tolist())\n",
    "            df.loc[idx] = model_ft[text]\n",
    "#             except:\n",
    "#                 error_idx_list.append(idx)\n",
    "        self.error_idx_list_v = error_idx_list\n",
    "        self.peds_vec = df.astype('float')\n",
    "#     def vectorize(self,peds_re,model_ft):\n",
    "#         df = peds_re.copy()\n",
    "        \n",
    "#         for idx in tqdm(df.index):\n",
    "#             for column in df.columns:\n",
    "#                 horse_name = df[column].loc[idx]\n",
    "#                 df[column].loc[idx] = model_ft[horse_name][0]\n",
    "\n",
    "#         self.peds_vec = df.astype('float')\n",
    "\n",
    "class Simulater():\n",
    "    \n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.return_tables = None\n",
    "        self.pred_df = None\n",
    "    \n",
    "\n",
    "    #     当日のデータでシミュレートするとあかん\n",
    "    def return_table(self, race_id_list):\n",
    "        return_tables = Return.scrape(race_id_list)\n",
    "        return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    \n",
    "    def return_table_today(self,race_id_list):\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = 'https://race.netkeiba.com/race/result.html?race_id='+race_id+'&amp;rf=race_submenu'\n",
    "                dfs = pd.read_html(url)\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(0.5)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "            #pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        self.return_tables = return_tables_df\n",
    "    \n",
    "   \n",
    "    def return_pred_table(self,st,return_tables):\n",
    "        me_st = ModelEvaluator(self.model, return_tables)\n",
    "        #予測\n",
    "        scores = me_st.predict_proba(st.data_c.drop(['date'],axis=1),train=False)\n",
    "        pred = st.data_c[['馬番']].copy()\n",
    "        pred['scores'] = scores\n",
    "        pred.index = pred.index.astype(int)\n",
    "        return pred\n",
    "        \n",
    "        \n",
    "    def show_results(self , st ,race_id_list , return_tables,bet = 100):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        target_race_dict = {}\n",
    "        self.pred_df = self.return_pred_table(st,return_tables)\n",
    "        tansho_list = []\n",
    "        fukusho_list = []\n",
    "        wide_list =[]\n",
    "\n",
    "        for race_id in race_id_list:\n",
    "            df_  = self.return_tables.loc[race_id]\n",
    "            print(\"-------------------\")\n",
    "            print(\"predict\")\n",
    "            pred_df = self.pred_df.loc[int(race_id)]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            print(pred_df.iloc[:3])\n",
    "            print(\"actual\")\n",
    "            print(self.return_tables.loc[race_id])\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "\n",
    "\n",
    "            if  pred_1 == df_[df_[0]=='単勝'][1].values[0]:\n",
    "                acc_dict['単勝'] += 1\n",
    "                profit = int(df_[df_[0]=='単勝'][2].values[0].replace('円','').replace(',',''))\n",
    "                return_dict['単勝'] += profit\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                tansho_list.append(race_id[-2:])\n",
    "                fukusho_list.append(race_id[-2:])\n",
    "\n",
    "            elif pred_1 in df_[df_[0]=='複勝'][1].str.split(' ')[0]:\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                fukusho_list.append(race_id[-2:])\n",
    "                \n",
    "\n",
    "            for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "                if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                    if i!=0:\n",
    "                        return_index = i-1\n",
    "                    else:\n",
    "                        return_index = i\n",
    "                    profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                    return_dict['ワイド'] += profit\n",
    "                    print(\"profit\",profit)\n",
    "                    acc_dict['ワイド'] += 1\n",
    "                    wide_list.append(race_id[-2:])\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for i, key in enumerate(acc_dict):\n",
    "            return_dict[key] -= bet * len(race_id_list)\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース\",tansho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        print(\"的中レース\",fukusho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"ワイド\")\n",
    "        print(\"的中率 :\",acc_dict['ワイド'],'/',len(race_id_list))\n",
    "        print(\"収支   :\",return_dict['ワイド'],'円')\n",
    "        print(\"的中レース\",wide_list)\n",
    "             \n",
    "class RankSimulater(Simulater):\n",
    "    \n",
    "    \n",
    "    def return_pred_table(self,data_c,is_long=False):\n",
    "        \n",
    "        #予測\n",
    "        if not is_long:\n",
    "            scores = pd.Series(self.model.predict(data_c.drop(['date'],axis=1)),index=data_c.index)\n",
    "        else:\n",
    "            scores = pd.Series(self.model.predict(data_c.drop(['date','rank','単勝'],axis=1)),index=data_c.index)\n",
    "        pred = data_c[['馬番']].copy()\n",
    "        pred['scores'] = scores\n",
    "        pred = pred.sort_values('scores',ascending=False)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "#     odds以上の馬券しか買わない\n",
    "    def show_long_results(self, data_c, return_tables, kaime='tansho', odds=2.0, bet = 100):\n",
    "        if kaime=='tansho':\n",
    "            pass\n",
    "        elif kaime=='fukusho':\n",
    "            pass\n",
    "        elif kaime=='wide':\n",
    "            pass\n",
    "        elif kaime=='wide_3_box':\n",
    "            pass\n",
    "        elif kaime=='umaren':\n",
    "            pass\n",
    "        elif kaime=='umatan':\n",
    "            pass\n",
    "        elif kaime=='sanrentan':\n",
    "            pass\n",
    "        elif kaime=='sanrenpuku':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"No such kaime.\")\n",
    "\n",
    "            \n",
    "    def calc_tansho(self,data_c,return_tables,odds=2.0,bet=100):\n",
    "#         data_c = r.data_cを仮定\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=True)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "\n",
    "            \n",
    "            pred_odds = data_c[data_c['馬番']==pred_1].loc[race_id]['単勝']\n",
    "            try:\n",
    "                rank = data_c[data_c['rank']==1].loc[race_id]['馬番']\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if type(rank)!=pd.core.series.Series:\n",
    "                if  pred_1 == rank:\n",
    "                    if pred_odds>=odds and score_1!= score_2:\n",
    "                        acc_dict['単勝'] += 1\n",
    "                        profit = pred_odds*bet\n",
    "                        return_dict['単勝'] += profit\n",
    "                        tansho_list.append(race_id)\n",
    "                    else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                        not_bet_count += 1\n",
    "#                     odds低かったら買わない\n",
    "                elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                    not_bet_count+=1\n",
    "            else:\n",
    "                if  pred_1 == rank.values[0] or pred_1 == rank.values[1]:\n",
    "                    if pred_odds>=odds and score_1!= score_2:\n",
    "                        acc_dict['単勝'] += 1\n",
    "                        profit = pred_odds*bet\n",
    "                        return_dict['単勝'] += profit\n",
    "                        tansho_list.append(race_id)\n",
    "                    else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                        not_bet_count += 1\n",
    "                elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                    not_bet_count+=1\n",
    "\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['単勝'] -= bet * real_race_len\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/real_race_len*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "#         print(\"的中レース\",tansho_list)\n",
    "\n",
    "    \n",
    "    def calc_tansho_top3(self,data_c,return_tables,odds=2.0,bet=100):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=True)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = pred_df['馬番'].iloc[0]\n",
    "            pred_2 = pred_df['馬番'].iloc[1]\n",
    "            pred_3 = pred_df['馬番'].iloc[1]\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "        \n",
    "        \n",
    "            odds_tmp = return_tables.loc[race_id].iloc[0][2].split('br')\n",
    "            real_odds = int(odds_tmp[0])/100\n",
    "            \n",
    "            \n",
    "            \n",
    "            rank_tmp = df_.iloc[0][1].split('br')\n",
    "            rank = int(rank_tmp[0])\n",
    "            # df_.iloc[0]が単勝\n",
    "            # df_.iloc[1]が複勝, etc..\n",
    "            # df_.iloc[x][1] が１着の馬番\n",
    "            # df_.iloc[x][2] がodds\n",
    "            # df_.iloc[x][3] が人気\n",
    "\n",
    "            if  pred_1 == rank or pred_2 == rank or pred_3==rank:\n",
    "                if real_odds>=odds and score_1!= score_2:\n",
    "                    acc_dict['単勝'] += 1\n",
    "                    profit = real_odds*bet\n",
    "                    return_dict['単勝'] += profit\n",
    "                    tansho_list.append(race_id)\n",
    "                else: #odds　低い or 出力の信頼性がないときは買わない\n",
    "                    not_bet_count += 1\n",
    "        \n",
    "#         top3 全てに賭けるから賭け金の3倍\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['単勝'] -= 3*bet * real_race_len\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list)-not_bet_count)\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        \n",
    "    \n",
    "    def calc_fukusho(self,data_c,return_tables,odds=2.0,bet=100):\n",
    "#         data_c = r.data_cを仮定\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        race_id_list = list(set(data_c.index))\n",
    "        not_bet_count = 0\n",
    "        \n",
    "        \n",
    "        for race_id in race_id_list: # race_id : int\n",
    "            pred_df = self.return_pred_table(data_c.loc[race_id],is_long=True)\n",
    "            df_  = return_tables.loc[race_id]\n",
    "            pred_df = pred_df.loc[race_id]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "#             上位２着の予測スコアが同じなら賭けない\n",
    "            score_1 = pred_df['scores'].iloc[0]\n",
    "            score_2 = pred_df['scores'].iloc[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # df_.iloc[0]が単勝\n",
    "            # df_.iloc[1]が複勝, etc..\n",
    "            # df_.iloc[x][1] が１着の馬番\n",
    "            # df_.iloc[x][2] がodds\n",
    "            # df_.iloc[x][3] が人気\n",
    "#             # 一着にのみかける\n",
    "# ############### 確定した odds と 単勝 odds が混在している, よくない\n",
    "            if pred_1 in df_[df_[0]=='複勝'][1].str.split('br').tolist()[0] and score_1!= score_2:\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split('br').tolist()[0].index(pred_1)\n",
    "                real_odds = int(df_[df_[0]=='複勝'][2].str.split('br').tolist()[0][return_index].replace(',',''))/100\n",
    "                \n",
    "                \n",
    "                if real_odds>=odds:    \n",
    "                    acc_dict['複勝'] += 1\n",
    "                    profit = real_odds*bet\n",
    "                    return_dict['複勝'] += profit \n",
    "                else:\n",
    "                    not_bet_count+=1\n",
    "#             odds が低かったら賭けない\n",
    "            elif data_c[data_c['馬番']==int(pred_1)].loc[race_id]['単勝']<odds:\n",
    "                not_bet_count+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        real_race_len = len(race_id_list) - not_bet_count\n",
    "        return_dict['複勝'] -= bet * real_race_len\n",
    "\n",
    "        print(\"---------------------\")\n",
    "        print(\"not_bet_count\",not_bet_count)\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',real_race_len)\n",
    "        print(\"的中% :\",'{:.2f}'.format((acc_dict['複勝']/real_race_len)*100),'%')\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        \n",
    "        \n",
    "    def calc_wide(self,data_c,return_tables,odds=2.0,bet=100):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        wide_list = []\n",
    "        race_id_list = data_c.index.tolist()\n",
    "        \n",
    "        for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "            if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                if i!=0:\n",
    "                    return_index = i-1\n",
    "                else:\n",
    "                    return_index = i\n",
    "\n",
    "            profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "            return_dict['ワイド'] += profit\n",
    "            print(\"profit\",profit)\n",
    "            acc_dict['ワイド'] += 1\n",
    "            wide_list.append(race_id[-2:])\n",
    "            break\n",
    "            \n",
    "            \n",
    "    def calc_wide_3box(self,data_c,return_tables,odds=2.0,bet=100):\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def clas_sanrenpuku(self,data_C,return_tables,odds=2.0,bet=100):\n",
    "        pass\n",
    "                \n",
    "    \n",
    "    \n",
    "    def show_results_today(self , st ,race_id_list ,bet = 100):\n",
    "        acc_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        return_dict = {'単勝':0,'複勝':0,'ワイド':0}\n",
    "        tansho_list = []\n",
    "        fukusho_list = []\n",
    "        wide_list =[]\n",
    "\n",
    "        for race_id in race_id_list:\n",
    "            self.pred_df = self.return_pred_table(st.data_c.loc[int(race_id)])\n",
    "#             self.return_tables.index =  self.return_tables.index.astype(int)\n",
    "            df_  = self.return_tables.loc[race_id]\n",
    "            print(\"-------------------\")\n",
    "            print(\"predict\")\n",
    "            pred_df = self.pred_df.loc[int(race_id)]\n",
    "            pred_df = pred_df.sort_values('scores',ascending=False)\n",
    "            print(pred_df.iloc[:3])\n",
    "            print(\"actual\")\n",
    "            print(self.return_tables.loc[race_id])\n",
    "            pred_1 = str(pred_df['馬番'].iloc[0])\n",
    "            pred_2 = str(pred_df['馬番'].iloc[1])\n",
    "\n",
    "\n",
    "            if  pred_1 == df_[df_[0]=='単勝'][1].values[0]:\n",
    "                acc_dict['単勝'] += 1\n",
    "                profit = int(df_[df_[0]=='単勝'][2].values[0].replace('円','').replace(',',''))\n",
    "                return_dict['単勝'] += profit\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                tansho_list.append(race_id[-2:])\n",
    "                fukusho_list.append(race_id[-2:])\n",
    "\n",
    "            elif pred_1 in df_[df_[0]=='複勝'][1].str.split(' ')[0]:\n",
    "                acc_dict['複勝'] += 1\n",
    "                return_index = df_[df_[0]=='複勝'][1].str.split(' ')[0].index(str(pred_1))\n",
    "                profit = int(df_[df_[0]=='複勝'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                return_dict['複勝'] += profit \n",
    "                fukusho_list.append(race_id[-2:])\n",
    "                \n",
    "\n",
    "            for i in range(len(df_[df_[0]=='ワイド'][1].str.split(' ')[0])//2):\n",
    "                if set([pred_1,pred_2])==set(df_[df_[0]=='ワイド'][1].str.split(' ')[0][i:i+2]):\n",
    "                    if i!=0:\n",
    "                        return_index = i-1\n",
    "                    else:\n",
    "                        return_index = i\n",
    "                    profit = int(df_[df_[0]=='ワイド'][2].str.split('円')[0][return_index].replace(',',''))\n",
    "                    return_dict['ワイド'] += profit\n",
    "                    print(\"profit\",profit)\n",
    "                    acc_dict['ワイド'] += 1\n",
    "                    wide_list.append(race_id[-2:])\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        for i, key in enumerate(acc_dict):\n",
    "            return_dict[key] -= bet * len(race_id_list)\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "        print(\"単勝\")\n",
    "        print(\"的中率 :\",acc_dict['単勝'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['単勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['単勝'],'円')\n",
    "        print(\"的中レース\",tansho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"複勝\")\n",
    "        print(\"的中率 :\",acc_dict['複勝'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['複勝']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['複勝'],'円')\n",
    "        print(\"的中レース\",fukusho_list)\n",
    "        print(\"---------------------\")\n",
    "        print(\"ワイド\")\n",
    "        print(\"的中率 :\",acc_dict['ワイド'],'/',len(race_id_list))\n",
    "        print(\"的中% :\",'{:.2f}'.format(acc_dict['ワイド']/len(race_id_list)*100),'%')\n",
    "        print(\"収支   :\",return_dict['ワイド'],'円')\n",
    "        print(\"的中レース\",wide_list)\n",
    "        \n",
    "class LearnLGBM():\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def get_train_data(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c738-f553-4488-8239-235c9808c184",
   "metadata": {},
   "source": [
    "# Simulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "b4c3498b-a9bf-4891-9ec4-748f1681fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "not_bet_count 24\n",
      "複勝\n",
      "的中率 : 774 / 1416\n",
      "的中% : 54.66 %\n",
      "収支   : -580.0 円\n",
      "time 18.137675046920776\n"
     ]
    }
   ],
   "source": [
    "sl = RankSimulater(lgb_rank)\n",
    "start_time = time.time()\n",
    "sl.calc_fukusho(r.data_c.iloc[-20000:].fillna(0),return_tables,odds=1.1)\n",
    "print(\"time\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920e716-5257-45d8-9831-2effe586aa2c",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cafb76f-29dc-4841-bb1b-2792c246cd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = load_csv(path_mac+'results.csv')\n",
    "horse_results = load_csv(path_mac+'horse_results.csv')\n",
    "peds = load_csv(path_mac+'peds.csv')\n",
    "# 何回やってもロードすると, nanが出る\n",
    "peds.fillna('nan',inplace=True)\n",
    "return_tables = load_csv(path_mac+'return.csv')\n",
    "return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b97e7-5e69-4334-a268-c7a57eaa10d8",
   "metadata": {},
   "source": [
    "# 日付に注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e00c431-6a37-4f54-bfd3-7a1cc01780cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022/12/31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1dd63-d51f-4e90-a4f3-7b9dad428a79",
   "metadata": {},
   "source": [
    "# peds dataを出馬表にあるやつ含めて最新にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af38dc6-7929-4486-a099-cba5c04e50ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e40ccc1fec1471691b8464bee0eaf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43da0aeb0ea443a5af179ab0bdfb8aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c75a4c65b4a4a1f8fad9d391d72225c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71942b97896a4586806976aad03e89f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092074e84bf84f4eb7eda798a50e2b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542f51b576a4af5a945873caaf26520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape peds at horse_id_list \"no_peds\"\n"
     ]
    }
   ],
   "source": [
    "# 当日の全ての出馬表をscrape\n",
    "\n",
    "race_id_list = ['2022050102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022070110{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022100106{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "data =  ShutubaTable.scrape(race_id_list, date)\n",
    "\n",
    "\n",
    "\n",
    "peds_id = data['horse_id'].astype(str).unique()\n",
    "peds_tmp = Peds.scrape(peds_id)\n",
    "new_peds = update_data(peds, peds_tmp)\n",
    "# ここで初めて学習データを作る\n",
    "p = Peds(new_peds)\n",
    "p.encode()\n",
    "p.peds_e.index = p.peds_e.index.astype(int)\n",
    "\n",
    "r = Results(results)\n",
    "#前処理\n",
    "r.preprocessing()\n",
    "#馬の過去成績データ追加\n",
    "# 過去聖遺跡データも最新にupdateする\n",
    "horse_id_list = data['horse_id'].astype(str).unique()\n",
    "horse_results_tmp = HorseResults.scrape(horse_id_list)\n",
    "new_horse_results = update_data(horse_results,horse_results_tmp)\n",
    "hr = HorseResults(horse_results)\n",
    "r.merge_horse_results(hr)\n",
    "r.merge_peds(p.peds_e)\n",
    "#カテゴリ変数の処理\n",
    "# pedsは既にカテゴリ化したdataをconcatしているので, ここでカテゴリ化せずとも良い\n",
    "r.process_categorical()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train,_ = split_data(r.data_c,test_size=0)\n",
    "x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_train = train['rank']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff135d8f-2a18-4583-8747-68c2938e6b79",
   "metadata": {},
   "source": [
    "# race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca02207f-1319-48d2-943f-195f395e1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 202206030101\n",
    "race_id_list = ['2022060301{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090201{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022070205{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060207{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090112{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022100204{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "\n",
    "race_id_list += ['2022060208{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022070204{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060301{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090201{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022070205{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060302{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090202{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022070206{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060303{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090203{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# race_id_list += ['2022070206{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "\n",
    "race_id_list += ['2022060304{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090204{}'.format(str(i).zfill(2)) for i in range(1,13)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ec390-bc7f-4821-8417-76382d034041",
   "metadata": {},
   "source": [
    "# Results scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131add0a-4ac5-4133-b242-3fa2109a4762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42bd75596a2440c8f39560dfa05eb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# race_id_list = results.index.astype('str')\n",
    "\n",
    "results = Results.scrape(race_id_list)\n",
    "\n",
    "results.to_csv(path_mac+'results_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ac189-894a-48ac-adb2-4e731640bc5c",
   "metadata": {},
   "source": [
    "# Horse_results scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb495b3-1044-4b5a-afdc-cdf1b60aa6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede82f7045884ba9a697a5b7d4045a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "horse_id_list = results['horse_id'].astype(str).unique()\n",
    "horse_results = HorseResults.scrape(horse_id_list)\n",
    "# save_path = '/Users/rince/Desktop/Horse/Data/horse_2020.csv'\n",
    "horse_results.to_csv(path_mac+'horse_results_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1569216-aa8f-40a3-8303-5d54a442b00e",
   "metadata": {},
   "source": [
    "# Peds scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56d56e6b-7a60-4721-a111-892aafa204d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a7d5c9a8a4312882696fe2f2d8f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02b031cfeb947dfa3212f80547f8353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peds_2021 = Peds.scrape(horse_id_list)\n",
    "pe_2021 = Peds(peds_2021)\n",
    "pe_2021.regularize_peds()\n",
    "pe_2021.peds_re.to_csv(path_mac+'peds_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1db0da-3505-4825-8408-e7dd58205c96",
   "metadata": {},
   "source": [
    "# Return scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56a4d97d-94cc-477f-b643-5144c7173613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c6ac3435814ce897bfb8add99457af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "returns_2021 = Return.scrape(race_id_list)\n",
    "returns_2021.to_csv(path_mac+'returns_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e175b9a-1a2d-4d5d-a5e2-e771c86dd8d2",
   "metadata": {},
   "source": [
    "# update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "428fa2b8-2058-4c1d-af05-ec10e9f3768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = update_data(load_csv(path_mac+'results.csv'), load_csv(path_mac+'results_new.csv'))\n",
    "new_horse_results = update_data(load_csv(path_mac+'horse_results.csv'), load_csv(path_mac+'horse_results_new.csv'))\n",
    "new_peds = update_data(load_csv(path_mac+'peds.csv'), load_csv(path_mac+'peds_new.csv'))\n",
    "new_return = update_data(load_csv(path_mac+'return.csv'), load_csv(path_mac+'returns_new.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77364dd-545a-43d4-97aa-070ca0595356",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875dc8e7-f696-4b78-933d-4f62f3603430",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results.to_csv(path_mac2+'results.csv')\n",
    "new_horse_results.to_csv(path_mac2+'horse_results.csv')\n",
    "new_peds.to_csv(path_mac2+'peds.csv')\n",
    "new_return.to_csv(path_mac2+'return.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e35c3-7240-4c3a-9a17-e840ccfcd68b",
   "metadata": {},
   "source": [
    "# 今後の方針\n",
    "1. XGB試してみる\n",
    "2. ME 自己流につくりかえる\n",
    "3. シミュレーションとか, 自分流に変える."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274e51f-047b-4f8a-903d-47cb0ef3f1e1",
   "metadata": {},
   "source": [
    "# rank　学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb1c99-2247-4624-b4c3-c317acb7a175",
   "metadata": {},
   "source": [
    "# まずpeds を正規化して, 保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "33cd2152-fd36-4015-8dcf-fc46f5d62a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pe = Peds(peds)\n",
    "# pe.regularize_peds()\n",
    "# pe.vectorize(pe.peds_re,model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "7649ee7a-ec03-49f7-b43f-ac5ae382bdaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peds_0</th>\n",
       "      <th>peds_1</th>\n",
       "      <th>peds_2</th>\n",
       "      <th>peds_3</th>\n",
       "      <th>peds_4</th>\n",
       "      <th>peds_5</th>\n",
       "      <th>peds_6</th>\n",
       "      <th>peds_7</th>\n",
       "      <th>peds_8</th>\n",
       "      <th>peds_9</th>\n",
       "      <th>...</th>\n",
       "      <th>peds_52</th>\n",
       "      <th>peds_53</th>\n",
       "      <th>peds_54</th>\n",
       "      <th>peds_55</th>\n",
       "      <th>peds_56</th>\n",
       "      <th>peds_57</th>\n",
       "      <th>peds_58</th>\n",
       "      <th>peds_59</th>\n",
       "      <th>peds_60</th>\n",
       "      <th>peds_61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019103200</th>\n",
       "      <td>モーリス 2011 鹿毛 [血統][産駒] Roberto系</td>\n",
       "      <td>ストロベリーズ 2012 青鹿毛 [血統][産駒] FNo.[1-o]</td>\n",
       "      <td>スクリーンヒーロー 2004 栗毛 [血統][産駒]</td>\n",
       "      <td>メジロフランシス 2001 鹿毛 [血統][産駒]</td>\n",
       "      <td>コマンズ Commands() 1996 [血統][産駒]</td>\n",
       "      <td>ストロベリーフェア Strawberry Fair(英) 2001 青鹿毛 [血統][産駒]</td>\n",
       "      <td>グラスワンダー 1995 栗毛 [血統][産駒]</td>\n",
       "      <td>ランニングヒロイン 1993 鹿毛 [血統][産駒]</td>\n",
       "      <td>カーネギー Carnegie(愛) 1991 鹿毛 [血統][産駒]</td>\n",
       "      <td>メジロモントレー 1986 黒鹿毛 [血統][産駒]</td>\n",
       "      <td>...</td>\n",
       "      <td>Pieces of Eight</td>\n",
       "      <td>Klairessa</td>\n",
       "      <td>Raise a Native</td>\n",
       "      <td>Gold Digger</td>\n",
       "      <td>Nureyev</td>\n",
       "      <td>Pasadoble</td>\n",
       "      <td>Storm Bird</td>\n",
       "      <td>Weekend Surprise</td>\n",
       "      <td>Fappiano</td>\n",
       "      <td>Minstress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019103200</th>\n",
       "      <td>モーリス 2011 鹿毛 [血統][産駒] Roberto系</td>\n",
       "      <td>ストロベリーズ 2012 青鹿毛 [血統][産駒] FNo.[1-o]</td>\n",
       "      <td>スクリーンヒーロー 2004 栗毛 [血統][産駒]</td>\n",
       "      <td>メジロフランシス 2001 鹿毛 [血統][産駒]</td>\n",
       "      <td>コマンズ Commands(豪) 1996 黒鹿毛 [血統][産駒]</td>\n",
       "      <td>ストロベリーフェア Strawberry Fair(英) 2001 青鹿毛 [血統][産駒]</td>\n",
       "      <td>グラスワンダー 1995 栗毛 [血統][産駒]</td>\n",
       "      <td>ランニングヒロイン 1993 鹿毛 [血統][産駒]</td>\n",
       "      <td>カーネギー Carnegie(愛) 1991 鹿毛 [血統][産駒]</td>\n",
       "      <td>メジロモントレー 1986 黒鹿毛 [血統][産駒]</td>\n",
       "      <td>...</td>\n",
       "      <td>Pieces of Eight</td>\n",
       "      <td>Klairessa</td>\n",
       "      <td>Raise a Native</td>\n",
       "      <td>Gold Digger</td>\n",
       "      <td>Nureyev</td>\n",
       "      <td>Pasadoble</td>\n",
       "      <td>Storm Bird</td>\n",
       "      <td>Weekend Surprise</td>\n",
       "      <td>Fappiano</td>\n",
       "      <td>Minstress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    peds_0  \\\n",
       "2019103200  モーリス 2011 鹿毛 [血統][産駒] Roberto系   \n",
       "2019103200  モーリス 2011 鹿毛 [血統][産駒] Roberto系   \n",
       "\n",
       "                                         peds_1                      peds_2  \\\n",
       "2019103200  ストロベリーズ 2012 青鹿毛 [血統][産駒] FNo.[1-o]  スクリーンヒーロー 2004 栗毛 [血統][産駒]   \n",
       "2019103200  ストロベリーズ 2012 青鹿毛 [血統][産駒] FNo.[1-o]  スクリーンヒーロー 2004 栗毛 [血統][産駒]   \n",
       "\n",
       "                               peds_3                              peds_4  \\\n",
       "2019103200  メジロフランシス 2001 鹿毛 [血統][産駒]       コマンズ Commands() 1996 [血統][産駒]   \n",
       "2019103200  メジロフランシス 2001 鹿毛 [血統][産駒]  コマンズ Commands(豪) 1996 黒鹿毛 [血統][産駒]   \n",
       "\n",
       "                                                    peds_5  \\\n",
       "2019103200  ストロベリーフェア Strawberry Fair(英) 2001 青鹿毛 [血統][産駒]   \n",
       "2019103200  ストロベリーフェア Strawberry Fair(英) 2001 青鹿毛 [血統][産駒]   \n",
       "\n",
       "                              peds_6                      peds_7  \\\n",
       "2019103200  グラスワンダー 1995 栗毛 [血統][産駒]  ランニングヒロイン 1993 鹿毛 [血統][産駒]   \n",
       "2019103200  グラスワンダー 1995 栗毛 [血統][産駒]  ランニングヒロイン 1993 鹿毛 [血統][産駒]   \n",
       "\n",
       "                                        peds_8                      peds_9  \\\n",
       "2019103200  カーネギー Carnegie(愛) 1991 鹿毛 [血統][産駒]  メジロモントレー 1986 黒鹿毛 [血統][産駒]   \n",
       "2019103200  カーネギー Carnegie(愛) 1991 鹿毛 [血統][産駒]  メジロモントレー 1986 黒鹿毛 [血統][産駒]   \n",
       "\n",
       "            ...          peds_52    peds_53         peds_54      peds_55  \\\n",
       "2019103200  ...  Pieces of Eight  Klairessa  Raise a Native  Gold Digger   \n",
       "2019103200  ...  Pieces of Eight  Klairessa  Raise a Native  Gold Digger   \n",
       "\n",
       "            peds_56    peds_57     peds_58           peds_59   peds_60  \\\n",
       "2019103200  Nureyev  Pasadoble  Storm Bird  Weekend Surprise  Fappiano   \n",
       "2019103200  Nureyev  Pasadoble  Storm Bird  Weekend Surprise  Fappiano   \n",
       "\n",
       "              peds_61  \n",
       "2019103200  Minstress  \n",
       "2019103200  Minstress  \n",
       "\n",
       "[2 rows x 62 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peds.loc[2019103200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e312719-7e24-47c2-83aa-1c54ec590b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx \n",
    "for col in peds.columns:\n",
    "            #     漢字 : 一-龥\n",
    "    code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％一-龥\\d]')\n",
    "    try:\n",
    "        cleaned_text = code_regex.sub('', peds[col].loc[idx])\n",
    "        one_word = \"\".join(cleaned_text.split())\n",
    "        p_alphabet = re.compile('[a-zA-Z]+')\n",
    "        p_katakana = re.compile(r'[ァ-ヶー]+')\n",
    "\n",
    "        peds[col].loc[idx] = one_word\n",
    "        if (not p_alphabet.fullmatch(one_word)) and not (p_katakana.fullmatch(one_word)):\n",
    "            peds[col].loc[idx] = re.sub('[a-zA-Z]+', '', one_word)\n",
    "    except:\n",
    "        print('peds[col].loc[idx]',peds[col].loc[idx])\n",
    "        print(\"idx\",idx,'col',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "6ff74063-a0d2-46fd-a30e-d80d4e8b4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = Peds(peds)\n",
    "pe.regularize_peds()\n",
    "pe.vectorize(peds,model_ft)\n",
    "pe.peds_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12ff12d5-29ce-4ab4-8c8a-43c30d3653a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639fc0926b74d058bb02e9673aae99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08d41f7c5d34cebbb9fe75d477a6c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b891a0129f4371b9e07d14465b8047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba2fad5fd5248c587351c4d9e2521a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d71ec96f924d2ea3ce9054abc6200f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90336bfee2bf468f92293b2cc2461c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape peds at horse_id_list \"no_peds\"\n"
     ]
    }
   ],
   "source": [
    "peds_id = results['horse_id'].astype(str).unique()\n",
    "peds_tmp = Peds.scrape(peds_id)\n",
    "new_peds = update_data(peds, peds_tmp)\n",
    "# ここで初めて学習データを作る\n",
    "date = '2022/12/31'\n",
    "# p = Peds(peds)\n",
    "# p.encode()\n",
    "# p.peds_e.index = p.peds_e.index.astype(int)\n",
    "pe = Peds(new_peds)\n",
    "pe.regularize_peds()\n",
    "pe.vectorize(pe.peds_re,model_ft)\n",
    "\n",
    "\n",
    "r = Results(results)\n",
    "#前処理\n",
    "r.preprocessing()\n",
    "#馬の過去成績データ追加\n",
    "# 過去聖遺跡データも最新にupdateする\n",
    "# horse_id_list = data['horse_id'].astype(str).unique()\n",
    "# horse_results_tmp = HorseResults.scrape(horse_id_list)\n",
    "# new_horse_results = update_data(horse_results,horse_results_tmp)\n",
    "hr = HorseResults(horse_results)\n",
    "r.merge_horse_results(hr)\n",
    "r.merge_peds(pe.peds_vec)\n",
    "#カテゴリ変数の処理\n",
    "# pedsは既にカテゴリ化したdataをconcatしているので, ここでカテゴリ化せずとも良い\n",
    "r.process_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "52ac3a4a-1f62-4605-8edf-307aed2cc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値は 0 とした\n",
    "train, test = split_data(r.data_c.fillna(0),test_size=0.2,rank_learning=False)\n",
    "# x_train = train.drop(['rank', 'date','体重','体重変化','単勝'], axis=1)\n",
    "x_train = train.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_train = train['rank']\n",
    "\n",
    "x_test = test.drop(['rank', 'date','単勝'], axis=1)\n",
    "y_test = test['rank']\n",
    "\n",
    "train_query = x_train.groupby(x_train.index).size()\n",
    "test_query = x_test.groupby(x_test.index).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "edc24cf8-2253-4a67-bfc8-2149d412700d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45535\n",
      "[LightGBM] [Info] Number of data points in the train set: 80507, number of used features: 172\n"
     ]
    }
   ],
   "source": [
    "# best parameters: {'lambdarank_truncation_level': 7, 'learning_rate': 0.06729249537901785}\n",
    "\n",
    "# 血統データベクトル前の最適パラメタ\n",
    "# lgbm_params =  {\n",
    "#     'objective': 'lambdarank',\n",
    "#     'metric': 'ndcg',\n",
    "#     'lambdarank_truncation_level': 2,\n",
    "#     #     上位３着を考慮\n",
    "#     'ndcg_eval_at': [1,2,3],\n",
    "#     'learning_rate': 0.09841058786136925,\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'random_state': 777\n",
    "# }\n",
    "lgbm_params = {\n",
    "    'lambdarank_truncation_level': 10,\n",
    "    'metric': 'ndcg',\n",
    "    'objective': 'lambdarank',\n",
    "    'ndcg_eval_at': [1,2,3],\n",
    "    'learning_rate': 0.016371907499492487,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 777\n",
    "}\n",
    "\n",
    " #学習 \n",
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "\n",
    "lgb_rank = lgb.train(\n",
    "   lgbm_params,\n",
    "   train,\n",
    "   num_boost_round=100,\n",
    "#    valid_sets=valid,\n",
    "   valid_names=['train'],\n",
    "#    early_stopping_rounds=20,\n",
    "#    verbose_eval=5\n",
    ")\n",
    "\n",
    "# early stopping -> test data ないと怒られる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b5244-fc8d-4f9d-b9cd-69b895851b73",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fa0af40b-fb3b-42b6-a50e-0943959c14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ebee6f9a-20ac-4523-adb6-378f9fcfcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    print(\"Training start:\")\n",
    "\n",
    "    N_boost_round = []\n",
    "    Score = []\n",
    "\n",
    "    lgb_results={}  #履歴格納用\n",
    "    train = lgb.Dataset(x_train, y_train, group=train_query)\n",
    "    valid = lgb.Dataset(x_test, y_test, reference=train, group=test_query)\n",
    "\n",
    "    \n",
    "    lgb_clf = lgb.train(\n",
    "       params,\n",
    "       train,\n",
    "       num_boost_round=1000,\n",
    "       valid_sets=valid,\n",
    "       valid_names=['valid'],\n",
    "       early_stopping_rounds=20,\n",
    "       verbose_eval=5,\n",
    "       evals_result=lgb_results\n",
    "    )\n",
    "#     return lgb_results\n",
    "    return {'loss': -1.0 * lgb_results['valid']['ndcg@3'][lgb_clf.best_iteration], 'status': STATUS_OK}\n",
    "\n",
    "def optimize(trials):\n",
    "#探索スペース\n",
    "    space = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [1,2,3],\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "#         なぜか, uniformだと strに変換されてしまう\n",
    "#         lambda_rank_truncation_levelは int型\n",
    "#         よって, int以外はstrに勝手に変換されてしまい, エラーとなったのではないか\n",
    "        'lambdarank_truncation_level': hp.choice('lambdarank_truncation_level',[ 1,2\n",
    "                                                                                ,4,6,8,10]),\n",
    "#         best paramsの返り値は, choiceだとindexか？\n",
    "#         n_estimaterとか サーチしてみたい\n",
    "#         'n_estimators': hp.choice('n_estimators',[ 1,10,100,500,750]),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 777,\n",
    "    }\n",
    "\n",
    "    max_evals = 50      #探索回数(25くらいで十分)\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "    print(\"best parameters:\", best)\n",
    "\n",
    "#     return {'loss': -1.0 * lgb_results['ndcg@3'][lgb_clf.best_iteration], 'status': STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "a5cf7392-a3d9-47b2-bb0a-d6e3f17d3ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6985907-c8c8-426d-be48-086c7ba1af64",
   "metadata": {},
   "source": [
    "# milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67532ce2-4766-485a-b767-2d20e84ebc74",
   "metadata": {},
   "source": [
    "# 実際に予測するときの手順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd8daf5-2a98-4901-aaec-a88887b81c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0145293a0442ebb2c1f2b459dd3041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "race_id_list = ['2022050108{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022090104{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "race_id_list += ['2022100204{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "data =  ShutubaTable.scrape(race_id_list, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7091fe-528d-4964-91e1-8adb4dcda22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae789d47d614df493340af822cc223d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b3aca37021469a953f90dbf2bfed32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79061a99a8646ddb2d6fc72a758ba8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979da67216e54446907ac9d3f0e71c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape peds at horse_id_list \"no_peds\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 中京\n",
    "# 202207010301\n",
    "\n",
    "# race_id = 202205010211\n",
    "# race_id_list = ['2022070102{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "# data =  ShutubaTable.scrape([str(race_id)], date)\n",
    "data =  ShutubaTable.scrape(race_id_list, date)\n",
    "\n",
    "# race_id_list = ['2020010106{}'.format(str(i).zfill(2)) for i in range(1,13)]\n",
    "st = ShutubaTable(data)\n",
    "\n",
    "\n",
    "#前処理\n",
    "st.preprocessing()\n",
    "\n",
    "#馬の過去成績データ追加\n",
    "st.merge_horse_results(hr)\n",
    "\n",
    "#血統データ追加\n",
    "st.merge_peds(p.peds_e)\n",
    "\n",
    "#カテゴリ変数の処理\n",
    "st.process_categorical(r.le_horse, r.le_jockey, r.data_pe)\n",
    "# sl = RankSimulater(lgb_rank)\n",
    "# sl.return_pred_table(st.data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c64f0ed3-8651-4105-aa14-83bfa45b20c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               features  importances\n",
      "4              horse_id         1819\n",
      "5             jockey_id          558\n",
      "25      賞金_race_type_5R          123\n",
      "10                着順_5R           56\n",
      "52      着順_race_type_9R           44\n",
      "80    着順_race_type_allR           43\n",
      "32             賞金_開催_5R           41\n",
      "24      着順_race_type_5R           40\n",
      "38                着順_9R           39\n",
      "11                賞金_5R           34\n",
      "12                着差_5R           26\n",
      "53      賞金_race_type_9R           25\n",
      "6                    年齢           24\n",
      "73   着順_course_len_allR           22\n",
      "7                    体重           15\n",
      "40                着差_9R           11\n",
      "18     賞金_course_len_5R            9\n",
      "26      着差_race_type_5R            9\n",
      "39                賞金_9R            7\n",
      "17     着順_course_len_5R            7\n",
      "45     着順_course_len_9R            7\n",
      "82    着差_race_type_allR            5\n",
      "9               n_horse            4\n",
      "153             peds_59            2\n",
      "60             賞金_開催_9R            2\n",
      "31             着順_開催_5R            2\n",
      "146             peds_52            2\n",
      "68              着差_allR            2\n",
      "124             peds_30            2\n",
      "122             peds_28            2\n"
     ]
    }
   ],
   "source": [
    "importances = pd.DataFrame(\n",
    "{'features' : x_train.columns, 'importances' : lgb_rank.feature_importance()})\n",
    "print(importances.sort_values('importances', ascending=False)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb716e-ef27-4c7b-8854-c68efd89d7be",
   "metadata": {},
   "source": [
    "# Rank Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19d8dbd3-4a97-47d0-91c9-e2001aafcb56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c545a85bf745638de52587bed37d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010801  10 -1.234598\n",
      "202205010801   3 -1.313891\n",
      "202205010801   6 -1.316911\n",
      "actual\n",
      "                0                1             2          3\n",
      "202205010801   単勝               14          230円        1人気\n",
      "202205010801   複勝          14 16 3  130円220円150円  1人気6人気2人気\n",
      "202205010801   枠連              7 8          620円        2人気\n",
      "202205010801   馬連            14 16        1,220円        4人気\n",
      "202205010801  ワイド  14 16 3 14 3 16  510円310円760円  4人気1人気9人気\n",
      "202205010801   馬単            14 16        1,740円        5人気\n",
      "202205010801  3連複          3 14 16        2,340円        6人気\n",
      "202205010801  3連単          14 16 3        9,840円       16人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010802  16 -1.251063\n",
      "202205010802  12 -1.396060\n",
      "202205010802   5 -1.604587\n",
      "actual\n",
      "                0                  1             2          3\n",
      "202205010802   単勝                 16          270円        1人気\n",
      "202205010802   複勝           16 15 14  130円190円130円  2人気4人気1人気\n",
      "202205010802   枠連                8 8        1,330円        5人気\n",
      "202205010802   馬連              15 16        1,070円        4人気\n",
      "202205010802  ワイド  15 16 14 16 14 15  490円240円500円  4人気1人気5人気\n",
      "202205010802   馬単              16 15        1,690円        7人気\n",
      "202205010802  3連複           14 15 16        1,240円        2人気\n",
      "202205010802  3連単           16 15 14        5,290円       10人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010803   1 -2.574721\n",
      "202205010803   2 -2.574721\n",
      "202205010803   3 -2.574721\n",
      "actual\n",
      "                0                1             2          3\n",
      "202205010803   単勝               11          320円        1人気\n",
      "202205010803   複勝          11 9 16  140円170円190円  1人気3人気4人気\n",
      "202205010803   枠連              5 6          830円        3人気\n",
      "202205010803   馬連             9 11        1,270円        3人気\n",
      "202205010803  ワイド  9 11 11 16 9 16  480円420円530円  4人気2人気6人気\n",
      "202205010803   馬単             11 9        2,040円        4人気\n",
      "202205010803  3連複          9 11 16        2,150円        3人気\n",
      "202205010803  3連単          11 9 16        9,050円       11人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010804  16 -0.966678\n",
      "202205010804   3 -1.152127\n",
      "202205010804   2 -1.456581\n",
      "actual\n",
      "                0                1             2           3\n",
      "202205010804   単勝               14        2,140円         6人気\n",
      "202205010804   複勝          14 11 2  330円110円110円   7人気1人気2人気\n",
      "202205010804   枠連              6 7        1,010円         3人気\n",
      "202205010804   馬連            11 14        2,220円         8人気\n",
      "202205010804  ワイド  11 14 2 14 2 11  690円770円140円  9人気10人気1人気\n",
      "202205010804   馬単            14 11        7,310円        22人気\n",
      "202205010804  3連複          2 11 14        1,000円         4人気\n",
      "202205010804  3連単          14 11 2       16,440円        45人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010805   6 -0.890175\n",
      "202205010805   7 -1.291851\n",
      "202205010805   9 -1.315193\n",
      "actual\n",
      "                0            1             2          3\n",
      "202205010805   単勝            7          150円        1人気\n",
      "202205010805   複勝        7 9 8  110円110円230円  1人気2人気5人気\n",
      "202205010805   枠連          4 5          160円        1人気\n",
      "202205010805   馬連          7 9          180円        1人気\n",
      "202205010805  ワイド  7 9 7 8 8 9  130円540円540円  1人気6人気7人気\n",
      "202205010805   馬単          7 9          240円        1人気\n",
      "202205010805  3連複        7 8 9          990円        3人気\n",
      "202205010805  3連単        7 9 8        2,390円        6人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010806  16  0.007486\n",
      "202205010806   8 -0.862449\n",
      "202205010806   3 -1.154240\n",
      "actual\n",
      "                0              1             2          3\n",
      "202205010806   単勝             14          150円        1人気\n",
      "202205010806   複勝         14 9 6  110円220円140円  1人気4人気2人気\n",
      "202205010806   枠連            5 7          780円        3人気\n",
      "202205010806   馬連           9 14          960円        3人気\n",
      "202205010806  ワイド  9 14 6 14 6 9  380円180円630円  3人気1人気6人気\n",
      "202205010806   馬単           14 9        1,200円        3人気\n",
      "202205010806  3連複         6 9 14        1,270円        2人気\n",
      "202205010806  3連単         14 9 6        4,180円        7人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010807  10  0.416925\n",
      "202205010807   3  0.054242\n",
      "202205010807   6 -0.747792\n",
      "actual\n",
      "                0              1             2          3\n",
      "202205010807   単勝              5          390円        2人気\n",
      "202205010807   複勝         5 8 16  150円190円210円  1人気4人気5人気\n",
      "202205010807   枠連            3 4        1,230円        6人気\n",
      "202205010807   馬連            5 8        1,330円        5人気\n",
      "202205010807  ワイド  5 8 5 16 8 16  440円570円770円  3人気6人気9人気\n",
      "202205010807   馬単            5 8        2,110円        5人気\n",
      "202205010807  3連複         5 8 16        2,950円        7人気\n",
      "202205010807  3連単         5 8 16       13,160円       25人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010808  12  1.953192\n",
      "202205010808   2  1.074999\n",
      "202205010808   8 -0.098584\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202205010808   単勝              4              230円          1人気\n",
      "202205010808   複勝         4 15 2      120円560円280円    1人気7人気5人気\n",
      "202205010808   枠連            2 8            1,560円          7人気\n",
      "202205010808   馬連           4 15            2,730円         10人気\n",
      "202205010808  ワイド  4 15 2 4 2 15  1,110円590円3,780円  13人気5人気32人気\n",
      "202205010808   馬単           4 15            3,360円         12人気\n",
      "202205010808  3連複         2 4 15            7,980円         23人気\n",
      "202205010808  3連単         4 15 2           23,290円         66人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010809   3 -0.062736\n",
      "202205010809   4 -0.433635\n",
      "202205010809   1 -1.112188\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202205010809   単勝              3              360円          2人気\n",
      "202205010809   複勝         3 14 6      160円290円690円    1人気5人気8人気\n",
      "202205010809   枠連            3 8            1,720円          8人気\n",
      "202205010809   馬連           3 14            2,060円          8人気\n",
      "202205010809  ワイド  3 14 3 6 6 14  710円2,200円3,600円  8人気23人気34人気\n",
      "202205010809   馬単           3 14            3,590円         15人気\n",
      "202205010809  3連複         3 6 14           15,530円         46人気\n",
      "202205010809  3連単         3 14 6           60,420円        198人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010810  10  1.216225\n",
      "202205010810   2  0.296137\n",
      "202205010810   6 -0.042707\n",
      "actual\n",
      "                0            1             2          3\n",
      "202205010810   単勝            4          590円        3人気\n",
      "202205010810   複勝        4 2 8  160円110円170円  3人気1人気4人気\n",
      "202205010810   枠連          2 4          560円        2人気\n",
      "202205010810   馬連          2 4          550円        2人気\n",
      "202205010810  ワイド  2 4 4 8 2 8  250円480円280円  2人気8人気3人気\n",
      "202205010810   馬単          4 2        1,440円        5人気\n",
      "202205010810  3連複        2 4 8          810円        2人気\n",
      "202205010810  3連単        4 2 8        5,490円       15人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010811   1  1.083133\n",
      "202205010811  10  0.357368\n",
      "202205010811  13  0.233240\n",
      "actual\n",
      "                0                1                   2            3\n",
      "202205010811   単勝                6                510円          2人気\n",
      "202205010811   複勝          6 15 11        230円310円410円    2人気5人気7人気\n",
      "202205010811   枠連              3 8                700円          2人気\n",
      "202205010811   馬連             6 15              2,640円          8人気\n",
      "202205010811  ワイド  6 15 6 11 11 15  1,230円1,980円3,830円  9人気22人気45人気\n",
      "202205010811   馬単             6 15              4,690円         13人気\n",
      "202205010811  3連複          6 11 15             16,200円         53人気\n",
      "202205010811  3連単          6 15 11             76,940円        240人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202205010812   8  2.261152\n",
      "202205010812   7 -0.300201\n",
      "202205010812  14 -0.404721\n",
      "actual\n",
      "                0                  1               2           3\n",
      "202205010812   単勝                 12            260円         1人気\n",
      "202205010812   複勝           12 10 16    140円380円350円   1人気7人気6人気\n",
      "202205010812   枠連                5 6          1,030円         6人気\n",
      "202205010812   馬連              10 12          2,120円         6人気\n",
      "202205010812  ワイド  10 12 12 16 10 16  860円690円2,970円  8人気5人気32人気\n",
      "202205010812   馬単              12 10          2,830円         9人気\n",
      "202205010812  3連複           10 12 16          7,610円        19人気\n",
      "202205010812  3連単           12 10 16         27,890円        71人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010401   5 -1.502265\n",
      "202209010401   9 -2.543418\n",
      "202209010401   6 -3.006866\n",
      "actual\n",
      "                0            1             2           3\n",
      "202209010401   単勝            5          150円         1人気\n",
      "202209010401   複勝        5 9 2  110円160円190円   1人気3人気4人気\n",
      "202209010401   枠連          5 8          670円         3人気\n",
      "202209010401   馬連          5 9          770円         3人気\n",
      "202209010401  ワイド  5 9 2 5 2 9  360円370円840円  3人気4人気13人気\n",
      "202209010401   馬単          5 9        1,150円         5人気\n",
      "202209010401  3連複        2 5 9        1,780円         9人気\n",
      "202209010401  3連単        5 9 2        5,950円        23人気\n",
      "profit 360\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010402   1 -1.048743\n",
      "202209010402   2 -1.136142\n",
      "202209010402  13 -1.928295\n",
      "actual\n",
      "                0            1               2           3\n",
      "202209010402   単勝            9            300円         2人気\n",
      "202209010402   複勝        9 8 4    150円450円280円   1人気7人気4人気\n",
      "202209010402   枠連          5 5          2,220円        10人気\n",
      "202209010402   馬連          8 9          1,970円         7人気\n",
      "202209010402  ワイド  8 9 4 9 4 8  670円820円2,530円  5人気9人気27人気\n",
      "202209010402   馬単          9 8          3,070円        10人気\n",
      "202209010402  3連複        4 8 9          9,210円        32人気\n",
      "202209010402  3連単        9 8 4         27,870円        91人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010403  11 -1.195228\n",
      "202209010403   6 -1.883671\n",
      "202209010403   2 -2.185601\n",
      "actual\n",
      "                0            1               2            3\n",
      "202209010403   単勝            6            290円          1人気\n",
      "202209010403   複勝        6 5 2    160円280円240円    2人気6人気4人気\n",
      "202209010403   枠連          5 5          1,560円          9人気\n",
      "202209010403   馬連          5 6          1,640円          9人気\n",
      "202209010403  ワイド  5 6 2 6 2 5  710円440円1,490円  10人気3人気23人気\n",
      "202209010403   馬単          6 5          2,900円         10人気\n",
      "202209010403  3連複        2 5 6          4,760円         22人気\n",
      "202209010403  3連単        6 5 2         16,730円         53人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010404   1 -2.574721\n",
      "202209010404   2 -2.574721\n",
      "202209010404   3 -2.574721\n",
      "actual\n",
      "                0                1               2           3\n",
      "202209010404   単勝                2            440円         2人気\n",
      "202209010404   複勝          2 14 13    150円110円460円   2人気1人気9人気\n",
      "202209010404   枠連              1 7            440円         1人気\n",
      "202209010404   馬連             2 14            410円         1人気\n",
      "202209010404  ワイド  2 14 2 13 13 14  190円1,380円650円  1人気13人気8人気\n",
      "202209010404   馬単             2 14          1,280円         4人気\n",
      "202209010404  3連複          2 13 14          2,250円         5人気\n",
      "202209010404  3連単          2 14 13         12,100円        31人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010405  10 -0.803773\n",
      "202209010405  12 -1.111741\n",
      "202209010405   4 -1.207214\n",
      "actual\n",
      "                0              1               2            3\n",
      "202209010405   単勝              2            520円          3人気\n",
      "202209010405   複勝         2 12 1    200円150円420円    4人気2人気6人気\n",
      "202209010405   枠連            2 7          1,450円          6人気\n",
      "202209010405   馬連           2 12          1,390円          5人気\n",
      "202209010405  ワイド  2 12 1 2 1 12  430円870円1,150円  4人気11人気13人気\n",
      "202209010405   馬単           2 12          2,950円         11人気\n",
      "202209010405  3連複         1 2 12          5,580円         16人気\n",
      "202209010405  3連単         2 12 1         23,130円         76人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010406   8 -0.282895\n",
      "202209010406   4 -0.645267\n",
      "202209010406  12 -0.756158\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209010406   単勝            4          370円        1人気\n",
      "202209010406   複勝        4 8 2  140円220円150円  1人気6人気2人気\n",
      "202209010406   枠連          4 6          430円        1人気\n",
      "202209010406   馬連          4 8        1,480円        6人気\n",
      "202209010406  ワイド  4 8 2 4 2 8  540円350円670円  6人気1人気9人気\n",
      "202209010406   馬単          4 8        2,570円        7人気\n",
      "202209010406  3連複        2 4 8        1,910円        3人気\n",
      "202209010406  3連単        4 8 2       10,050円       13人気\n",
      "profit 540\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010407  11 -0.959477\n",
      "202209010407   7 -1.350909\n",
      "202209010407   4 -1.360053\n",
      "actual\n",
      "                0                1             2          3\n",
      "202209010407   単勝               11          560円        2人気\n",
      "202209010407   複勝          11 7 14  170円140円130円  4人気2人気1人気\n",
      "202209010407   枠連              5 7        1,850円        9人気\n",
      "202209010407   馬連             7 11        1,530円        5人気\n",
      "202209010407  ワイド  7 11 11 14 7 14  430円340円300円  3人気2人気1人気\n",
      "202209010407   馬単             11 7        2,890円       11人気\n",
      "202209010407  3連複          7 11 14        1,350円        1人気\n",
      "202209010407  3連単          11 7 14        9,760円       16人気\n",
      "profit 430\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010408   4  0.536544\n",
      "202209010408  14 -0.358380\n",
      "202209010408   5 -0.666008\n",
      "actual\n",
      "                0              1             2           3\n",
      "202209010408   単勝              4          190円         1人気\n",
      "202209010408   複勝         4 3 14  110円240円200円   1人気5人気4人気\n",
      "202209010408   枠連            2 2          990円         4人気\n",
      "202209010408   馬連            3 4        1,010円         4人気\n",
      "202209010408  ワイド  3 4 4 14 3 14  430円330円960円  4人気2人気11人気\n",
      "202209010408   馬単            4 3        1,210円         4人気\n",
      "202209010408  3連複         3 4 14        2,280円         6人気\n",
      "202209010408  3連単         4 3 14        6,720円        13人気\n",
      "profit 330\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010409   8 -0.735026\n",
      "202209010409   2 -1.061181\n",
      "202209010409   7 -1.555414\n",
      "actual\n",
      "                0            1             2          3\n",
      "202209010409   単勝            4          280円        2人気\n",
      "202209010409   複勝        4 5 9  130円240円110円  1人気5人気3人気\n",
      "202209010409   枠連          4 5        2,110円        7人気\n",
      "202209010409   馬連          4 5        1,970円        6人気\n",
      "202209010409  ワイド  4 5 4 9 5 9  430円170円400円  6人気1人気4人気\n",
      "202209010409   馬単          4 5        3,040円       10人気\n",
      "202209010409  3連複        4 5 9          970円        2人気\n",
      "202209010409  3連単        4 5 9        8,160円       25人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010410   9 -0.233131\n",
      "202209010410   4 -0.554119\n",
      "202209010410   2 -0.554806\n",
      "actual\n",
      "                0            1                 2            3\n",
      "202209010410   単勝            3              390円          2人気\n",
      "202209010410   複勝        3 2 1      200円310円640円    1人気4人気6人気\n",
      "202209010410   枠連          2 3            1,510円          6人気\n",
      "202209010410   馬連          2 3            1,120円          4人気\n",
      "202209010410  ワイド  2 3 1 3 1 2  410円1,040円1,460円  4人気11人気17人気\n",
      "202209010410   馬単          3 2            1,930円          8人気\n",
      "202209010410  3連複        1 2 3            5,630円         17人気\n",
      "202209010410  3連単        3 2 1           23,610円         71人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010411   4  0.745287\n",
      "202209010411  10  0.639916\n",
      "202209010411  16  0.250642\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202209010411   単勝             16              440円          2人気\n",
      "202209010411   複勝         16 6 3      190円130円760円   2人気1人気11人気\n",
      "202209010411   枠連            3 8              580円          1人気\n",
      "202209010411   馬連           6 16              720円          1人気\n",
      "202209010411  ワイド  6 16 3 16 3 6  310円3,380円2,180円  1人気43人気28人気\n",
      "202209010411   馬単           16 6            1,360円          2人気\n",
      "202209010411  3連複         3 6 16            9,710円         29人気\n",
      "202209010411  3連単         16 6 3           37,920円        112人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202209010412  13  1.684908\n",
      "202209010412   7  0.771902\n",
      "202209010412   5 -1.125634\n",
      "actual\n",
      "                0              1             2          3\n",
      "202209010412   単勝              7          240円        1人気\n",
      "202209010412   複勝         7 1 13  120円160円150円  1人気3人気2人気\n",
      "202209010412   枠連            1 5          800円        3人気\n",
      "202209010412   馬連            1 7          680円        2人気\n",
      "202209010412  ワイド  1 7 7 13 1 13  260円220円550円  2人気1人気5人気\n",
      "202209010412   馬単            7 1        1,070円        2人気\n",
      "202209010412  3連複         1 7 13        1,310円        1人気\n",
      "202209010412  3連単         7 1 13        5,730円        4人気\n",
      "profit 220\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020401   7 -0.832685\n",
      "202210020401   6 -1.334914\n",
      "202210020401   3 -1.559199\n",
      "actual\n",
      "                0              1                 2            3\n",
      "202210020401   単勝              3              690円          3人気\n",
      "202210020401   複勝         3 13 7    270円2,530円370円   3人気11人気4人気\n",
      "202210020401   枠連            2 7           10,560円         24人気\n",
      "202210020401   馬連           3 13           28,890円         42人気\n",
      "202210020401  ワイド  3 13 3 7 7 13  6,730円780円8,940円  42人気8人気50人気\n",
      "202210020401   馬単           3 13           45,400円         70人気\n",
      "202210020401  3連複         3 7 13           59,780円        103人気\n",
      "202210020401  3連単         3 13 7          410,210円        566人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020402   6 -0.640959\n",
      "202210020402   4 -1.468175\n",
      "202210020402   3 -1.761449\n",
      "actual\n",
      "                0                1                 2            3\n",
      "202210020402   単勝               11              370円          2人気\n",
      "202210020402   複勝          11 4 14      160円160円600円   2人気3人気10人気\n",
      "202210020402   枠連              3 6              480円          1人気\n",
      "202210020402   馬連             4 11              680円          1人気\n",
      "202210020402  ワイド  4 11 11 14 4 14  370円1,510円1,670円  3人気20人気23人気\n",
      "202210020402   馬単             11 4            1,470円          4人気\n",
      "202210020402  3連複          4 11 14            5,990円         21人気\n",
      "202210020402  3連単          11 4 14           21,710円         63人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020403  11  0.158777\n",
      "202210020403   3 -0.093334\n",
      "202210020403   8 -0.450858\n",
      "actual\n",
      "                0              1               2           3\n",
      "202210020403   単勝              8            710円         3人気\n",
      "202210020403   複勝         8 9 11    290円300円190円   5人気4人気2人気\n",
      "202210020403   枠連            5 6          3,690円        12人気\n",
      "202210020403   馬連            8 9          3,200円        11人気\n",
      "202210020403  ワイド  8 9 8 11 9 11  1,090円750円760円  12人気6人気8人気\n",
      "202210020403   馬単            8 9          7,300円        23人気\n",
      "202210020403  3連複         8 9 11          4,800円        13人気\n",
      "202210020403  3連単         8 9 11         29,450円        92人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020404   6 -1.587430\n",
      "202210020404   1 -2.085934\n",
      "202210020404   7 -2.677076\n",
      "actual\n",
      "                0                1             2          3\n",
      "202210020404   単勝               10          180円        1人気\n",
      "202210020404   複勝          10 4 11  110円180円180円  1人気3人気4人気\n",
      "202210020404   枠連              4 8          500円        1人気\n",
      "202210020404   馬連             4 10          590円        1人気\n",
      "202210020404  ワイド  4 10 10 11 4 11  250円250円630円  3人気2人気8人気\n",
      "202210020404   馬単             10 4          790円        2人気\n",
      "202210020404  3連複          4 10 11        1,060円        2人気\n",
      "202210020404  3連単          10 4 11        2,670円        3人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020405   3 -1.022404\n",
      "202210020405  10 -1.457789\n",
      "202210020405  17 -1.689679\n",
      "actual\n",
      "                0              1                    2              3\n",
      "202210020405   単勝             17                 200円            1人気\n",
      "202210020405   複勝         17 8 6     130円1,370円1,390円    1人気12人気13人気\n",
      "202210020405   枠連            4 8               4,320円           13人気\n",
      "202210020405   馬連           8 17               9,170円           28人気\n",
      "202210020405  ワイド  8 17 6 17 6 8  2,890円2,350円26,560円  29人気25人気106人気\n",
      "202210020405   馬単           17 8              14,020円           41人気\n",
      "202210020405  3連複         6 8 17             134,840円          240人気\n",
      "202210020405  3連単         17 8 6             527,840円          994人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020406   7 -1.683949\n",
      "202210020406   2 -2.162376\n",
      "202210020406   4 -2.350711\n",
      "actual\n",
      "                0              1               2            3\n",
      "202210020406   単勝             14            790円          4人気\n",
      "202210020406   複勝         14 2 7    320円160円290円    5人気2人気4人気\n",
      "202210020406   枠連            1 7          1,280円          5人気\n",
      "202210020406   馬連           2 14          2,370円          7人気\n",
      "202210020406  ワイド  2 14 7 14 2 7  990円1,620円700円  10人気18人気5人気\n",
      "202210020406   馬単           14 2          4,830円         16人気\n",
      "202210020406  3連複         2 7 14          6,650円         18人気\n",
      "202210020406  3連単         14 2 7         30,430円         78人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020407  13 -0.236554\n",
      "202210020407   3 -0.304679\n",
      "202210020407   9 -0.637772\n",
      "actual\n",
      "                0                1               2           3\n",
      "202210020407   単勝               15            390円         1人気\n",
      "202210020407   複勝          15 6 11    140円310円200円   1人気5人気2人気\n",
      "202210020407   枠連              3 8          1,580円         6人気\n",
      "202210020407   馬連             6 15          3,000円         9人気\n",
      "202210020407  ワイド  6 15 11 15 6 11  870円540円1,830円  5人気2人気23人気\n",
      "202210020407   馬単             15 6          4,840円        11人気\n",
      "202210020407  3連複          6 11 15          7,470円        13人気\n",
      "202210020407  3連単          15 6 11         38,130円        68人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020408   2 -1.481265\n",
      "202210020408   8 -1.542226\n",
      "202210020408  10 -1.588125\n",
      "actual\n",
      "                0            1             2          3\n",
      "202210020408   単勝            8          280円        1人気\n",
      "202210020408   複勝        8 5 6  140円230円180円  1人気5人気3人気\n",
      "202210020408   枠連          4 6          640円        2人気\n",
      "202210020408   馬連          5 8          690円        1人気\n",
      "202210020408  ワイド  5 8 6 8 5 6  290円430円770円  1人気3人気8人気\n",
      "202210020408   馬単          8 5        1,010円        1人気\n",
      "202210020408  3連複        5 6 8        1,950円        3人気\n",
      "202210020408  3連単        8 5 6        6,370円        3人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020409  12  0.408575\n",
      "202210020409   3 -0.027389\n",
      "202210020409   5 -1.226901\n",
      "actual\n",
      "                0                  1                 2            3\n",
      "202210020409   単勝                 12              550円          3人気\n",
      "202210020409   複勝           12 13 15      200円210円820円   2人気4人気11人気\n",
      "202210020409   枠連                6 7            1,000円          3人気\n",
      "202210020409   馬連              12 13            1,760円          4人気\n",
      "202210020409  ワイド  12 13 12 15 13 15  680円3,360円2,240円  4人気40人気26人気\n",
      "202210020409   馬単              12 13            3,290円          9人気\n",
      "202210020409  3連複           12 13 15           16,140円         56人気\n",
      "202210020409  3連単           12 13 15           70,620円        234人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020410  12  0.714611\n",
      "202210020410   1 -0.538924\n",
      "202210020410  13 -0.623485\n",
      "actual\n",
      "                0              1                   2             3\n",
      "202210020410   単勝              6              1,130円           5人気\n",
      "202210020410   複勝         6 8 12        320円780円340円    3人気12人気4人気\n",
      "202210020410   枠連            4 5              1,090円           5人気\n",
      "202210020410   馬連            6 8             14,560円          46人気\n",
      "202210020410  ワイド  6 8 6 12 8 12  3,510円1,490円4,970円  39人気13人気53人気\n",
      "202210020410   馬単            6 8             27,100円          87人気\n",
      "202210020410  3連複         6 8 12             47,300円         150人気\n",
      "202210020410  3連単         6 8 12            291,990円         890人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020411   4  1.285969\n",
      "202210020411   5  0.604811\n",
      "202210020411  16  0.373183\n",
      "actual\n",
      "                0                1                 2            3\n",
      "202210020411   単勝               16              440円          1人気\n",
      "202210020411   複勝          16 12 7      190円230円480円    1人気4人気9人気\n",
      "202210020411   枠連              6 8            1,270円          4人気\n",
      "202210020411   馬連            12 16            1,580円          2人気\n",
      "202210020411  ワイド  12 16 7 16 7 12  700円1,950円1,940円  2人気21人気20人気\n",
      "202210020411   馬単            16 12            3,030円          5人気\n",
      "202210020411  3連複          7 12 16           10,670円         29人気\n",
      "202210020411  3連単          16 12 7           39,610円        104人気\n",
      "-------------------\n",
      "predict\n",
      "              馬番    scores\n",
      "202210020412  12  2.139502\n",
      "202210020412   4 -0.196595\n",
      "202210020412  11 -0.574883\n",
      "actual\n",
      "                0                1               2           3\n",
      "202210020412   単勝                4            900円         4人気\n",
      "202210020412   複勝          4 10 11    230円290円150円   4人気6人気1人気\n",
      "202210020412   枠連              2 5          4,170円        16人気\n",
      "202210020412   馬連             4 10          5,320円        16人気\n",
      "202210020412  ワイド  4 10 4 11 10 11  1,500円690円610円  16人気5人気4人気\n",
      "202210020412   馬単             4 10          9,740円        29人気\n",
      "202210020412  3連複          4 10 11          4,600円        10人気\n",
      "202210020412  3連単          4 10 11         43,100円       115人気\n",
      "---------------------\n",
      "単勝\n",
      "的中率 : 6 / 36\n",
      "的中% : 16.67 %\n",
      "収支   : -1520 円\n",
      "的中レース ['02', '09', '01', '07', '08', '09']\n",
      "---------------------\n",
      "複勝\n",
      "的中率 : 12 / 36\n",
      "的中% : 33.33 %\n",
      "収支   : -1160 円\n",
      "的中レース ['02', '09', '01', '06', '07', '08', '12', '01', '03', '06', '09', '10']\n",
      "---------------------\n",
      "ワイド\n",
      "的中率 : 5 / 36\n",
      "的中% : 13.89 %\n",
      "収支   : -1720 円\n",
      "的中レース ['01', '06', '07', '08', '12']\n"
     ]
    }
   ],
   "source": [
    "sl = RankSimulater(lgb_rank)\n",
    "sl.return_table_today(race_id_list)\n",
    "# return_tables.rename(columns={'0':0,'1':1,'2':2,'3':3},inplace=True)\n",
    "sl.show_results(st ,race_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc1142-ff9b-4568-80b8-18ef0d9010c1",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29d7bf-5052-44dc-ac80-dea59ed65444",
   "metadata": {},
   "source": [
    "流れ\n",
    "1. fasttext用の血統データの学習データを作る (血統の情報のみ, index ヘッダはいらない)\n",
    "2. fasttext学習\n",
    "3. 学習モデルを使って, 血統データをベクトル化\n",
    "4. ベクトル化して r.data_cに concat\n",
    "5. 学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d375b9-e292-4d4a-97c2-905e3060419f",
   "metadata": {},
   "source": [
    "教師あり, 教師なしでも生成されるベクトルは等しい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5229f-9b05-4bc1-820a-0e275a1d6c8f",
   "metadata": {},
   "source": [
    "# model_ft 作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "554f0c62-72c2-4a2e-b45e-b0b4d37c2e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 相対パスしかできない\n",
    "# dim : 出力の次元\n",
    "# minn : n_gramの最小単位\n",
    "# maxn : n_gramの最大単位\n",
    "model_ft = ft.train_unsupervised(test_path,dim=62,minn=2,maxn=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94807517-3ed5-427d-9886-72c42b7589cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0003445873"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_txt['hoge'] で 'hoge'の単語ベクトル入手\n",
    "model_ft['アカイイト'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1e2be52-3b04-4390-9769-c63ce59a643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5724351e-03,  2.1741162e-03, -6.2224795e-03,  7.9450803e-03,\n",
       "        9.2663774e-03,  1.2499948e-03, -1.9003255e-03, -1.4434209e-03,\n",
       "       -7.0348015e-04, -2.3646757e-03,  8.4775481e-03,  7.6517521e-04,\n",
       "        1.2443915e-04,  6.6868594e-04, -2.2353258e-03, -3.1006148e-03,\n",
       "       -5.1603146e-04, -6.8897572e-03,  8.9530461e-03,  6.7916275e-03,\n",
       "        2.7701540e-03, -1.4256138e-03,  9.4705867e-03, -9.9947751e-03,\n",
       "       -4.6277489e-03, -7.6256958e-03,  7.5081405e-03, -3.4463892e-03,\n",
       "       -3.8665808e-03,  1.7099102e-03,  9.7447438e-03, -7.7348817e-03,\n",
       "       -2.9115018e-04, -3.1618846e-03,  6.5541668e-03, -7.9466682e-03,\n",
       "       -5.2352938e-05, -8.1277534e-04, -1.2927600e-03, -9.0491874e-03,\n",
       "       -4.8794332e-03, -1.3042025e-03, -8.9057656e-03, -2.9853662e-03,\n",
       "        6.7775971e-03,  9.9755134e-03, -2.7005693e-03,  5.2640764e-03,\n",
       "       -6.6113472e-03,  5.9692696e-04, -9.7892229e-03, -4.3281284e-03,\n",
       "        6.6364333e-03, -9.9991390e-04,  3.3466963e-03, -7.2530257e-03,\n",
       "        1.4511276e-03,  9.4374539e-03,  6.8324972e-03,  1.0399714e-03,\n",
       "        6.0194843e-03,  5.8186194e-03,  3.8527260e-03,  6.5378621e-03,\n",
       "       -3.9170850e-03,  9.6854856e-03,  4.2792736e-03,  7.6427357e-04,\n",
       "       -4.8055393e-03,  9.4173355e-03,  9.0470780e-03,  9.1776745e-03,\n",
       "        6.0645705e-03, -7.4923565e-03, -2.5749654e-03, -1.7557642e-03,\n",
       "        3.6343501e-04,  7.6567060e-03,  4.2951941e-03, -5.3092451e-03,\n",
       "       -2.7257178e-03,  5.8091432e-03, -3.5633314e-03, -3.1582750e-03,\n",
       "        9.5966822e-03, -6.4879083e-03, -3.9101331e-03,  4.1160994e-04,\n",
       "       -7.1197501e-03, -2.3638823e-03, -5.1401439e-03,  3.2334772e-03,\n",
       "        7.0011085e-03, -8.3606951e-03, -9.8121529e-03, -9.1953417e-03,\n",
       "        5.8084358e-03, -2.6821357e-03, -5.3060763e-03,  3.9062789e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.get_input_vector(ind=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd9030-310c-414d-8c91-6178159c00e6",
   "metadata": {},
   "source": [
    "model[model.words[1]] と model.get_input_vector(ind=1) は等価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f5e42fb-7fb1-4449-9d16-c1a21d87aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.64855465e-05,  1.37111065e-05,  1.41594879e-04,  3.69198642e-05,\n",
       "        9.37314871e-06,  9.83630889e-05, -4.32917550e-05, -5.60286717e-05,\n",
       "       -1.21071007e-05,  3.47241585e-05, -1.29177488e-05,  5.48821408e-05,\n",
       "       -7.11681787e-05,  1.35873206e-05, -6.51547089e-05,  1.05369854e-05,\n",
       "        2.46712134e-05, -2.98814448e-05, -6.97223822e-06,  5.47772688e-05,\n",
       "       -4.34648828e-05, -6.77032876e-05,  3.82750259e-05,  4.62639291e-05,\n",
       "        3.87809414e-05, -5.79457264e-05, -3.11739132e-05, -3.45420995e-05,\n",
       "        2.56179737e-05,  1.88591548e-05, -1.06936168e-04, -3.09621441e-06,\n",
       "       -3.30380026e-05, -2.44859002e-05,  2.54371498e-05,  2.28005192e-05,\n",
       "       -1.14125714e-05, -7.71405212e-06, -2.62292688e-05,  4.95023669e-05,\n",
       "        6.83483158e-05,  7.41472240e-06, -7.45871466e-06, -1.99570986e-05,\n",
       "       -8.77055936e-06,  6.14155870e-05, -3.37384336e-05, -7.03690312e-05,\n",
       "       -6.21120780e-05, -3.50524570e-05, -2.38443281e-05,  3.41939740e-05,\n",
       "       -5.05409917e-05, -3.19997957e-06, -3.45971457e-05,  2.91866854e-05,\n",
       "        8.44113019e-05,  3.85636376e-05, -3.52261850e-05, -7.60995536e-05,\n",
       "        4.26866463e-05,  6.05096320e-05], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[test_str]\n",
    "\n",
    "# model[model.words[1]] と model.get_input_vector(ind=1) は等価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6836443-36c4-4f2e-aa7e-c911fd9b31ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22636686,  0.0358811 ,  0.3706047 ,  0.09663288,  0.02452924,\n",
       "        0.2574478 , -0.11331306, -0.1466456 , -0.03169499,  0.0908782 ,\n",
       "       -0.03380894,  0.14364257, -0.18627687,  0.03555746, -0.17052874,\n",
       "        0.0275799 ,  0.06457247, -0.07821366, -0.01824913,  0.14337559,\n",
       "       -0.11376097, -0.1772068 ,  0.10017442,  0.12109151,  0.10149854,\n",
       "       -0.15165876, -0.08159366, -0.09040555,  0.0670519 ,  0.04935378,\n",
       "       -0.27988228, -0.00810147, -0.08646543, -0.06408333,  0.06658382,\n",
       "        0.05967693, -0.02987295, -0.02018429, -0.06864916,  0.12956837,\n",
       "        0.17888325,  0.01940197, -0.0195243 , -0.05223562, -0.02295211,\n",
       "        0.16074347, -0.08830138, -0.18417585, -0.16256206, -0.09174566,\n",
       "       -0.06240372,  0.0894906 , -0.1322762 , -0.00837872, -0.09055168,\n",
       "        0.076395  ,  0.22093119,  0.10093257, -0.09220136, -0.19917955,\n",
       "        0.11172937,  0.15837023], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文字列のベクトル表現\n",
    "model.get_sentence_vector(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84bd217d-81fc-4dd4-8c84-983f25649a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01349934,  0.01271002, -0.01006453, ...,  0.00531607,\n",
       "        -0.0106207 ,  0.00814016],\n",
       "       [ 0.01048684,  0.00816879, -0.00584027, ...,  0.01594336,\n",
       "         0.00641512,  0.01121091],\n",
       "       [-0.01389093, -0.00994238, -0.01586624, ...,  0.00450218,\n",
       "         0.00770794,  0.00581788],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これまで入力した行列を返す関数\n",
    "model.get_input_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6e9be41-b57e-4617-9cce-c93421b3e512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['HailtoReason', '</s>'], [])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.get_line(model_ft.words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0a5ba061-a66a-486f-be7c-dc74134e5a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 62), dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a74eb78-be1b-4437-9dd3-0848392ff303",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = model.get_word_vector('サトノダイヤモンド')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1542697-b382-473e-9290-de1f9f1982fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = model.get_word_vector('ディープインパクト')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b102f30-6bf5-4913-844f-cbaeee0486d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.7157024e-05"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(S1,S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9fb9255c-ad98-4860-8526-40930792384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.5730346e-05, -8.2462875e-04, -3.1586326e-04,  4.7356146e-04,\n",
       "        5.8596971e-04, -6.1965646e-04, -3.4621550e-04, -2.0356594e-04,\n",
       "        4.1895488e-04,  9.0188440e-04,  3.8259976e-05,  7.6713978e-04,\n",
       "        1.0702623e-03, -1.1213292e-04, -6.6813658e-04,  8.1465213e-04,\n",
       "       -4.3050353e-05,  2.5083427e-05,  6.8161904e-04,  4.3567832e-04,\n",
       "        1.6084009e-04, -9.3031843e-04,  4.2508260e-04,  6.1241310e-04,\n",
       "        3.3009407e-04,  2.0903254e-04,  2.9974934e-04, -5.7975241e-05,\n",
       "        5.6907971e-04,  6.3735433e-04, -6.2590954e-04, -7.7536242e-04,\n",
       "        5.6357408e-04, -3.5650644e-04, -7.2097318e-04, -6.0377805e-04,\n",
       "       -5.2635855e-04,  2.9387549e-04, -2.0975820e-04,  1.6490524e-04,\n",
       "        3.7632947e-04, -8.7560957e-06, -2.0931315e-04,  7.3979911e-04,\n",
       "       -2.5976190e-04,  2.9825282e-04,  7.2142771e-05, -6.9494192e-05,\n",
       "       -1.1075408e-04, -6.0473965e-04, -4.6147266e-04,  4.7886188e-05,\n",
       "       -2.1094021e-04,  4.6147863e-04, -1.1938680e-03,  1.6803351e-04,\n",
       "       -2.5032635e-04, -4.6535756e-04, -6.7986548e-05, -8.6104317e-04,\n",
       "       -2.1196474e-04, -4.1519249e-07], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['ディープインパクト,サトノダイヤモンド']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b7f4182d-76ed-4f15-b5a0-8f335c01954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a4d4c4a7-0b74-4a16-8c76-958ff374c7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.64855465e-05,  1.37111065e-05,  1.41594879e-04,  3.69198642e-05,\n",
       "        9.37314871e-06,  9.83630889e-05, -4.32917550e-05, -5.60286717e-05,\n",
       "       -1.21071007e-05,  3.47241585e-05, -1.29177488e-05,  5.48821408e-05,\n",
       "       -7.11681787e-05,  1.35873206e-05, -6.51547089e-05,  1.05369854e-05,\n",
       "        2.46712134e-05, -2.98814448e-05, -6.97223822e-06,  5.47772688e-05,\n",
       "       -4.34648828e-05, -6.77032876e-05,  3.82750259e-05,  4.62639291e-05,\n",
       "        3.87809414e-05, -5.79457264e-05, -3.11739132e-05, -3.45420995e-05,\n",
       "        2.56179737e-05,  1.88591548e-05, -1.06936168e-04, -3.09621441e-06,\n",
       "       -3.30380026e-05, -2.44859002e-05,  2.54371498e-05,  2.28005192e-05,\n",
       "       -1.14125714e-05, -7.71405212e-06, -2.62292688e-05,  4.95023669e-05,\n",
       "        6.83483158e-05,  7.41472240e-06, -7.45871466e-06, -1.99570986e-05,\n",
       "       -8.77055936e-06,  6.14155870e-05, -3.37384336e-05, -7.03690312e-05,\n",
       "       -6.21120780e-05, -3.50524570e-05, -2.38443281e-05,  3.41939740e-05,\n",
       "       -5.05409917e-05, -3.19997957e-06, -3.45971457e-05,  2.91866854e-05,\n",
       "        8.44113019e-05,  3.85636376e-05, -3.52261850e-05, -7.60995536e-05,\n",
       "        4.26866463e-05,  6.05096320e-05], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model[test_str]と等価\n",
    "model.get_word_vector(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c6aaee33-be1e-4148-a8e1-d2d664d0b716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glvv,qqxs,frfk,xdkq,genx,xrcy,giwg,okwt,moyi,wcfv,wugp,jfcs,phgn,rutx,frbg,wmge,hycv,itrt,rzxq,mrep,fphb,rrqg,vkzp,hpfp,kicb,ibdz,hwvd,crsd,zqnx,yovf,xvdo,owyi,ioup,nyjx,hhnq,bjng,waem,umdj,gfro,btqj,ptjy,ayro,lefh,wkcw,uafv,ranu,ujcm,fepc,rvfu,slbm,zdjt,yqvm,zisy,kabq,ynec,mqlt,bgpc,ynca,ptmf,cmas,pqeg,sllj'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05b6629-3c9c-42a3-82b9-06bf42417ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(97,122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d49ced-60d6-4abd-85f8-160b5d6d5895",
   "metadata": {},
   "source": [
    "# ランダム文字列でテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5da0cf7-dbb0-4d7f-8637-14a273583b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(62):\n",
    "    rand_name = ''\n",
    "    for j in range(4):\n",
    "        rand_name += chr(random.randint(97,122))\n",
    "    word_list.append(rand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8bc506-a0e6-4bbb-a32a-6903b97093ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \",\".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67a29f66-bb78-4aed-b713-f1479bf60774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ae7a7ba9-5855-468d-b07c-567d8b1e08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fastText/text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bccf95c8-78ef-4579-a69b-a7b80674672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0].tolist()[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55180185-a8e8-4364-b52f-608e8936cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'スズカマンボ'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6bdb5fea-fc63-497b-b454-8026ac692196",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"fastText/text.txt\"\n",
    "\n",
    "with open(file_name) as f:\n",
    "    data_lines = f.read()\n",
    "\n",
    "# 文字列置換\n",
    "data_lines = data_lines.replace(\",\", \" \")\n",
    "\n",
    "# 同じファイル名で保存\n",
    "with open(file_name, mode=\"w\") as f:\n",
    "    f.write(data_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0a26e83e-1f72-44e6-884b-05d3b9a0254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ポイントフラッグ', 0.951915442943573),\n",
       " ('カレイメモワール', 0.8899344801902771),\n",
       " ('リヤンドファミユ', 0.8506640791893005),\n",
       " ('パストラリズム', 0.8343662619590759),\n",
       " ('コスモスカイライン', 0.8293111324310303),\n",
       " ('ドリームジャーニー', 0.828325092792511),\n",
       " ('ハッシュバンバン', 0.8259212374687195),\n",
       " ('ナカヤマフェスタ', 0.8249091506004333),\n",
       " ('タイセイレジェンド', 0.8150109648704529),\n",
       " ('オーシャンブルー', 0.8144512176513672)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim \n",
    "gen_model = gensim.models.KeyedVectors.load_word2vec_format('fastText/ketto_model.vec', binary=False)\n",
    "\n",
    "# most_similarメソッドを使って演算\n",
    "# positiveに足し合わせるデータをリストで渡し、negativeに差し引くデータをリストで渡す。\n",
    "\n",
    "gen_model.most_similar(\n",
    "    positive=[ \"ゴールドシップ\"],\n",
    "#     negative=[\"ディープインパクト\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca4955e3-7b1f-4955-9066-9b7a2232f7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ウインドインハーヘア', 0.8664582371711731),\n",
       " ('ローリエ', 0.7226738333702087),\n",
       " ('オンヴェラ', 0.7160682082176208),\n",
       " ('アイスドール', 0.7097043395042419),\n",
       " ('アローム', 0.7028155326843262),\n",
       " ('サトノアラジン', 0.7014816403388977),\n",
       " ('クロノロジスト', 0.6986632943153381),\n",
       " ('ピンクアリエス', 0.6985989212989807),\n",
       " ('ナイトマジック', 0.6953324675559998),\n",
       " ('ヘヴンリークルーズ', 0.6944254636764526)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.most_similar(\n",
    "    positive=[ \"ディープインパクト\"],\n",
    "#     negative=[\"ディープインパクト\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85747d8f-c452-4eaf-ab8a-1d964ca42805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_txt = ft.train_unsupervised('fastText/text.txt',minn=2,maxn=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6752119-bd97-4367-8094-9bd13cd0d3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9368"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_txt.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b53cd19-3662-4207-a5df-a6e6e900e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:461: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(1,svd_solver='auto')#n_components=2)\n",
    "coords = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c457db1-dd68-4b50-9f38-19ec087dba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cb7c503-6504-4e5a-a4cb-ef33b90ea30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50ed1c05-6afa-45e7-b6f5-b089279995a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors  = model_txt['キズナ'].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7abac68e-1781-47ea-8db1-4c203cd05bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdabb53d-b024-43b4-ae34-aca06fe2727f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0507768 , -0.03158796,  0.3719879 , -0.43337092,  0.04407024,\n",
       "        -0.04416358,  0.56797636,  0.3727986 ,  0.52248025, -0.4481369 ,\n",
       "         0.2606985 , -0.20405166, -0.3789636 ,  0.09112091, -0.27974096,\n",
       "         0.18359385, -0.2400678 ,  0.74059075,  0.3779946 , -0.7949204 ,\n",
       "        -0.6549442 ,  0.00354731, -0.62105113, -0.04626863, -0.10678367,\n",
       "         0.35455292, -0.48053965,  0.8271895 ,  1.0350835 ,  0.06584841,\n",
       "        -0.2702481 , -0.17578961,  0.2406562 , -0.01046936, -0.09063246,\n",
       "         0.296278  , -0.76684433, -0.2213059 ,  1.2849661 , -0.22702964,\n",
       "        -0.1260167 , -0.45229167, -0.30201095, -0.08013313,  0.38698265,\n",
       "        -0.25812092,  0.1184909 ,  0.177462  ,  0.0502384 ,  0.12209569,\n",
       "        -0.7180075 ,  0.3745045 , -0.82095605,  0.40734512, -0.3441998 ,\n",
       "         0.7743839 , -0.1306628 , -0.6589422 ,  0.10314472,  0.17247966,\n",
       "        -0.01692947,  0.15154183,  0.7737049 , -0.86726075,  0.38742578,\n",
       "        -0.29168054, -0.21751773,  0.30418798, -0.28203708,  0.5258079 ,\n",
       "         0.44744217, -0.02149613, -0.10703801, -0.49976835, -0.1979347 ,\n",
       "        -0.6855942 , -0.12264266, -0.47512937,  0.11616299, -0.02876348,\n",
       "         0.3574077 , -0.67298234,  0.03579545, -0.1604615 , -0.1598372 ,\n",
       "         0.9801576 , -0.34563044, -0.4170036 ,  0.26159236,  0.31148013,\n",
       "         0.2492534 , -0.52977777,  0.51793206,  0.47258243,  0.36744073,\n",
       "         0.2189175 , -0.23959026, -0.52281433,  0.6178897 ,  0.04191767]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "822d1a86-fbe1-4cd7-8c58-3f5c42ab13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4545997 , -0.08741464,  0.31173626,  0.13845074, -0.76570636,\n",
       "        0.07802508,  0.64899915,  0.6832365 ,  1.4973719 , -0.3659583 ,\n",
       "       -0.10207128, -0.51044196, -0.40104282, -0.064634  , -0.6910132 ,\n",
       "        0.6079259 , -0.05301953,  0.8091327 , -0.10024333, -0.18845513,\n",
       "       -0.21310434,  0.01393075, -0.61000365,  0.5268892 , -0.14041117,\n",
       "       -0.10406648, -0.71680737,  0.81365883,  1.0771564 ,  0.00515136,\n",
       "        0.0345313 ,  0.13858005,  0.79001707, -0.2743086 , -0.191795  ,\n",
       "       -0.30741683, -0.23364004, -0.44694647,  0.4207134 , -0.55999047,\n",
       "       -0.10824247, -0.3253643 , -0.05974116, -0.49804798,  0.06789133,\n",
       "       -0.37461394, -0.02879322,  0.10663368, -0.19275875,  0.8938551 ,\n",
       "       -0.52291214,  0.39916897, -0.7857111 ,  0.4856257 , -0.40032613,\n",
       "        1.27588   ,  0.22451466, -1.2030284 ,  0.00870536,  0.34859702,\n",
       "        0.05455235,  0.12034642,  0.60126066, -0.76645064,  0.4021038 ,\n",
       "       -0.59574425,  0.27805454, -0.1309369 , -0.70244116,  0.45846057,\n",
       "        0.14381546, -0.41491303, -0.19148181, -0.50548697, -0.34376445,\n",
       "        0.05106369, -0.06135022, -1.0126988 ,  0.34059715, -0.47681922,\n",
       "       -0.12842496, -0.4898993 , -0.29974318,  0.18294708, -0.54171324,\n",
       "        0.98443955,  0.11519836, -0.11139733, -0.13544452,  0.17204411,\n",
       "        0.8353284 , -0.71006197,  0.15498304,  0.46772584,  0.18199985,\n",
       "        0.39634904,  0.13632812, -0.44665393,  0.38587388, -0.12222904],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt['キズナ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e651aa-8d78-4c02-bc8e-4aa2e7370682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1273632], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt['ウインドインハーヘア']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "893f7f2c-5cf8-4c05-a35f-dc715ef83976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'キセキ' in model_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dd290616-3da4-48d4-a36e-708c7f16f0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10564891], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt['ggggg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ff2899c4-6fb9-4a1c-a6c6-dd1188016560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<キ', '<キセ', '<キセキ', '<キセキ>', 'キセ', 'キセキ', 'キセキ>', 'セキ', 'セキ>', 'キ>'],\n",
       " array([1668417, 1684405,  727383,  376875,  705019, 1474665, 1347925,\n",
       "        1148731, 1613759,  471987]))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt.get_subwords('キセキ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "777b7e8a-c49b-4d20-9c4f-7e7ab1386744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3261814], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt['キセキ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "85ff4a52-04be-4258-b163-d775f87722de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5716173e-05,  1.8187744e-03, -9.1101311e-04,  2.1235285e-04,\n",
       "        7.0718746e-04,  1.0516392e-03,  1.1351848e-04,  8.2758488e-04,\n",
       "        2.7018306e-03, -9.5881027e-05,  2.9185105e-05, -1.0434553e-03,\n",
       "       -7.9279731e-04,  9.7809790e-04,  1.0020308e-03, -9.9704921e-06,\n",
       "        1.7076595e-03,  2.0725920e-04,  1.9977726e-03,  2.2366480e-03,\n",
       "        2.1179956e-03,  2.2501138e-03, -1.3245193e-03,  8.5473980e-04,\n",
       "        2.1319906e-04,  1.9163315e-03, -1.8604699e-04, -2.4861936e-03,\n",
       "       -2.0311947e-04,  1.5762139e-03, -1.0374290e-03, -3.4588840e-04,\n",
       "        1.3026310e-03, -7.2811241e-04,  5.4169149e-04, -1.7128785e-03,\n",
       "       -4.9187336e-04,  8.6225322e-05,  1.0082403e-03,  1.5551767e-03,\n",
       "       -9.9105854e-04, -5.5738556e-04,  6.8321481e-04, -8.1636908e-04,\n",
       "        1.1570966e-03, -6.2821846e-04,  1.1838707e-03,  2.3540622e-03,\n",
       "       -4.7980080e-04, -1.3543551e-03, -9.9284640e-05, -5.7554425e-04,\n",
       "        6.3399930e-04, -1.7117684e-03,  2.8442516e-04,  9.7960467e-04,\n",
       "        6.5276094e-05, -9.5968309e-04,  7.9806743e-04,  6.3678504e-05,\n",
       "        1.0281358e-03, -1.3261484e-03], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['アカイイト']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "899d10c2-c3a3-495c-9866-f15a24aa0245",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<デ',\n",
       "  '<ディ',\n",
       "  '<ディー',\n",
       "  '<ディープ',\n",
       "  '<ディープイ',\n",
       "  '<ディープイン',\n",
       "  '<ディープインパ',\n",
       "  '<ディープインパク',\n",
       "  '<ディープインパクト',\n",
       "  '<ディープインパクト,',\n",
       "  '<ディープインパクト,ク',\n",
       "  '<ディープインパクト,クロ',\n",
       "  '<ディープインパクト,クロウ',\n",
       "  'ディ',\n",
       "  'ディー',\n",
       "  'ディープ',\n",
       "  'ディープイ',\n",
       "  'ディープイン',\n",
       "  'ディープインパ',\n",
       "  'ディープインパク',\n",
       "  'ディープインパクト',\n",
       "  'ディープインパクト,',\n",
       "  'ディープインパクト,ク',\n",
       "  'ディープインパクト,クロ',\n",
       "  'ディープインパクト,クロウ',\n",
       "  'ディープインパクト,クロウキ',\n",
       "  'ィー',\n",
       "  'ィープ',\n",
       "  'ィープイ',\n",
       "  'ィープイン',\n",
       "  'ィープインパ',\n",
       "  'ィープインパク',\n",
       "  'ィープインパクト',\n",
       "  'ィープインパクト,',\n",
       "  'ィープインパクト,ク',\n",
       "  'ィープインパクト,クロ',\n",
       "  'ィープインパクト,クロウ',\n",
       "  'ィープインパクト,クロウキ',\n",
       "  'ィープインパクト,クロウキャ',\n",
       "  'ープ',\n",
       "  'ープイ',\n",
       "  'ープイン',\n",
       "  'ープインパ',\n",
       "  'ープインパク',\n",
       "  'ープインパクト',\n",
       "  'ープインパクト,',\n",
       "  'ープインパクト,ク',\n",
       "  'ープインパクト,クロ',\n",
       "  'ープインパクト,クロウ',\n",
       "  'ープインパクト,クロウキ',\n",
       "  'ープインパクト,クロウキャ',\n",
       "  'ープインパクト,クロウキャニ',\n",
       "  'プイ',\n",
       "  'プイン',\n",
       "  'プインパ',\n",
       "  'プインパク',\n",
       "  'プインパクト',\n",
       "  'プインパクト,',\n",
       "  'プインパクト,ク',\n",
       "  'プインパクト,クロ',\n",
       "  'プインパクト,クロウ',\n",
       "  'プインパクト,クロウキ',\n",
       "  'プインパクト,クロウキャ',\n",
       "  'プインパクト,クロウキャニ',\n",
       "  'プインパクト,クロウキャニオ',\n",
       "  'イン',\n",
       "  'インパ',\n",
       "  'インパク',\n",
       "  'インパクト',\n",
       "  'インパクト,',\n",
       "  'インパクト,ク',\n",
       "  'インパクト,クロ',\n",
       "  'インパクト,クロウ',\n",
       "  'インパクト,クロウキ',\n",
       "  'インパクト,クロウキャ',\n",
       "  'インパクト,クロウキャニ',\n",
       "  'インパクト,クロウキャニオ',\n",
       "  'インパクト,クロウキャニオン',\n",
       "  'ンパ',\n",
       "  'ンパク',\n",
       "  'ンパクト',\n",
       "  'ンパクト,',\n",
       "  'ンパクト,ク',\n",
       "  'ンパクト,クロ',\n",
       "  'ンパクト,クロウ',\n",
       "  'ンパクト,クロウキ',\n",
       "  'ンパクト,クロウキャ',\n",
       "  'ンパクト,クロウキャニ',\n",
       "  'ンパクト,クロウキャニオ',\n",
       "  'ンパクト,クロウキャニオン',\n",
       "  'ンパクト,クロウキャニオン,',\n",
       "  'パク',\n",
       "  'パクト',\n",
       "  'パクト,',\n",
       "  'パクト,ク',\n",
       "  'パクト,クロ',\n",
       "  'パクト,クロウ',\n",
       "  'パクト,クロウキ',\n",
       "  'パクト,クロウキャ',\n",
       "  'パクト,クロウキャニ',\n",
       "  'パクト,クロウキャニオ',\n",
       "  'パクト,クロウキャニオン',\n",
       "  'パクト,クロウキャニオン,',\n",
       "  'パクト,クロウキャニオン,サ',\n",
       "  'クト',\n",
       "  'クト,',\n",
       "  'クト,ク',\n",
       "  'クト,クロ',\n",
       "  'クト,クロウ',\n",
       "  'クト,クロウキ',\n",
       "  'クト,クロウキャ',\n",
       "  'クト,クロウキャニ',\n",
       "  'クト,クロウキャニオ',\n",
       "  'クト,クロウキャニオン',\n",
       "  'クト,クロウキャニオン,',\n",
       "  'クト,クロウキャニオン,サ',\n",
       "  'クト,クロウキャニオン,サン',\n",
       "  'ト,',\n",
       "  'ト,ク',\n",
       "  'ト,クロ',\n",
       "  'ト,クロウ',\n",
       "  'ト,クロウキ',\n",
       "  'ト,クロウキャ',\n",
       "  'ト,クロウキャニ',\n",
       "  'ト,クロウキャニオ',\n",
       "  'ト,クロウキャニオン',\n",
       "  'ト,クロウキャニオン,',\n",
       "  'ト,クロウキャニオン,サ',\n",
       "  'ト,クロウキャニオン,サン',\n",
       "  'ト,クロウキャニオン,サンデ',\n",
       "  ',ク',\n",
       "  ',クロ',\n",
       "  ',クロウ',\n",
       "  ',クロウキ',\n",
       "  ',クロウキャ',\n",
       "  ',クロウキャニ',\n",
       "  ',クロウキャニオ',\n",
       "  ',クロウキャニオン',\n",
       "  ',クロウキャニオン,',\n",
       "  ',クロウキャニオン,サ',\n",
       "  ',クロウキャニオン,サン',\n",
       "  ',クロウキャニオン,サンデ',\n",
       "  ',クロウキャニオン,サンデー',\n",
       "  'クロ',\n",
       "  'クロウ',\n",
       "  'クロウキ',\n",
       "  'クロウキャ',\n",
       "  'クロウキャニ',\n",
       "  'クロウキャニオ',\n",
       "  'クロウキャニオン',\n",
       "  'クロウキャニオン,',\n",
       "  'クロウキャニオン,サ',\n",
       "  'クロウキャニオン,サン',\n",
       "  'クロウキャニオン,サンデ',\n",
       "  'クロウキャニオン,サンデー',\n",
       "  'クロウキャニオン,サンデーサ',\n",
       "  'ロウ',\n",
       "  'ロウキ',\n",
       "  'ロウキャ',\n",
       "  'ロウキャニ',\n",
       "  'ロウキャニオ',\n",
       "  'ロウキャニオン',\n",
       "  'ロウキャニオン,',\n",
       "  'ロウキャニオン,サ',\n",
       "  'ロウキャニオン,サン',\n",
       "  'ロウキャニオン,サンデ',\n",
       "  'ロウキャニオン,サンデー',\n",
       "  'ロウキャニオン,サンデーサ',\n",
       "  'ロウキャニオン,サンデーサイ',\n",
       "  'ウキ',\n",
       "  'ウキャ',\n",
       "  'ウキャニ',\n",
       "  'ウキャニオ',\n",
       "  'ウキャニオン',\n",
       "  'ウキャニオン,',\n",
       "  'ウキャニオン,サ',\n",
       "  'ウキャニオン,サン',\n",
       "  'ウキャニオン,サンデ',\n",
       "  'ウキャニオン,サンデー',\n",
       "  'ウキャニオン,サンデーサ',\n",
       "  'ウキャニオン,サンデーサイ',\n",
       "  'ウキャニオン,サンデーサイレ',\n",
       "  'キャ',\n",
       "  'キャニ',\n",
       "  'キャニオ',\n",
       "  'キャニオン',\n",
       "  'キャニオン,',\n",
       "  'キャニオン,サ',\n",
       "  'キャニオン,サン',\n",
       "  'キャニオン,サンデ',\n",
       "  'キャニオン,サンデー',\n",
       "  'キャニオン,サンデーサ',\n",
       "  'キャニオン,サンデーサイ',\n",
       "  'キャニオン,サンデーサイレ',\n",
       "  'キャニオン,サンデーサイレン',\n",
       "  'ャニ',\n",
       "  'ャニオ',\n",
       "  'ャニオン',\n",
       "  'ャニオン,',\n",
       "  'ャニオン,サ',\n",
       "  'ャニオン,サン',\n",
       "  'ャニオン,サンデ',\n",
       "  'ャニオン,サンデー',\n",
       "  'ャニオン,サンデーサ',\n",
       "  'ャニオン,サンデーサイ',\n",
       "  'ャニオン,サンデーサイレ',\n",
       "  'ャニオン,サンデーサイレン',\n",
       "  'ャニオン,サンデーサイレンス',\n",
       "  'ニオ',\n",
       "  'ニオン',\n",
       "  'ニオン,',\n",
       "  'ニオン,サ',\n",
       "  'ニオン,サン',\n",
       "  'ニオン,サンデ',\n",
       "  'ニオン,サンデー',\n",
       "  'ニオン,サンデーサ',\n",
       "  'ニオン,サンデーサイ',\n",
       "  'ニオン,サンデーサイレ',\n",
       "  'ニオン,サンデーサイレン',\n",
       "  'ニオン,サンデーサイレンス',\n",
       "  'ニオン,サンデーサイレンス,',\n",
       "  'オン',\n",
       "  'オン,',\n",
       "  'オン,サ',\n",
       "  'オン,サン',\n",
       "  'オン,サンデ',\n",
       "  'オン,サンデー',\n",
       "  'オン,サンデーサ',\n",
       "  'オン,サンデーサイ',\n",
       "  'オン,サンデーサイレ',\n",
       "  'オン,サンデーサイレン',\n",
       "  'オン,サンデーサイレンス',\n",
       "  'オン,サンデーサイレンス,',\n",
       "  'オン,サンデーサイレンス,ウ',\n",
       "  'ン,',\n",
       "  'ン,サ',\n",
       "  'ン,サン',\n",
       "  'ン,サンデ',\n",
       "  'ン,サンデー',\n",
       "  'ン,サンデーサ',\n",
       "  'ン,サンデーサイ',\n",
       "  'ン,サンデーサイレ',\n",
       "  'ン,サンデーサイレン',\n",
       "  'ン,サンデーサイレンス',\n",
       "  'ン,サンデーサイレンス,',\n",
       "  'ン,サンデーサイレンス,ウ',\n",
       "  'ン,サンデーサイレンス,ウイ',\n",
       "  ',サ',\n",
       "  ',サン',\n",
       "  ',サンデ',\n",
       "  ',サンデー',\n",
       "  ',サンデーサ',\n",
       "  ',サンデーサイ',\n",
       "  ',サンデーサイレ',\n",
       "  ',サンデーサイレン',\n",
       "  ',サンデーサイレンス',\n",
       "  ',サンデーサイレンス,',\n",
       "  ',サンデーサイレンス,ウ',\n",
       "  ',サンデーサイレンス,ウイ',\n",
       "  ',サンデーサイレンス,ウイン',\n",
       "  'サン',\n",
       "  'サンデ',\n",
       "  'サンデー',\n",
       "  'サンデーサ',\n",
       "  'サンデーサイ',\n",
       "  'サンデーサイレ',\n",
       "  'サンデーサイレン',\n",
       "  'サンデーサイレンス',\n",
       "  'サンデーサイレンス,',\n",
       "  'サンデーサイレンス,ウ',\n",
       "  'サンデーサイレンス,ウイ',\n",
       "  'サンデーサイレンス,ウイン',\n",
       "  'サンデーサイレンス,ウインド',\n",
       "  'ンデ',\n",
       "  'ンデー',\n",
       "  'ンデーサ',\n",
       "  'ンデーサイ',\n",
       "  'ンデーサイレ',\n",
       "  'ンデーサイレン',\n",
       "  'ンデーサイレンス',\n",
       "  'ンデーサイレンス,',\n",
       "  'ンデーサイレンス,ウ',\n",
       "  'ンデーサイレンス,ウイ',\n",
       "  'ンデーサイレンス,ウイン',\n",
       "  'ンデーサイレンス,ウインド',\n",
       "  'ンデーサイレンス,ウインドイ',\n",
       "  'デー',\n",
       "  'デーサ',\n",
       "  'デーサイ',\n",
       "  'デーサイレ',\n",
       "  'デーサイレン',\n",
       "  'デーサイレンス',\n",
       "  'デーサイレンス,',\n",
       "  'デーサイレンス,ウ',\n",
       "  'デーサイレンス,ウイ',\n",
       "  'デーサイレンス,ウイン',\n",
       "  'デーサイレンス,ウインド',\n",
       "  'デーサイレンス,ウインドイ',\n",
       "  'デーサイレンス,ウインドイン',\n",
       "  'ーサ',\n",
       "  'ーサイ',\n",
       "  'ーサイレ',\n",
       "  'ーサイレン',\n",
       "  'ーサイレンス',\n",
       "  'ーサイレンス,',\n",
       "  'ーサイレンス,ウ',\n",
       "  'ーサイレンス,ウイ',\n",
       "  'ーサイレンス,ウイン',\n",
       "  'ーサイレンス,ウインド',\n",
       "  'ーサイレンス,ウインドイ',\n",
       "  'ーサイレンス,ウインドイン',\n",
       "  'ーサイレンス,ウインドインハ',\n",
       "  'サイ',\n",
       "  'サイレ',\n",
       "  'サイレン',\n",
       "  'サイレンス',\n",
       "  'サイレンス,',\n",
       "  'サイレンス,ウ',\n",
       "  'サイレンス,ウイ',\n",
       "  'サイレンス,ウイン',\n",
       "  'サイレンス,ウインド',\n",
       "  'サイレンス,ウインドイ',\n",
       "  'サイレンス,ウインドイン',\n",
       "  'サイレンス,ウインドインハ',\n",
       "  'サイレンス,ウインドインハー',\n",
       "  'イレ',\n",
       "  'イレン',\n",
       "  'イレンス',\n",
       "  'イレンス,',\n",
       "  'イレンス,ウ',\n",
       "  'イレンス,ウイ',\n",
       "  'イレンス,ウイン',\n",
       "  'イレンス,ウインド',\n",
       "  'イレンス,ウインドイ',\n",
       "  'イレンス,ウインドイン',\n",
       "  'イレンス,ウインドインハ',\n",
       "  'イレンス,ウインドインハー',\n",
       "  'イレンス,ウインドインハーヘ',\n",
       "  'レン',\n",
       "  'レンス',\n",
       "  'レンス,',\n",
       "  'レンス,ウ',\n",
       "  'レンス,ウイ',\n",
       "  'レンス,ウイン',\n",
       "  'レンス,ウインド',\n",
       "  'レンス,ウインドイ',\n",
       "  'レンス,ウインドイン',\n",
       "  'レンス,ウインドインハ',\n",
       "  'レンス,ウインドインハー',\n",
       "  'レンス,ウインドインハーヘ',\n",
       "  'レンス,ウインドインハーヘア',\n",
       "  'ンス',\n",
       "  'ンス,',\n",
       "  'ンス,ウ',\n",
       "  'ンス,ウイ',\n",
       "  'ンス,ウイン',\n",
       "  'ンス,ウインド',\n",
       "  'ンス,ウインドイ',\n",
       "  'ンス,ウインドイン',\n",
       "  'ンス,ウインドインハ',\n",
       "  'ンス,ウインドインハー',\n",
       "  'ンス,ウインドインハーヘ',\n",
       "  'ンス,ウインドインハーヘア',\n",
       "  'ンス,ウインドインハーヘア,',\n",
       "  'ス,',\n",
       "  'ス,ウ',\n",
       "  'ス,ウイ',\n",
       "  'ス,ウイン',\n",
       "  'ス,ウインド',\n",
       "  'ス,ウインドイ',\n",
       "  'ス,ウインドイン',\n",
       "  'ス,ウインドインハ',\n",
       "  'ス,ウインドインハー',\n",
       "  'ス,ウインドインハーヘ',\n",
       "  'ス,ウインドインハーヘア',\n",
       "  'ス,ウインドインハーヘア,',\n",
       "  'ス,ウインドインハーヘア,フ',\n",
       "  ',ウ',\n",
       "  ',ウイ',\n",
       "  ',ウイン',\n",
       "  ',ウインド',\n",
       "  ',ウインドイ',\n",
       "  ',ウインドイン',\n",
       "  ',ウインドインハ',\n",
       "  ',ウインドインハー',\n",
       "  ',ウインドインハーヘ',\n",
       "  ',ウインドインハーヘア',\n",
       "  ',ウインドインハーヘア,',\n",
       "  ',ウインドインハーヘア,フ',\n",
       "  ',ウインドインハーヘア,フレ',\n",
       "  'ウイ',\n",
       "  'ウイン',\n",
       "  'ウインド',\n",
       "  'ウインドイ',\n",
       "  'ウインドイン',\n",
       "  'ウインドインハ',\n",
       "  'ウインドインハー',\n",
       "  'ウインドインハーヘ',\n",
       "  'ウインドインハーヘア',\n",
       "  'ウインドインハーヘア,',\n",
       "  'ウインドインハーヘア,フ',\n",
       "  'ウインドインハーヘア,フレ',\n",
       "  'ウインドインハーヘア,フレン',\n",
       "  'イン',\n",
       "  'インド',\n",
       "  'インドイ',\n",
       "  'インドイン',\n",
       "  'インドインハ',\n",
       "  'インドインハー',\n",
       "  'インドインハーヘ',\n",
       "  'インドインハーヘア',\n",
       "  'インドインハーヘア,',\n",
       "  'インドインハーヘア,フ',\n",
       "  'インドインハーヘア,フレ',\n",
       "  'インドインハーヘア,フレン',\n",
       "  'インドインハーヘア,フレンチ',\n",
       "  'ンド',\n",
       "  'ンドイ',\n",
       "  'ンドイン',\n",
       "  'ンドインハ',\n",
       "  'ンドインハー',\n",
       "  'ンドインハーヘ',\n",
       "  'ンドインハーヘア',\n",
       "  'ンドインハーヘア,',\n",
       "  'ンドインハーヘア,フ',\n",
       "  'ンドインハーヘア,フレ',\n",
       "  'ンドインハーヘア,フレン',\n",
       "  'ンドインハーヘア,フレンチ',\n",
       "  'ンドインハーヘア,フレンチデ',\n",
       "  'ドイ',\n",
       "  'ドイン',\n",
       "  'ドインハ',\n",
       "  'ドインハー',\n",
       "  'ドインハーヘ',\n",
       "  'ドインハーヘア',\n",
       "  'ドインハーヘア,',\n",
       "  'ドインハーヘア,フ',\n",
       "  'ドインハーヘア,フレ',\n",
       "  'ドインハーヘア,フレン',\n",
       "  'ドインハーヘア,フレンチ',\n",
       "  'ドインハーヘア,フレンチデ',\n",
       "  'ドインハーヘア,フレンチデピ',\n",
       "  'イン',\n",
       "  'インハ',\n",
       "  'インハー',\n",
       "  'インハーヘ',\n",
       "  'インハーヘア',\n",
       "  'インハーヘア,',\n",
       "  'インハーヘア,フ',\n",
       "  'インハーヘア,フレ',\n",
       "  'インハーヘア,フレン',\n",
       "  'インハーヘア,フレンチ',\n",
       "  'インハーヘア,フレンチデ',\n",
       "  'インハーヘア,フレンチデピ',\n",
       "  'インハーヘア,フレンチデピュ',\n",
       "  'ンハ',\n",
       "  'ンハー',\n",
       "  'ンハーヘ',\n",
       "  'ンハーヘア',\n",
       "  'ンハーヘア,',\n",
       "  'ンハーヘア,フ',\n",
       "  'ンハーヘア,フレ',\n",
       "  'ンハーヘア,フレン',\n",
       "  'ンハーヘア,フレンチ',\n",
       "  'ンハーヘア,フレンチデ',\n",
       "  'ンハーヘア,フレンチデピ',\n",
       "  'ンハーヘア,フレンチデピュ',\n",
       "  'ンハーヘア,フレンチデピュテ',\n",
       "  'ハー',\n",
       "  'ハーヘ',\n",
       "  'ハーヘア',\n",
       "  'ハーヘア,',\n",
       "  'ハーヘア,フ',\n",
       "  'ハーヘア,フレ',\n",
       "  'ハーヘア,フレン',\n",
       "  'ハーヘア,フレンチ',\n",
       "  'ハーヘア,フレンチデ',\n",
       "  'ハーヘア,フレンチデピ',\n",
       "  'ハーヘア,フレンチデピュ',\n",
       "  'ハーヘア,フレンチデピュテ',\n",
       "  'ハーヘア,フレンチデピュティ',\n",
       "  'ーヘ',\n",
       "  'ーヘア',\n",
       "  'ーヘア,',\n",
       "  'ーヘア,フ',\n",
       "  'ーヘア,フレ',\n",
       "  'ーヘア,フレン',\n",
       "  'ーヘア,フレンチ',\n",
       "  'ーヘア,フレンチデ',\n",
       "  'ーヘア,フレンチデピ',\n",
       "  'ーヘア,フレンチデピュ',\n",
       "  'ーヘア,フレンチデピュテ',\n",
       "  'ーヘア,フレンチデピュティ',\n",
       "  'ーヘア,フレンチデピュティ,',\n",
       "  'ヘア',\n",
       "  'ヘア,',\n",
       "  'ヘア,フ',\n",
       "  'ヘア,フレ',\n",
       "  'ヘア,フレン',\n",
       "  'ヘア,フレンチ',\n",
       "  'ヘア,フレンチデ',\n",
       "  'ヘア,フレンチデピ',\n",
       "  'ヘア,フレンチデピュ',\n",
       "  'ヘア,フレンチデピュテ',\n",
       "  'ヘア,フレンチデピュティ',\n",
       "  'ヘア,フレンチデピュティ,',\n",
       "  'ヘア,フレンチデピュティ,ク',\n",
       "  'ア,',\n",
       "  'ア,フ',\n",
       "  'ア,フレ',\n",
       "  'ア,フレン',\n",
       "  'ア,フレンチ',\n",
       "  'ア,フレンチデ',\n",
       "  'ア,フレンチデピ',\n",
       "  'ア,フレンチデピュ',\n",
       "  'ア,フレンチデピュテ',\n",
       "  'ア,フレンチデピュティ',\n",
       "  'ア,フレンチデピュティ,',\n",
       "  'ア,フレンチデピュティ,ク',\n",
       "  'ア,フレンチデピュティ,クロ',\n",
       "  ',フ',\n",
       "  ',フレ',\n",
       "  ',フレン',\n",
       "  ',フレンチ',\n",
       "  ',フレンチデ',\n",
       "  ',フレンチデピ',\n",
       "  ',フレンチデピュ',\n",
       "  ',フレンチデピュテ',\n",
       "  ',フレンチデピュティ',\n",
       "  ',フレンチデピュティ,',\n",
       "  ',フレンチデピュティ,ク',\n",
       "  ',フレンチデピュティ,クロ',\n",
       "  ',フレンチデピュティ,クロカ',\n",
       "  'フレ',\n",
       "  'フレン',\n",
       "  'フレンチ',\n",
       "  'フレンチデ',\n",
       "  'フレンチデピ',\n",
       "  'フレンチデピュ',\n",
       "  'フレンチデピュテ',\n",
       "  'フレンチデピュティ',\n",
       "  'フレンチデピュティ,',\n",
       "  'フレンチデピュティ,ク',\n",
       "  'フレンチデピュティ,クロ',\n",
       "  'フレンチデピュティ,クロカ',\n",
       "  'フレンチデピュティ,クロカミ',\n",
       "  'レン',\n",
       "  'レンチ',\n",
       "  'レンチデ',\n",
       "  'レンチデピ',\n",
       "  'レンチデピュ',\n",
       "  'レンチデピュテ',\n",
       "  'レンチデピュティ',\n",
       "  'レンチデピュティ,',\n",
       "  'レンチデピュティ,ク',\n",
       "  'レンチデピュティ,クロ',\n",
       "  'レンチデピュティ,クロカ',\n",
       "  'レンチデピュティ,クロカミ',\n",
       "  'レンチデピュティ,クロカミ,',\n",
       "  'ンチ',\n",
       "  'ンチデ',\n",
       "  'ンチデピ',\n",
       "  'ンチデピュ',\n",
       "  'ンチデピュテ',\n",
       "  'ンチデピュティ',\n",
       "  'ンチデピュティ,',\n",
       "  'ンチデピュティ,ク',\n",
       "  'ンチデピュティ,クロ',\n",
       "  'ンチデピュティ,クロカ',\n",
       "  'ンチデピュティ,クロカミ',\n",
       "  'ンチデピュティ,クロカミ,',\n",
       "  'ンチデピュティ,クロカミ,H',\n",
       "  'チデ',\n",
       "  'チデピ',\n",
       "  'チデピュ',\n",
       "  'チデピュテ',\n",
       "  'チデピュティ',\n",
       "  'チデピュティ,',\n",
       "  'チデピュティ,ク',\n",
       "  'チデピュティ,クロ',\n",
       "  'チデピュティ,クロカ',\n",
       "  'チデピュティ,クロカミ',\n",
       "  'チデピュティ,クロカミ,',\n",
       "  'チデピュティ,クロカミ,H',\n",
       "  'チデピュティ,クロカミ,Ha',\n",
       "  'デピ',\n",
       "  'デピュ',\n",
       "  'デピュテ',\n",
       "  'デピュティ',\n",
       "  'デピュティ,',\n",
       "  'デピュティ,ク',\n",
       "  'デピュティ,クロ',\n",
       "  'デピュティ,クロカ',\n",
       "  'デピュティ,クロカミ',\n",
       "  'デピュティ,クロカミ,',\n",
       "  'デピュティ,クロカミ,H',\n",
       "  'デピュティ,クロカミ,Ha',\n",
       "  'デピュティ,クロカミ,Hal',\n",
       "  'ピュ',\n",
       "  'ピュテ',\n",
       "  'ピュティ',\n",
       "  'ピュティ,',\n",
       "  'ピュティ,ク',\n",
       "  'ピュティ,クロ',\n",
       "  'ピュティ,クロカ',\n",
       "  'ピュティ,クロカミ',\n",
       "  'ピュティ,クロカミ,',\n",
       "  'ピュティ,クロカミ,H',\n",
       "  'ピュティ,クロカミ,Ha',\n",
       "  'ピュティ,クロカミ,Hal',\n",
       "  'ピュティ,クロカミ,Halo',\n",
       "  'ュテ',\n",
       "  'ュティ',\n",
       "  'ュティ,',\n",
       "  'ュティ,ク',\n",
       "  'ュティ,クロ',\n",
       "  'ュティ,クロカ',\n",
       "  'ュティ,クロカミ',\n",
       "  'ュティ,クロカミ,',\n",
       "  'ュティ,クロカミ,H',\n",
       "  'ュティ,クロカミ,Ha',\n",
       "  'ュティ,クロカミ,Hal',\n",
       "  'ュティ,クロカミ,Halo',\n",
       "  'ュティ,クロカミ,Halo,',\n",
       "  'ティ',\n",
       "  'ティ,',\n",
       "  'ティ,ク',\n",
       "  'ティ,クロ',\n",
       "  'ティ,クロカ',\n",
       "  'ティ,クロカミ',\n",
       "  'ティ,クロカミ,',\n",
       "  'ティ,クロカミ,H',\n",
       "  'ティ,クロカミ,Ha',\n",
       "  'ティ,クロカミ,Hal',\n",
       "  'ティ,クロカミ,Halo',\n",
       "  'ティ,クロカミ,Halo,',\n",
       "  'ティ,クロカミ,Halo,W',\n",
       "  'ィ,',\n",
       "  'ィ,ク',\n",
       "  'ィ,クロ',\n",
       "  'ィ,クロカ',\n",
       "  'ィ,クロカミ',\n",
       "  'ィ,クロカミ,',\n",
       "  'ィ,クロカミ,H',\n",
       "  'ィ,クロカミ,Ha',\n",
       "  'ィ,クロカミ,Hal',\n",
       "  'ィ,クロカミ,Halo',\n",
       "  'ィ,クロカミ,Halo,',\n",
       "  'ィ,クロカミ,Halo,W',\n",
       "  'ィ,クロカミ,Halo,Wi',\n",
       "  ',ク',\n",
       "  ',クロ',\n",
       "  ',クロカ',\n",
       "  ',クロカミ',\n",
       "  ',クロカミ,',\n",
       "  ',クロカミ,H',\n",
       "  ',クロカミ,Ha',\n",
       "  ',クロカミ,Hal',\n",
       "  ',クロカミ,Halo',\n",
       "  ',クロカミ,Halo,',\n",
       "  ',クロカミ,Halo,W',\n",
       "  ',クロカミ,Halo,Wi',\n",
       "  ',クロカミ,Halo,Wis',\n",
       "  'クロ',\n",
       "  'クロカ',\n",
       "  'クロカミ',\n",
       "  'クロカミ,',\n",
       "  'クロカミ,H',\n",
       "  'クロカミ,Ha',\n",
       "  'クロカミ,Hal',\n",
       "  'クロカミ,Halo',\n",
       "  'クロカミ,Halo,',\n",
       "  'クロカミ,Halo,W',\n",
       "  'クロカミ,Halo,Wi',\n",
       "  'クロカミ,Halo,Wis',\n",
       "  'クロカミ,Halo,Wish',\n",
       "  'ロカ',\n",
       "  'ロカミ',\n",
       "  'ロカミ,',\n",
       "  'ロカミ,H',\n",
       "  'ロカミ,Ha',\n",
       "  'ロカミ,Hal',\n",
       "  'ロカミ,Halo',\n",
       "  'ロカミ,Halo,',\n",
       "  'ロカミ,Halo,W',\n",
       "  'ロカミ,Halo,Wi',\n",
       "  'ロカミ,Halo,Wis',\n",
       "  'ロカミ,Halo,Wish',\n",
       "  'ロカミ,Halo,Wishi',\n",
       "  'カミ',\n",
       "  'カミ,',\n",
       "  'カミ,H',\n",
       "  'カミ,Ha',\n",
       "  'カミ,Hal',\n",
       "  'カミ,Halo',\n",
       "  'カミ,Halo,',\n",
       "  'カミ,Halo,W',\n",
       "  'カミ,Halo,Wi',\n",
       "  'カミ,Halo,Wis',\n",
       "  'カミ,Halo,Wish',\n",
       "  'カミ,Halo,Wishi',\n",
       "  'カミ,Halo,Wishin',\n",
       "  'ミ,',\n",
       "  'ミ,H',\n",
       "  'ミ,Ha',\n",
       "  'ミ,Hal',\n",
       "  'ミ,Halo',\n",
       "  'ミ,Halo,',\n",
       "  'ミ,Halo,W',\n",
       "  'ミ,Halo,Wi',\n",
       "  'ミ,Halo,Wis',\n",
       "  'ミ,Halo,Wish',\n",
       "  'ミ,Halo,Wishi',\n",
       "  'ミ,Halo,Wishin',\n",
       "  'ミ,Halo,Wishing',\n",
       "  ',H',\n",
       "  ',Ha',\n",
       "  ',Hal',\n",
       "  ',Halo',\n",
       "  ',Halo,',\n",
       "  ',Halo,W',\n",
       "  ',Halo,Wi',\n",
       "  ',Halo,Wis',\n",
       "  ',Halo,Wish',\n",
       "  ',Halo,Wishi',\n",
       "  ',Halo,Wishin',\n",
       "  ',Halo,Wishing',\n",
       "  ',Halo,WishingW',\n",
       "  'Ha',\n",
       "  'Hal',\n",
       "  'Halo',\n",
       "  'Halo,',\n",
       "  'Halo,W',\n",
       "  'Halo,Wi',\n",
       "  'Halo,Wis',\n",
       "  'Halo,Wish',\n",
       "  'Halo,Wishi',\n",
       "  'Halo,Wishin',\n",
       "  'Halo,Wishing',\n",
       "  'Halo,WishingW',\n",
       "  'Halo,WishingWe',\n",
       "  'al',\n",
       "  'alo',\n",
       "  'alo,',\n",
       "  'alo,W',\n",
       "  'alo,Wi',\n",
       "  'alo,Wis',\n",
       "  'alo,Wish',\n",
       "  'alo,Wishi',\n",
       "  'alo,Wishin',\n",
       "  'alo,Wishing',\n",
       "  'alo,WishingW',\n",
       "  'alo,WishingWe',\n",
       "  'alo,WishingWel',\n",
       "  'lo',\n",
       "  'lo,',\n",
       "  'lo,W',\n",
       "  'lo,Wi',\n",
       "  'lo,Wis',\n",
       "  'lo,Wish',\n",
       "  'lo,Wishi',\n",
       "  'lo,Wishin',\n",
       "  'lo,Wishing',\n",
       "  'lo,WishingW',\n",
       "  'lo,WishingWe',\n",
       "  'lo,WishingWel',\n",
       "  'lo,WishingWell',\n",
       "  'o,',\n",
       "  'o,W',\n",
       "  'o,Wi',\n",
       "  'o,Wis',\n",
       "  'o,Wish',\n",
       "  'o,Wishi',\n",
       "  'o,Wishin',\n",
       "  'o,Wishing',\n",
       "  'o,WishingW',\n",
       "  'o,WishingWe',\n",
       "  'o,WishingWel',\n",
       "  'o,WishingWell',\n",
       "  'o,WishingWell,',\n",
       "  ',W',\n",
       "  ',Wi',\n",
       "  ',Wis',\n",
       "  ',Wish',\n",
       "  ',Wishi',\n",
       "  ',Wishin',\n",
       "  ',Wishing',\n",
       "  ',WishingW',\n",
       "  ',WishingWe',\n",
       "  ',WishingWel',\n",
       "  ',WishingWell',\n",
       "  ',WishingWell,',\n",
       "  ',WishingWell,A',\n",
       "  'Wi',\n",
       "  'Wis',\n",
       "  'Wish',\n",
       "  'Wishi',\n",
       "  'Wishin',\n",
       "  'Wishing',\n",
       "  'WishingW',\n",
       "  'WishingWe',\n",
       "  'WishingWel',\n",
       "  'WishingWell',\n",
       "  'WishingWell,',\n",
       "  'WishingWell,A',\n",
       "  'WishingWell,Al',\n",
       "  'is',\n",
       "  'ish',\n",
       "  'ishi',\n",
       "  'ishin',\n",
       "  'ishing',\n",
       "  'ishingW',\n",
       "  'ishingWe',\n",
       "  'ishingWel',\n",
       "  'ishingWell',\n",
       "  'ishingWell,',\n",
       "  'ishingWell,A',\n",
       "  'ishingWell,Al',\n",
       "  'ishingWell,Alz',\n",
       "  'sh',\n",
       "  'shi',\n",
       "  'shin',\n",
       "  'shing',\n",
       "  'shingW',\n",
       "  'shingWe',\n",
       "  'shingWel',\n",
       "  'shingWell',\n",
       "  'shingWell,',\n",
       "  'shingWell,A',\n",
       "  'shingWell,Al',\n",
       "  'shingWell,Alz',\n",
       "  'shingWell,Alza',\n",
       "  'hi',\n",
       "  'hin',\n",
       "  'hing',\n",
       "  'hingW',\n",
       "  'hingWe',\n",
       "  'hingWel',\n",
       "  'hingWell',\n",
       "  'hingWell,',\n",
       "  'hingWell,A',\n",
       "  'hingWell,Al',\n",
       "  'hingWell,Alz',\n",
       "  'hingWell,Alza',\n",
       "  'hingWell,Alzao',\n",
       "  'in',\n",
       "  'ing',\n",
       "  'ingW',\n",
       "  'ingWe',\n",
       "  'ingWel',\n",
       "  'ingWell',\n",
       "  'ingWell,',\n",
       "  'ingWell,A',\n",
       "  'ingWell,Al',\n",
       "  'ingWell,Alz',\n",
       "  'ingWell,Alza',\n",
       "  'ingWell,Alzao',\n",
       "  'ingWell,Alzao,',\n",
       "  'ng',\n",
       "  'ngW',\n",
       "  'ngWe',\n",
       "  'ngWel',\n",
       "  'ngWell',\n",
       "  'ngWell,',\n",
       "  'ngWell,A',\n",
       "  'ngWell,Al',\n",
       "  'ngWell,Alz',\n",
       "  'ngWell,Alza',\n",
       "  'ngWell,Alzao',\n",
       "  'ngWell,Alzao,',\n",
       "  'ngWell,Alzao,B',\n",
       "  'gW',\n",
       "  'gWe',\n",
       "  'gWel',\n",
       "  'gWell',\n",
       "  'gWell,',\n",
       "  'gWell,A',\n",
       "  'gWell,Al',\n",
       "  'gWell,Alz',\n",
       "  'gWell,Alza',\n",
       "  'gWell,Alzao',\n",
       "  'gWell,Alzao,',\n",
       "  'gWell,Alzao,B',\n",
       "  'gWell,Alzao,Bu',\n",
       "  'We',\n",
       "  'Wel',\n",
       "  'Well',\n",
       "  'Well,',\n",
       "  'Well,A',\n",
       "  'Well,Al',\n",
       "  'Well,Alz',\n",
       "  'Well,Alza',\n",
       "  'Well,Alzao',\n",
       "  'Well,Alzao,',\n",
       "  'Well,Alzao,B',\n",
       "  'Well,Alzao,Bu',\n",
       "  'Well,Alzao,Bur',\n",
       "  'el',\n",
       "  'ell',\n",
       "  'ell,',\n",
       "  'ell,A',\n",
       "  'ell,Al',\n",
       "  'ell,Alz',\n",
       "  'ell,Alza',\n",
       "  'ell,Alzao',\n",
       "  'ell,Alzao,',\n",
       "  'ell,Alzao,B',\n",
       "  'ell,Alzao,Bu',\n",
       "  'ell,Alzao,Bur',\n",
       "  'ell,Alzao,Burg',\n",
       "  'll',\n",
       "  'll,',\n",
       "  'll,A',\n",
       "  'll,Al',\n",
       "  'll,Alz',\n",
       "  'll,Alza',\n",
       "  'll,Alzao',\n",
       "  'll,Alzao,',\n",
       "  'll,Alzao,B',\n",
       "  'll,Alzao,Bu',\n",
       "  'll,Alzao,Bur',\n",
       "  'll,Alzao,Burg',\n",
       "  'll,Alzao,Burgh',\n",
       "  'l,',\n",
       "  'l,A',\n",
       "  'l,Al',\n",
       "  'l,Alz',\n",
       "  'l,Alza',\n",
       "  'l,Alzao',\n",
       "  'l,Alzao,',\n",
       "  'l,Alzao,B',\n",
       "  'l,Alzao,Bu',\n",
       "  'l,Alzao,Bur',\n",
       "  'l,Alzao,Burg',\n",
       "  'l,Alzao,Burgh',\n",
       "  'l,Alzao,Burghc',\n",
       "  ',A',\n",
       "  ',Al',\n",
       "  ',Alz',\n",
       "  ',Alza',\n",
       "  ',Alzao',\n",
       "  ',Alzao,',\n",
       "  ',Alzao,B',\n",
       "  ',Alzao,Bu',\n",
       "  ',Alzao,Bur',\n",
       "  ',Alzao,Burg',\n",
       "  ',Alzao,Burgh',\n",
       "  ',Alzao,Burghc',\n",
       "  ',Alzao,Burghcl',\n",
       "  'Al',\n",
       "  'Alz',\n",
       "  'Alza',\n",
       "  'Alzao',\n",
       "  'Alzao,',\n",
       "  'Alzao,B',\n",
       "  'Alzao,Bu',\n",
       "  'Alzao,Bur',\n",
       "  'Alzao,Burg',\n",
       "  'Alzao,Burgh',\n",
       "  'Alzao,Burghc',\n",
       "  'Alzao,Burghcl',\n",
       "  'Alzao,Burghcle',\n",
       "  'lz',\n",
       "  'lza',\n",
       "  'lzao',\n",
       "  'lzao,',\n",
       "  'lzao,B',\n",
       "  'lzao,Bu',\n",
       "  'lzao,Bur',\n",
       "  'lzao,Burg',\n",
       "  'lzao,Burgh',\n",
       "  'lzao,Burghc',\n",
       "  'lzao,Burghcl',\n",
       "  'lzao,Burghcle',\n",
       "  'lzao,Burghcler',\n",
       "  'za',\n",
       "  'zao',\n",
       "  'zao,',\n",
       "  'zao,B',\n",
       "  'zao,Bu',\n",
       "  'zao,Bur',\n",
       "  'zao,Burg',\n",
       "  'zao,Burgh',\n",
       "  'zao,Burghc',\n",
       "  'zao,Burghcl',\n",
       "  'zao,Burghcle',\n",
       "  'zao,Burghcler',\n",
       "  'zao,Burghclere',\n",
       "  'ao',\n",
       "  'ao,',\n",
       "  'ao,B',\n",
       "  'ao,Bu',\n",
       "  'ao,Bur',\n",
       "  'ao,Burg',\n",
       "  'ao,Burgh',\n",
       "  'ao,Burghc',\n",
       "  'ao,Burghcl',\n",
       "  'ao,Burghcle',\n",
       "  'ao,Burghcler',\n",
       "  'ao,Burghclere',\n",
       "  ...],\n",
       " array([ 743866,  291704,  948238, ..., 1967956, 1364542,  467858]))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_txt.get_subwords(model.words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e80ec-9450-43b0-8e0d-f170f67086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f41fe-1013-4eaf-9776-07d8819750b1",
   "metadata": {},
   "source": [
    "# 今後の方針\n",
    "1. とりあえず, Peds class で, 馬名 -> 馬名正規化\n",
    "2. 仕様は決めていないが, 学習ずみ, fasttext モデルで血統をベクトル化\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f671e88-ce0c-4943-b598-e0c8c1f43a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
